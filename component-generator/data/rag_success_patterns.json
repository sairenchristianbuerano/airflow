{
  "patterns": [
    {
      "id": "nemo_qa_operator_success_2026_01_20",
      "component_name": "NeMoQuestionAnsweringOperator",
      "category": "ml",
      "subcategory": "nlp",
      "framework": "nvidia_nemo",
      "component_type": "operator",
      "operator_code": "try:\n    from airflow.sdk.bases.operator import BaseOperator\nexcept ImportError:\n    from airflow.models import BaseOperator\n\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence\nfrom pathlib import Path\nimport json\nimport os\nimport logging\n\nclass NeMoQuestionAnsweringOperator(BaseOperator):\n    \"\"\"\n    Execute NVIDIA NeMo Question Answering for training or inference.\n    \n    Supports extractive QA (BERT-based) and generative QA (T5/GPT-based models).\n    Handles dataset in SQuAD format for training and evaluation.\n    \n    Args:\n        mode: Operation mode - train, inference, or evaluate\n        model_type: QA model type - extractive, generative_s2s, or generative_gpt\n        pretrained_model_name: Pretrained model name from HuggingFace or NeMo\n        dataset_file: Path to dataset file in SQuAD format\n        output_dir: Output directory for results\n        **kwargs: Additional keyword arguments passed to BaseOperator\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['mode', 'model_type', 'dataset_file', 'output_dir']\n    ui_color: str = \"#76b900\"",
      "metadata": {
        "component_name": "NeMoQuestionAnsweringOperator",
        "component_type": "operator",
        "category": "ml",
        "subcategory": "nlp",
        "framework": "nvidia_nemo",
        "generation_date": "2026-01-20",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "ml_operator_with_runtime_params",
      "indexed_at": "2026-01-20T19:50:56.057278"
    },
    {
      "id": "nvidia_riva_operator_success_2026_01_27",
      "component_name": "NvidiaRivaOperator",
      "category": "ml",
      "subcategory": "speech-ai",
      "framework": "nvidia_riva",
      "component_type": "operator",
      "operator_code": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence\nimport grpc\nimport json\nimport wave\nimport os\nfrom pathlib import Path\n\nclass NvidiaRivaOperator(BaseOperator):\n    \"\"\"\n    Operator for NVIDIA Riva conversational AI operations.\n\n    Supports speech-to-text (ASR), text-to-speech (TTS), and NLP inference\n    using NVIDIA Riva gRPC API.\n    \"\"\"\n\n    template_fields: Sequence[str] = [\n        'operation_type', 'language_code', 'text_input', 'audio_file_path',\n        'output_path', 'model_name'\n    ]\n    ui_color: str = \"#76B900\"",
      "metadata": {
        "component_name": "NvidiaRivaOperator",
        "component_type": "operator",
        "category": "ml",
        "subcategory": "speech-ai",
        "framework": "nvidia_riva",
        "generation_date": "2026-01-27",
        "generation_status": "success"
      },
      "success_score": 175,
      "pattern_type": "ml_operator_with_runtime_params",
      "indexed_at": "2026-01-27T00:00:00.000000",
      "code_patterns": {
        "imports": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence\nimport grpc",
        "class_definition": "class NvidiaRivaOperator(BaseOperator):",
        "param_import_pattern": "try:\n    from airflow.sdk import Param  # Airflow 3.x\nexcept ImportError:\n    from airflow.models.param import Param  # Airflow 2.x fallback"
      }
    },
    {
      "id": "api_health_sensor_success_2026_01_28",
      "component_name": "ApiHealthSensor",
      "category": "monitoring",
      "subcategory": "api-health",
      "framework": "requests",
      "component_type": "sensor",
      "sensor_code": "try:\n    from airflow.sdk.bases.sensor import BaseSensor\nexcept ImportError:\n    from airflow.sensors.base import BaseSensor\n\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence\nimport requests\nimport json\nfrom datetime import timedelta\n\n\nclass ApiHealthSensor(BaseSensor):\n    \"\"\"\n    Sensor that monitors API endpoint health and waits for successful response before proceeding.\n\n    This sensor performs HTTP GET requests to a specified endpoint and checks for:\n    - Expected HTTP status code\n    - Optional response body validation using JSON path\n    - Request timeout handling\n    \"\"\"\n\n    template_fields: Sequence[str] = ['endpoint_url', 'expected_status_code', 'headers', 'response_check']\n    ui_color: str = \"#50C878\"\n\n    def __init__(\n        self,\n        endpoint_url: str,\n        expected_status_code: int = 200,\n        request_timeout: int = 30,\n        headers: Optional[Dict[str, str]] = None,\n        response_check: Optional[str] = None,\n        poke_interval: int = 60,\n        timeout: int = 300,\n        mode: str = 'poke',\n        **kwargs\n    ):\n        super().__init__(\n            poke_interval=poke_interval,\n            timeout=timeout,\n            mode=mode,\n            **kwargs\n        )\n        self.endpoint_url = endpoint_url\n        self.expected_status_code = expected_status_code\n        self.request_timeout = request_timeout\n        self.headers = headers or {}\n        self.response_check = response_check\n\n    def poke(self, context: Dict[str, Any]) -> bool:\n        \"\"\"Check if API endpoint is healthy by making HTTP request.\"\"\"\n        self.log.info(f\"Checking API health for endpoint: {self.endpoint_url}\")\n\n        try:\n            if not self.endpoint_url or not isinstance(self.endpoint_url, str):\n                raise AirflowException(\"endpoint_url must be a non-empty string\")\n\n            response = requests.get(\n                url=self.endpoint_url,\n                headers=self.headers,\n                timeout=self.request_timeout,\n                allow_redirects=True\n            )\n\n            if response.status_code != self.expected_status_code:\n                self.log.warning(\n                    f\"Status code {response.status_code} does not match expected {self.expected_status_code}\"\n                )\n                return False\n\n            self.log.info(\"API health check passed successfully\")\n            return True\n\n        except requests.exceptions.Timeout:\n            self.log.warning(f\"Request timeout after {self.request_timeout} seconds\")\n            return False\n\n        except requests.exceptions.ConnectionError as e:\n            self.log.warning(f\"Connection error: {str(e)}\")\n            return False\n\n        except requests.exceptions.RequestException as e:\n            raise AirflowException(f\"Request failed with error: {str(e)}\")",
      "metadata": {
        "component_name": "ApiHealthSensor",
        "component_type": "sensor",
        "category": "monitoring",
        "subcategory": "api-health",
        "framework": "requests",
        "generation_date": "2026-01-28",
        "generation_status": "success",
        "component_features": {
          "poke_method": true,
          "runtime_parameters": true,
          "template_fields": true,
          "airflow_2x_compatible": true,
          "airflow_3x_compatible": true,
          "type_hints": true,
          "error_handling": "comprehensive",
          "json_path_validation": true,
          "timeout_handling": true
        },
        "inputs": [
          {
            "name": "endpoint_url",
            "type": "str",
            "required": true,
            "template_field": true
          },
          {
            "name": "expected_status_code",
            "type": "int",
            "required": false,
            "default": 200,
            "template_field": true
          },
          {
            "name": "request_timeout",
            "type": "int",
            "required": false,
            "default": 30
          },
          {
            "name": "headers",
            "type": "dict",
            "required": false,
            "template_field": true
          },
          {
            "name": "response_check",
            "type": "str",
            "required": false,
            "template_field": true
          },
          {
            "name": "poke_interval",
            "type": "int",
            "required": false,
            "default": 60
          },
          {
            "name": "timeout",
            "type": "int",
            "required": false,
            "default": 300
          },
          {
            "name": "mode",
            "type": "str",
            "required": false,
            "default": "poke"
          }
        ],
        "runtime_params": [
          {
            "name": "endpoint_url",
            "type": "string",
            "default": "https://httpbin.org/status/200"
          },
          {
            "name": "expected_status_code",
            "type": "integer",
            "default": 200
          }
        ],
        "dependencies": [
          "apache-airflow>=2.0.0",
          "requests>=2.28.0"
        ],
        "success_factors": [
          "Dual import pattern for Airflow 2.x/3.x BaseSensor compatibility",
          "Proper super().__init__ call with poke_interval, timeout, mode",
          "poke() method returns boolean for sensor behavior",
          "JSON path validation for response body checks",
          "Graceful handling of timeouts and connection errors",
          "Clear separation of terminal vs. retry-able errors"
        ],
        "recommended_use_cases": [
          "Wait for API to become healthy before downstream tasks",
          "Monitor microservice dependencies",
          "Health check gating in CI/CD pipelines",
          "Service readiness checks in Kubernetes workflows"
        ],
        "tags": [
          "sensor",
          "monitoring",
          "api-health",
          "http",
          "requests",
          "airflow-3x-compatible"
        ]
      },
      "success_score": 170,
      "pattern_type": "monitoring_sensor_with_runtime_params",
      "indexed_at": "2026-01-28T00:00:00.000000",
      "code_patterns": {
        "sensor_import": "try:\n    from airflow.sdk.bases.sensor import BaseSensor\nexcept ImportError:\n    from airflow.sensors.base import BaseSensor",
        "class_definition": "class ApiHealthSensor(BaseSensor):",
        "super_init": "super().__init__(\n    poke_interval=poke_interval,\n    timeout=timeout,\n    mode=mode,\n    **kwargs\n)",
        "poke_method": "def poke(self, context: Dict[str, Any]) -> bool:\n    # Returns True when condition is met, False to continue poking"
      },
      "relevance_keywords": [
        "sensor",
        "monitoring",
        "api health",
        "http",
        "requests",
        "poke",
        "status code",
        "endpoint",
        "health check"
      ]
    },
    {
      "id": "rest_api_hook_success_2026_01_28",
      "component_name": "RestApiHook",
      "category": "integration",
      "subcategory": "http",
      "framework": "requests",
      "component_type": "hook",
      "hook_code": "from airflow.hooks.base import BaseHook\nfrom airflow.models import Connection\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence, Union\nimport requests\nimport time\nimport base64\nfrom urllib.parse import urljoin\n\n\nclass RestApiHook(BaseHook):\n    \"\"\"\n    Hook for connecting to REST APIs with authentication, retry logic, and response handling.\n\n    This hook provides a standardized way to interact with REST APIs, supporting various\n    authentication methods, automatic retries, and comprehensive error handling.\n    \"\"\"\n\n    template_fields: Sequence[str] = ['endpoint', 'method']\n    ui_color: str = \"#f0ede4\"\n    conn_name_attr: str = 'conn_id'\n    conn_type: str = 'http'\n    hook_name: str = 'rest_api'\n\n    def __init__(\n        self,\n        conn_id: str = 'rest_api_default',\n        base_url: Optional[str] = None,\n        auth_type: str = 'none',\n        default_headers: Optional[Dict[str, str]] = None,\n        retry_count: int = 3,\n        timeout: int = 30\n    ):\n        super().__init__()\n        self.conn_id = conn_id\n        self.base_url = base_url\n        self.auth_type = auth_type\n        self.default_headers = default_headers or {}\n        self.retry_count = retry_count\n        self.timeout = timeout\n        self._connection = None\n        self._session = None\n\n    def get_conn(self) -> requests.Session:\n        \"\"\"Get or create HTTP session with authentication and default headers.\"\"\"\n        if self._session is not None:\n            return self._session\n\n        try:\n            conn = BaseHook.get_connection(self.conn_id)\n            self._session = self._create_session(conn)\n            self.log.info(f\"Connected to REST API at {self._get_base_url(conn)}\")\n            return self._session\n        except Exception as e:\n            raise AirflowException(f\"Failed to create REST API connection: {str(e)}\")",
      "metadata": {
        "component_name": "RestApiHook",
        "component_type": "hook",
        "category": "integration",
        "subcategory": "http",
        "framework": "requests",
        "generation_date": "2026-01-28",
        "generation_status": "success",
        "component_features": {
          "get_conn_method": true,
          "connection_caching": true,
          "runtime_parameters": true,
          "template_fields": true,
          "airflow_2x_compatible": true,
          "airflow_3x_compatible": true,
          "type_hints": true,
          "error_handling": "comprehensive",
          "authentication_types": [
            "none",
            "basic",
            "bearer",
            "api_key"
          ],
          "retry_logic": true,
          "exponential_backoff": true,
          "http_methods": [
            "GET",
            "POST",
            "PUT",
            "DELETE",
            "PATCH"
          ],
          "session_management": true
        },
        "inputs": [
          {
            "name": "conn_id",
            "type": "str",
            "required": false,
            "default": "rest_api_default"
          },
          {
            "name": "base_url",
            "type": "str",
            "required": false
          },
          {
            "name": "auth_type",
            "type": "str",
            "required": false,
            "default": "none",
            "enum": [
              "none",
              "basic",
              "bearer",
              "api_key"
            ]
          },
          {
            "name": "default_headers",
            "type": "dict",
            "required": false
          },
          {
            "name": "retry_count",
            "type": "int",
            "required": false,
            "default": 3
          },
          {
            "name": "timeout",
            "type": "int",
            "required": false,
            "default": 30
          }
        ],
        "runtime_params": [
          {
            "name": "endpoint",
            "type": "string",
            "default": "/health"
          },
          {
            "name": "method",
            "type": "string",
            "default": "GET"
          }
        ],
        "dependencies": [
          "apache-airflow>=2.0.0",
          "requests>=2.28.0"
        ],
        "success_factors": [
          "Proper BaseHook inheritance with get_conn() implementation",
          "conn_name_attr, conn_type, hook_name class attributes",
          "Session caching for connection reuse",
          "Multiple authentication methods (basic, bearer, api_key)",
          "Retry logic with exponential backoff",
          "Convenience methods for HTTP verbs (get, post, put, delete, patch)",
          "Connection test method for validation",
          "Session close method for cleanup"
        ],
        "recommended_use_cases": [
          "REST API integration in operators",
          "Shared API client across multiple tasks",
          "Authentication-aware HTTP requests",
          "Retry-enabled API calls"
        ],
        "tags": [
          "hook",
          "integration",
          "http",
          "rest-api",
          "requests",
          "authentication",
          "retry",
          "airflow-3x-compatible"
        ]
      },
      "success_score": 180,
      "pattern_type": "integration_hook_with_auth",
      "indexed_at": "2026-01-28T00:00:00.000000",
      "code_patterns": {
        "hook_import": "from airflow.hooks.base import BaseHook",
        "class_definition": "class RestApiHook(BaseHook):",
        "class_attributes": "conn_name_attr: str = 'conn_id'\nconn_type: str = 'http'\nhook_name: str = 'rest_api'",
        "get_conn_method": "def get_conn(self) -> requests.Session:\n    if self._session is not None:\n        return self._session\n    # Create and cache session",
        "auth_patterns": {
          "basic": "session.auth = (conn.login, conn.password)",
          "bearer": "session.headers['Authorization'] = f'Bearer {conn.password}'",
          "api_key": "session.headers[api_key_header] = api_key"
        },
        "retry_with_backoff": "for attempt in range(self.retry_count + 1):\n    try:\n        response = session.request(method, url, **kwargs)\n    except Exception:\n        time.sleep(2 ** attempt)  # Exponential backoff"
      },
      "relevance_keywords": [
        "hook",
        "integration",
        "rest api",
        "http",
        "requests",
        "authentication",
        "bearer",
        "api key",
        "basic auth",
        "retry",
        "connection"
      ]
    },
    {
      "id": "s3fileoperator_success_2026_01_28",
      "component_name": "S3FileOperator",
      "category": "cloud",
      "subcategory": "aws",
      "framework": "boto3",
      "component_type": "operator",
      "operator_code": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom typing import Dict, Any, Optional, Sequence\nfrom pathlib import Path\n\n\nclass S3FileOperator(BaseOperator):\n    \"\"\"\n    Operator for AWS S3 file operations including upload, download, copy, and delete.\n    \n    This operator supports various S3 operations:\n    - upload: Upload a local file to S3\n    - download: Download an S3 object to local file\n    - copy: Copy an S3 object to another location\n    - delete: Delete an S3 object\n    \n    Args:\n        bucket_name: S3 bucket name\n        key: S3 object key\n        operation: Operation type (upload, download, copy, delete)\n        local_path: Local file path for upload or download operations\n        dest_bucket: Destination bucket for copy operation\n        dest_key: Destination key for copy operation\n        aws_conn_id: Airflow connection ID for AWS credentials\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['bucket_name', 'key', 'operation', 'local_path', 'dest_bucket', 'dest_key']\n    ui_color: str = \"#ff9900\"\n    \n    def __init__(\n        self,\n        bucket_name: str,\n        key: str,\n        operation: str,\n        local_path: Optional[str] = None,\n        dest_bucket: Optional[str] = None,\n        dest_key: Optional[str] = None,\n        aws_conn_id: str = 'aws_default',\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        \n        self.bucket_name = bucket_name\n        self.key = key\n        self.operation = operation\n        self.local_path = local_path\n        self.dest_bucket = dest_bucket\n        self.dest_key = dest_key\n        self.aws_conn_id = aws_conn_id\n        \n        # Validate operation if not a template\n        valid_operations = ['upload', 'download', 'copy', 'delete']\n        if '{{' not in str(operation) and operation not in valid_operations:\n            raise AirflowException(f\"Invalid operation '{operation}'. Must be one of:",
      "metadata": {
        "component_name": "S3FileOperator",
        "component_type": "operator",
        "category": "cloud",
        "subcategory": "aws",
        "framework": "boto3",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "cloud_operator_pattern",
      "indexed_at": "2026-01-28T15:32:51.666809",
      "relevance_keywords": [
        "cloud",
        "aws",
        "operator",
        "boto3",
        "s3fileoperator"
      ]
    },
    {
      "id": "slacknotificationoperator_success_2026_01_28",
      "component_name": "SlackNotificationOperator",
      "category": "notification",
      "subcategory": "messaging",
      "framework": "slack-sdk",
      "component_type": "operator",
      "operator_code": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook\nfrom airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator\nfrom typing import Dict, Any, Optional, List, Sequence, Union\nimport json\n\n\nclass SlackNotificationOperator(BaseOperator):\n    \"\"\"\n    Operator for sending notifications to Slack channels with support for formatted messages and attachments.\n    \n    This operator sends messages to Slack channels using the Slack webhook API. It supports\n    custom usernames, emoji icons, and rich attachments for enhanced messaging capabilities.\n    \n    Args:\n        channel: Slack channel to send message to (e.g., '#general' or '@username')\n        message: Message text to send to the channel\n        username: Bot username to display (optional)\n        icon_emoji: Emoji icon for the bot (e.g., ':robot_face:') (optional)\n        attachments: List of Slack attachments for rich formatting (optional)\n        slack_conn_id: Airflow connection ID for Slack webhook (default: 'slack_default')\n    \n    Example:\n        slack_notification = SlackNotificationOperator(\n            task_id='send_slack_notification',\n            channel='#alerts',\n            message='Task completed successfully!',\n            username='Airflow Bot',\n            icon_emoji=':white_check_mark:',\n            slack_conn_id='slack_webhook'\n        )\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['channel', 'message', 'username', 'icon_emoji', 'attachments']\n    ui_color: str = \"#36C5F0\"\n    \n    def __init__(\n        self,\n        channel: str,\n        message: str,\n        username: Optional[str] = None,\n        icon_emoji: Optional[str] = None,\n        attachments: Optional[List[Dict[str, Any]]] = None,\n        slack_conn_id: str = 'slack_default',\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        \n        # Validate required parameters (skip if Jinja templ",
      "metadata": {
        "component_name": "SlackNotificationOperator",
        "component_type": "operator",
        "category": "notification",
        "subcategory": "messaging",
        "framework": "slack-sdk",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "notification_operator_pattern",
      "indexed_at": "2026-01-28T15:32:51.667477",
      "relevance_keywords": [
        "notification",
        "messaging",
        "operator",
        "slack-sdk",
        "slacknotificationoperator"
      ]
    },
    {
      "id": "postgresqueryoperator_success_2026_01_28",
      "component_name": "PostgresQueryOperator",
      "category": "database",
      "subcategory": "postgresql",
      "framework": "psycopg2",
      "component_type": "operator",
      "operator_code": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom airflow.providers.postgres.hooks.postgres import PostgresHook\nfrom typing import Dict, Any, Optional, Sequence, Union\nimport logging\n\nclass PostgresQueryOperator(BaseOperator):\n    \"\"\"\n    Operator for executing SQL queries on PostgreSQL databases with parameterized queries and result handling.\n    \n    This operator connects to a PostgreSQL database and executes SQL queries with support for\n    parameterized queries, transaction control, and result handling.\n    \n    Args:\n        sql (str): SQL query to execute\n        parameters (dict, optional): Query parameters for parameterized queries\n        autocommit (bool, optional): Whether to autocommit the transaction. Defaults to True\n        postgres_conn_id (str, optional): Airflow connection ID for PostgreSQL. Defaults to 'postgres_default'\n        database (str, optional): Database name to connect to. If not provided, uses connection default\n        **kwargs: Additional arguments passed to BaseOperator\n    \n    Example:\n        >>> operator = PostgresQueryOperator(\n        ...     task_id='run_query',\n        ...     sql=\"SELECT * FROM users WHERE city = %(city)s\",\n        ...     parameters={'city': 'New York'},\n        ...     postgres_conn_id='my_postgres_conn'\n        ... )\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['sql', 'parameters']\n    ui_color: str = \"#336791\"\n    \n    def __init__(\n        self,\n        sql: str,\n        parameters: Optional[Dict[str, Any]] = None,\n        autocommit: bool = True,\n        postgres_conn_id: str = 'postgres_default',\n        database: Optional[str] = None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        \n        # Validate sql parameter (skip if Jinja template)\n        if '{{' not in str(sql) and not sql.strip():\n            raise AirflowException(\"SQL query cannot be empty\")\n        \n        # Validate parameters (skip if Jinja template)\n        if para",
      "metadata": {
        "component_name": "PostgresQueryOperator",
        "component_type": "operator",
        "category": "database",
        "subcategory": "postgresql",
        "framework": "psycopg2",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "database_operator_pattern",
      "indexed_at": "2026-01-28T15:32:51.667904",
      "relevance_keywords": [
        "database",
        "postgresql",
        "operator",
        "psycopg2",
        "postgresqueryoperator"
      ]
    },
    {
      "id": "datavalidationoperator_success_2026_01_28",
      "component_name": "DataValidationOperator",
      "category": "data-quality",
      "subcategory": "validation",
      "framework": "pandas",
      "component_type": "operator",
      "operator_code": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, List, Sequence, Union\nimport pandas as pd\nimport re\nfrom pathlib import Path\nimport json\n\nclass DataValidationOperator(BaseOperator):\n    \"\"\"\n    Operator for validating data quality with configurable rules including null checks, \n    range validation, and regex patterns.\n    \n    This operator supports various validation rules:\n    - null_check: Check for null/missing values\n    - range_check: Validate numeric values within specified ranges\n    - regex_pattern: Validate string values against regex patterns\n    - unique_check: Check for duplicate values\n    - length_check: Validate string length constraints\n    \n    Args:\n        data_source (str): Path or connection to data source (CSV, JSON, or database connection)\n        validation_rules (list): List of validation rule dictionaries. Each rule should contain:\n            - column: Column name to validate\n            - rule_type: Type of validation (null_check, range_check, regex_pattern, unique_check, length_check)\n            - parameters: Rule-specific parameters\n        fail_on_error (bool): Whether to fail the task on validation errors. Default is True.\n        output_report (str): Optional path to save validation report as JSON\n    \n    Example validation_rules:\n        [\n            {\"column\": \"age\", \"rule_type\": \"range_check\", \"parameters\": {\"min\": 0, \"max\": 120}},\n            {\"column\": \"email\", \"rule_type\": \"regex_pattern\", \"parameters\": {\"pattern\": r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"}},\n            {\"column\": \"name\", \"rule_type\": \"null_check\", \"parameters\": {}},\n            {\"column\": \"id\", \"rule_type\": \"unique_check\", \"parameters\": {}}\n        ]\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['data_source', 'output_report', 'fail_on_error']\n    ui_color: str = \"#f0ede4\"\n    \n    def __init__(\n        self,\n        data_source: str,\n        validation_rules: ",
      "metadata": {
        "component_name": "DataValidationOperator",
        "component_type": "operator",
        "category": "data-quality",
        "subcategory": "validation",
        "framework": "pandas",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "data-quality_operator_pattern",
      "indexed_at": "2026-01-28T15:32:51.668403",
      "relevance_keywords": [
        "data-quality",
        "validation",
        "operator",
        "pandas",
        "datavalidationoperator"
      ]
    },
    {
      "id": "jsontransformoperator_success_2026_01_28",
      "component_name": "JsonTransformOperator",
      "category": "data-processing",
      "subcategory": "transformation",
      "framework": "jmespath",
      "component_type": "operator",
      "operator_code": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Sequence\nimport json\nimport jmespath\n\n\nclass JsonTransformOperator(BaseOperator):\n    \"\"\"\n    Operator for transforming JSON data using JMESPath expressions.\n    \n    This operator takes a JSON string and applies a JMESPath expression to transform\n    or extract data from it. JMESPath is a query language for JSON that allows you\n    to declaratively specify how to extract elements from a JSON document.\n    \n    Args:\n        input_json (str): Input JSON string to transform\n        expression (str): JMESPath expression to apply to the JSON data\n        \n    Returns:\n        Any: The result of applying the JMESPath expression to the input JSON\n        \n    Example:\n        >>> operator = JsonTransformOperator(\n        ...     task_id='transform_json',\n        ...     input_json='{\"users\": [{\"name\": \"John\", \"age\": 30}, {\"name\": \"Jane\", \"age\": 25}]}',\n        ...     expression='users[?age > `27`].name'\n        ... )\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['input_json', 'expression']\n    ui_color: str = \"#f0ede4\"\n    \n    def __init__(\n        self,\n        input_json: str,\n        expression: str,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        \n        # Store parameters\n        self.input_json = input_json\n        self.expression = expression\n        \n        # Validate non-template parameters or template parameters that don't contain Jinja\n        if '{{' not in str(input_json):\n            if not input_json or not input_json.strip():\n                raise AirflowException(\"input_json cannot be empty\")\n            \n            # Validate JSON format\n            try:\n                json.loads(input_json)\n            except json.JSONDecodeError as e:\n                raise AirflowException(f\"Invalid JSON format in input_json: {e}\")\n        \n        if '{{' not in str(expression):\n            if not expression or not expres",
      "metadata": {
        "component_name": "JsonTransformOperator",
        "component_type": "operator",
        "category": "data-processing",
        "subcategory": "transformation",
        "framework": "jmespath",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "data-processing_operator_pattern",
      "indexed_at": "2026-01-28T15:32:51.668764",
      "relevance_keywords": [
        "data-processing",
        "transformation",
        "operator",
        "jmespath",
        "jsontransformoperator"
      ]
    },
    {
      "id": "filecleanupoperator_success_2026_01_28",
      "component_name": "FileCleanupOperator",
      "category": "utility",
      "subcategory": "filesystem",
      "framework": "builtin",
      "component_type": "operator",
      "operator_code": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nimport os\nimport glob\nimport fnmatch\n\n\nclass FileCleanupOperator(BaseOperator):\n    \"\"\"\n    Operator for cleaning up temporary files and directories with age-based and pattern-based filtering.\n    \n    This operator can delete files based on age, pattern matching, and supports both recursive\n    and non-recursive directory traversal. It also supports dry-run mode for testing.\n    \n    Args:\n        directory (str): Directory to clean up\n        pattern (str, optional): File pattern to match for deletion (supports wildcards)\n        max_age_days (int, optional): Maximum age in days for files to keep\n        recursive (bool, optional): Whether to recursively clean subdirectories. Defaults to False\n        dry_run (bool, optional): If true, only log files that would be deleted. Defaults to False\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['directory', 'pattern', 'max_age_days', 'dry_run']\n    ui_color: str = \"#f0ede4\"\n    \n    def __init__(\n        self,\n        directory: str,\n        pattern: Optional[str] = None,\n        max_age_days: Optional[int] = None,\n        recursive: bool = False,\n        dry_run: bool = False,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        \n        self.directory = directory\n        self.pattern = pattern\n        self.max_age_days = max_age_days\n        self.recursive = recursive\n        self.dry_run = dry_run\n        \n        # Validate non-template fields\n        if not isinstance(recursive, bool):\n            raise AirflowException(f\"recursive must be a boolean, got {type(recursive)}\")\n        \n        # Validate template fields only if they don't contain Jinja templates\n        if '{{' not in str(directory) and not directory:\n            raise AirflowException(\"directory parameter cannot be empty\")\n            ",
      "metadata": {
        "component_name": "FileCleanupOperator",
        "component_type": "operator",
        "category": "utility",
        "subcategory": "filesystem",
        "framework": "builtin",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "utility_operator_pattern",
      "indexed_at": "2026-01-28T15:32:51.669074",
      "relevance_keywords": [
        "utility",
        "filesystem",
        "operator",
        "builtin",
        "filecleanupoperator"
      ]
    },
    {
      "id": "httprequestoperator_success_2026_01_28",
      "component_name": "HttpRequestOperator",
      "category": "integration",
      "subcategory": "http",
      "framework": "requests",
      "component_type": "operator",
      "operator_code": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence, Union\nimport requests\nfrom requests.auth import HTTPBasicAuth, HTTPDigestAuth\nimport json\n\n\nclass HttpRequestOperator(BaseOperator):\n    \"\"\"\n    Operator for making HTTP requests with support for all methods, headers, authentication, and response handling.\n    \n    This operator can make HTTP requests to any URL with configurable method, headers, data, timeout,\n    and SSL verification. It supports various authentication methods and provides comprehensive\n    response handling and logging.\n    \n    :param url: URL to send request to\n    :type url: str\n    :param method: HTTP method (GET, POST, PUT, DELETE, PATCH)\n    :type method: str\n    :param headers: HTTP headers to include in the request\n    :type headers: dict\n    :param data: Request body data (will be JSON serialized if dict)\n    :type data: dict\n    :param timeout: Request timeout in seconds\n    :type timeout: int\n    :param verify_ssl: Whether to verify SSL certificates\n    :type verify_ssl: bool\n    :param auth_type: Authentication type ('basic', 'digest', 'bearer')\n    :type auth_type: str\n    :param username: Username for authentication\n    :type username: str\n    :param password: Password for authentication\n    :type password: str\n    :param token: Bearer token for authentication\n    :type token: str\n    :param expected_status_codes: List of expected HTTP status codes for success\n    :type expected_status_codes: list\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['url', 'method', 'headers', 'data']\n    ui_color: str = \"#4CAF50\"\n    \n    def __init__(\n        self,\n        url: str,\n        method: str = 'GET',\n        headers: Optional[Dict[str, str]] = None,\n        data: Optional[Union[Dict[str, Any], str]] = None,\n        timeout: int = 30,\n        verify_ssl: bool = True,\n        auth_type: Optional[str] = None,\n        username: Optional[str] = None,\n       ",
      "metadata": {
        "component_name": "HttpRequestOperator",
        "component_type": "operator",
        "category": "integration",
        "subcategory": "http",
        "framework": "requests",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "integration_operator_pattern",
      "indexed_at": "2026-01-28T15:32:51.669493",
      "relevance_keywords": [
        "integration",
        "http",
        "operator",
        "requests",
        "httprequestoperator"
      ]
    },
    {
      "id": "s3keysensor_success_2026_01_28",
      "component_name": "S3KeySensor",
      "category": "cloud",
      "subcategory": "aws",
      "framework": "boto3",
      "component_type": "sensor",
      "sensor_code": "from airflow.sensors.base import BaseSensor\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence\nfrom datetime import datetime, timedelta\nimport fnmatch\n\n\nclass S3KeySensor(BaseSensor):\n    \"\"\"\n    Sensor that waits for a key to appear in an S3 bucket.\n    \n    This sensor polls an S3 bucket for the existence of a specific key.\n    It supports wildcard matching and custom check functions for flexible\n    key detection patterns.\n    \n    Args:\n        bucket_name (str): S3 bucket name to monitor\n        bucket_key (str): S3 key to wait for\n        wildcard_match (bool): Whether to use wildcard matching for key. Defaults to False\n        aws_conn_id (str): Airflow connection ID for AWS. Defaults to 'aws_default'\n        check_fn (str): Custom check function name. Defaults to None\n        poke_interval (int): Time in seconds between pokes. Defaults to 60\n        timeout (int): Maximum time to wait in seconds. Defaults to 3600\n        mode (str): How the sensor operates ('poke' or 'reschedule'). Defaults to 'poke'\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['bucket_name', 'bucket_key']\n    ui_color: str = \"#f0ede4\"\n    \n    def __init__(\n        self,\n        bucket_name: str,\n        bucket_key: str,\n        wildcard_match: bool = False,\n        aws_conn_id: str = 'aws_default',\n        check_fn: Optional[str] = None,\n        poke_interval: int = 60,\n        timeout: int = 3600,\n        mode: str = 'poke',\n        **kwargs\n    ):\n        super().__init__(\n            poke_interval=poke_interval,\n            timeout=timeout,\n            mode=mode,\n            **kwargs\n        )\n        self.bucket_name = bucket_name\n        self.bucket_key = bucket_key\n        self.wildcard_match = wildcard_match\n        self.aws_conn_id = aws_conn_id\n        self.check_fn = check_fn\n        self._s3_hook = None\n    \n    @property\n    def s3_hook(self) -> S3Hook:\n        \"\"\"Lazy i",
      "metadata": {
        "component_name": "S3KeySensor",
        "component_type": "sensor",
        "category": "cloud",
        "subcategory": "aws",
        "framework": "boto3",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "cloud_sensor_pattern",
      "indexed_at": "2026-01-28T15:32:51.669856",
      "relevance_keywords": [
        "cloud",
        "aws",
        "sensor",
        "boto3",
        "s3keysensor"
      ]
    },
    {
      "id": "databaserecordsensor_success_2026_01_28",
      "component_name": "DatabaseRecordSensor",
      "category": "database",
      "subcategory": "sql",
      "framework": "sqlalchemy",
      "component_type": "sensor",
      "sensor_code": "from airflow.sensors.base import BaseSensor\nfrom airflow.hooks.base import BaseHook\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence\nimport sqlparse\n\nclass DatabaseRecordSensor(BaseSensor):\n    \"\"\"\n    Sensor that waits for a record to exist in a database table based on SQL condition.\n    \n    This sensor polls a database table and checks if records matching the specified\n    SQL condition exist. The sensor succeeds when the expected number of records\n    is found.\n    \n    Args:\n        table (str): Database table name to query\n        sql_condition (str): SQL WHERE condition to check for records\n        conn_id (str, optional): Airflow connection ID for database. Defaults to 'default_db'\n        expected_count (int, optional): Minimum number of records expected. Defaults to 1\n        poke_interval (int, optional): Time in seconds between pokes. Defaults to 60\n        timeout (int, optional): Time in seconds before timing out. Defaults to 3600\n        mode (str, optional): How the sensor operates ('poke' or 'reschedule'). Defaults to 'poke'\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['table', 'sql_condition', 'conn_id']\n    ui_color: str = \"#87CEEB\"\n    \n    def __init__(\n        self,\n        table: str,\n        sql_condition: str,\n        conn_id: str = 'default_db',\n        expected_count: int = 1,\n        poke_interval: int = 60,\n        timeout: int = 3600,\n        mode: str = 'poke',\n        **kwargs\n    ):\n        super().__init__(\n            poke_interval=poke_interval,\n            timeout=timeout,\n            mode=mode,\n            **kwargs\n        )\n        self.table = table\n        self.sql_condition = sql_condition\n        self.conn_id = conn_id\n        self.expected_count = expected_count\n        self._hook = None\n    \n    def _get_hook(self):\n        \"\"\"Get database hook based on connection type.\"\"\"\n        if self._hook is None:\n            try:\n                connection = BaseHook.get_connect",
      "metadata": {
        "component_name": "DatabaseRecordSensor",
        "component_type": "sensor",
        "category": "database",
        "subcategory": "sql",
        "framework": "sqlalchemy",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "database_sensor_pattern",
      "indexed_at": "2026-01-28T15:32:51.670137",
      "relevance_keywords": [
        "database",
        "sql",
        "sensor",
        "sqlalchemy",
        "databaserecordsensor"
      ]
    },
    {
      "id": "rediskeysensor_success_2026_01_28",
      "component_name": "RedisKeySensor",
      "category": "cache",
      "subcategory": "redis",
      "framework": "redis",
      "component_type": "sensor",
      "sensor_code": "from airflow.sensors.base import BaseSensor\nfrom airflow.exceptions import AirflowException\nfrom airflow.hooks.base import BaseHook\nfrom typing import Dict, Any, Optional, Sequence\nimport redis\n\n\nclass RedisKeySensor(BaseSensor):\n    \"\"\"\n    Sensor that waits for a key to exist in Redis with optional value checking.\n    \n    This sensor will poke Redis at regular intervals to check if a specified key exists.\n    Optionally, it can also verify that the key contains an expected value.\n    \n    Args:\n        key: Redis key to wait for\n        expected_value: Expected value for the key (optional)\n        redis_conn_id: Airflow connection ID for Redis (default: 'redis_default')\n        db: Redis database number (default: 0)\n        poke_interval: Time in seconds between pokes (default: 60)\n        timeout: Maximum time to wait in seconds (default: 60*60*24*7)\n        mode: How the sensor operates - 'poke' or 'reschedule' (default: 'poke')\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['key', 'expected_value']\n    ui_color: str = \"#dc143c\"\n    \n    def __init__(\n        self,\n        key: str,\n        expected_value: Optional[str] = None,\n        redis_conn_id: str = 'redis_default',\n        db: int = 0,\n        poke_interval: int = 60,\n        timeout: int = 60 * 60 * 24 * 7,\n        mode: str = 'poke',\n        **kwargs\n    ):\n        super().__init__(\n            poke_interval=poke_interval,\n            timeout=timeout,\n            mode=mode,\n            **kwargs\n        )\n        self.key = key\n        self.expected_value = expected_value\n        self.redis_conn_id = redis_conn_id\n        self.db = db\n        self._redis_client = None\n    \n    def _get_redis_client(self) -> redis.Redis:\n        \"\"\"\n        Get Redis client using Airflow connection.\n        \n        Returns:\n            redis.Redis: Redis client instance\n        \"\"\"\n        if self._redis_client is None:\n            try:\n                connection = BaseHook.get_connection(self.redis_conn_id)\n      ",
      "metadata": {
        "component_name": "RedisKeySensor",
        "component_type": "sensor",
        "category": "cache",
        "subcategory": "redis",
        "framework": "redis",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "cache_sensor_pattern",
      "indexed_at": "2026-01-28T15:32:51.670381",
      "relevance_keywords": [
        "cache",
        "redis",
        "sensor",
        "redis",
        "rediskeysensor"
      ]
    },
    {
      "id": "filepatternsensor_success_2026_01_28",
      "component_name": "FilePatternSensor",
      "category": "utility",
      "subcategory": "filesystem",
      "framework": "builtin",
      "component_type": "sensor",
      "sensor_code": "from airflow.sensors.base import BaseSensor\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence\nfrom pathlib import Path\nimport glob\n\n\nclass FilePatternSensor(BaseSensor):\n    \"\"\"\n    Sensor that waits for files matching a glob pattern to appear in a directory.\n    \n    This sensor monitors a specified directory for files matching a given glob pattern.\n    It can optionally check for a minimum number of files and verify that files\n    have content (size > 0).\n    \n    Args:\n        directory (str): Directory to monitor for files\n        pattern (str): Glob pattern to match files (e.g., '*.txt', 'data_*.csv')\n        min_files (int, optional): Minimum number of files that must match. Defaults to 1\n        check_size (bool, optional): Whether to check that files have size > 0. Defaults to True\n        poke_interval (int, optional): Time in seconds between pokes. Defaults to 60\n        timeout (int, optional): Maximum time to wait in seconds. Defaults to 3600 (1 hour)\n        mode (str, optional): How the sensor operates ('poke' or 'reschedule'). Defaults to 'poke'\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['directory', 'pattern']\n    ui_color: str = \"#f0ede4\"\n    \n    def __init__(\n        self,\n        directory: str,\n        pattern: str,\n        min_files: int = 1,\n        check_size: bool = True,\n        poke_interval: int = 60,\n        timeout: int = 3600,\n        mode: str = 'poke',\n        **kwargs\n    ):\n        super().__init__(\n            poke_interval=poke_interval,\n            timeout=timeout,\n            mode=mode,\n            **kwargs\n        )\n        self.directory = directory\n        self.pattern = pattern\n        self.min_files = min_files\n        self.check_size = check_size\n    \n    def poke(self, context: Dict[str, Any]) -> bool:\n        \"\"\"\n        Check if files matching the pattern exist in the directory.\n        \n        Args:\n            context: Airflow context dictionary\n            \n   ",
      "metadata": {
        "component_name": "FilePatternSensor",
        "component_type": "sensor",
        "category": "utility",
        "subcategory": "filesystem",
        "framework": "builtin",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "utility_sensor_pattern",
      "indexed_at": "2026-01-28T15:32:51.670592",
      "relevance_keywords": [
        "utility",
        "filesystem",
        "sensor",
        "builtin",
        "filepatternsensor"
      ]
    },
    {
      "id": "sqsmessagesensor_success_2026_01_28",
      "component_name": "SqsMessageSensor",
      "category": "cloud",
      "subcategory": "aws",
      "framework": "boto3",
      "component_type": "sensor",
      "sensor_code": "from airflow.sensors.base import BaseSensor\nfrom airflow.exceptions import AirflowException\nfrom airflow.providers.amazon.aws.hooks.sqs import SqsHook\nfrom typing import Dict, Any, Optional, Sequence\nfrom datetime import datetime, timedelta\n\n\nclass SqsMessageSensor(BaseSensor):\n    \"\"\"\n    Sensor that waits for a message to arrive in an AWS SQS queue.\n    \n    This sensor polls an SQS queue at regular intervals and succeeds when\n    at least one message is available in the queue.\n    \n    Args:\n        queue_url: SQS queue URL to monitor\n        max_messages: Maximum number of messages to receive (1-10)\n        wait_time_seconds: Long polling wait time in seconds (0-20)\n        visibility_timeout: Message visibility timeout in seconds\n        aws_conn_id: Airflow connection ID for AWS credentials\n        poke_interval: Time in seconds between pokes\n        timeout: Maximum time to wait for messages\n        mode: Sensor mode ('poke' or 'reschedule')\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['queue_url']\n    ui_color: str = \"#ff9900\"\n    \n    def __init__(\n        self,\n        queue_url: str,\n        max_messages: int = 1,\n        wait_time_seconds: int = 0,\n        visibility_timeout: Optional[int] = None,\n        aws_conn_id: str = 'aws_default',\n        poke_interval: int = 30,\n        timeout: int = 60 * 60 * 24,\n        mode: str = 'poke',\n        **kwargs\n    ):\n        super().__init__(\n            poke_interval=poke_interval,\n            timeout=timeout,\n            mode=mode,\n            **kwargs\n        )\n        self.queue_url = queue_url\n        self.max_messages = max_messages\n        self.wait_time_seconds = wait_time_seconds\n        self.visibility_timeout = visibility_timeout\n        self.aws_conn_id = aws_conn_id\n        self._sqs_hook = None\n    \n    def _get_sqs_hook(self) -> SqsHook:\n        \"\"\"Get SQS hook with lazy initialization.\"\"\"\n        if self._sqs_hook is None:\n            self._sqs_hook = SqsHook(aws_conn_id=self.aws_conn_id)\n  ",
      "metadata": {
        "component_name": "SqsMessageSensor",
        "component_type": "sensor",
        "category": "cloud",
        "subcategory": "aws",
        "framework": "boto3",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "cloud_sensor_pattern",
      "indexed_at": "2026-01-28T15:32:51.670797",
      "relevance_keywords": [
        "cloud",
        "aws",
        "sensor",
        "boto3",
        "sqsmessagesensor"
      ]
    },
    {
      "id": "emailnotificationoperator_success_2026_01_28",
      "component_name": "EmailNotificationOperator",
      "category": "notification",
      "subcategory": "email",
      "framework": "smtplib",
      "component_type": "operator",
      "operator_code": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom airflow.hooks.base import BaseHook\nfrom airflow.utils.email import send_email\nfrom typing import Dict, Any, Optional, Sequence\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.base import MIMEBase\nfrom email import encoders\nimport os\nfrom pathlib import Path\n\n\nclass EmailNotificationOperator(BaseOperator):\n    \"\"\"\n    Operator for sending email notifications with HTML support and attachments.\n    \n    This operator sends email notifications using SMTP configuration from Airflow connections.\n    Supports both plain text and HTML email bodies with optional file attachments.\n    \n    Args:\n        to (str): Recipient email address\n        subject (str): Email subject line\n        body (str): Email body content (plain text or HTML)\n        html (bool, optional): Whether body content is HTML format. Defaults to False.\n        smtp_conn_id (str, optional): Airflow connection ID for SMTP configuration. \n                                     Defaults to 'smtp_default'.\n        attachments (list, optional): List of file paths to attach to email\n        cc (str, optional): CC email addresses (comma-separated)\n        bcc (str, optional): BCC email addresses (comma-separated)\n    \n    Example:\n        email_task = EmailNotificationOperator(\n            task_id='send_notification',\n            to='user@example.com',\n            subject='Task Completed',\n            body='<h1>Task finished successfully</h1>',\n            html=True,\n            smtp_conn_id='my_smtp_conn'\n        )\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['to', 'subject', 'body', 'cc', 'bcc']\n    ui_color: str = \"#e8f4fd\"\n    \n    def __init__(\n        self,\n        to: str,\n        subject: str,\n        body: str,\n        html: bool = False,\n        smtp_conn_id: str = 'smtp_default',\n        attachments: Optional[list] = None,\n        cc: Optiona",
      "metadata": {
        "component_name": "EmailNotificationOperator",
        "component_type": "operator",
        "category": "notification",
        "subcategory": "email",
        "framework": "smtplib",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "notification_operator_pattern",
      "indexed_at": "2026-01-28T15:32:51.671051",
      "relevance_keywords": [
        "notification",
        "email",
        "operator",
        "smtplib",
        "emailnotificationoperator"
      ]
    },
    {
      "id": "bashcommandoperator_success_2026_01_28",
      "component_name": "BashCommandOperator",
      "category": "utility",
      "subcategory": "shell",
      "framework": "subprocess",
      "component_type": "operator",
      "operator_code": "from airflow.models import BaseOperator\nfrom airflow.exceptions import AirflowException\nfrom typing import Dict, Any, Optional, Sequence\nimport subprocess\nimport os\nfrom pathlib import Path\n\n\nclass BashCommandOperator(BaseOperator):\n    \"\"\"\n    Operator for executing bash commands with environment variables and working directory support.\n    \n    This operator executes bash commands in a subprocess with optional environment variables,\n    working directory specification, and timeout control.\n    \n    Args:\n        bash_command (str): Bash command to execute\n        env (dict, optional): Environment variables for the command\n        cwd (str, optional): Working directory for command execution\n        timeout (int, optional): Command timeout in seconds\n    \"\"\"\n    \n    template_fields: Sequence[str] = ['bash_command']\n    ui_color: str = \"#f0ede4\"\n    \n    def __init__(\n        self,\n        bash_command: str,\n        env: Optional[Dict[str, str]] = None,\n        cwd: Optional[str] = None,\n        timeout: Optional[int] = None,\n        **kwargs\n    ):\n        super().__init__(**kwargs)\n        \n        # Store parameters\n        self.bash_command = bash_command\n        self.env = env or {}\n        self.cwd = cwd\n        self.timeout = timeout\n        \n        # Validate non-template fields\n        if self.env and not isinstance(self.env, dict):\n            raise AirflowException(\"env parameter must be a dictionary\")\n            \n        if self.cwd and '{{' not in str(self.cwd):\n            cwd_path = Path(self.cwd)\n            if not cwd_path.exists():\n                raise AirflowException(f\"Working directory does not exist: {self.cwd}\")\n            if not cwd_path.is_dir():\n                raise AirflowException(f\"Working directory path is not a directory: {self.cwd}\")\n                \n        if self.timeout is not None:\n            if not isinstance(self.timeout, int) or self.timeout <= 0:\n                raise AirflowException(\"timeout must be a positive integer",
      "metadata": {
        "component_name": "BashCommandOperator",
        "component_type": "operator",
        "category": "utility",
        "subcategory": "shell",
        "framework": "subprocess",
        "generation_date": "2026-01-28",
        "generation_status": "success"
      },
      "success_score": 165,
      "pattern_type": "utility_operator_pattern",
      "indexed_at": "2026-01-28T15:32:51.671352",
      "relevance_keywords": [
        "utility",
        "shell",
        "operator",
        "subprocess",
        "bashcommandoperator"
      ]
    },
    {
      "id": "official_appflowbaseoperator_20260128_160330",
      "component_name": "AppflowBaseOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AppflowBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "flow_name",
          "type": "str",
          "required": true
        },
        {
          "name": "flow_update",
          "type": "bool",
          "required": true
        },
        {
          "name": "source",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "source_field",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter_date",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, cast\nfrom airflow.providers.amazon.aws.hooks.appflow import AppflowHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils import datetime_to_epoch_ms\nfrom airflow.providers.amazon.aws.utils.mixins import AwsBaseHookMixin, AwsHookParams, aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.common.compat.standard.operators import ShortCircuitOperator",
        "class_attributes": [
          "aws_hook_class",
          "ui_color",
          "template_fields",
          "UPDATE_PROPAGATION_TIME"
        ],
        "methods": [
          "__init__",
          "execute",
          "_get_connector_type",
          "_update_flow",
          "_run_flow"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Amazon AppFlow Base Operator class (not supposed to be used directly in DAGs).\n\n:param source: The source name (Supported: salesforce, zendesk)\n:param flow_name: The flow name\n:param flow_update: A boolean to enable/disable a flow update before the run\n:param source_field: The field name to apply filters\n:param filter_date: The date value (or template) to be used in filters.\n:param poll_interval: how often in seconds to check the query status\n:param max_attempts: how many times to check for stat",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.789960",
      "relevance_keywords": [
        "appflowbaseoperator",
        "operator",
        "appflow",
        "base",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_appflowrunoperator_20260128_160330",
      "component_name": "AppflowRunOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AppflowRunOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AppflowBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "flow_name",
          "type": "str",
          "required": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AppflowBaseOperator"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, cast\nfrom airflow.providers.amazon.aws.hooks.appflow import AppflowHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils import datetime_to_epoch_ms\nfrom airflow.providers.amazon.aws.utils.mixins import AwsBaseHookMixin, AwsHookParams, aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.common.compat.standard.operators import ShortCircuitOperator",
        "class_attributes": [],
        "methods": [
          "__init__"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Execute an AppFlow run as is.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AppflowRunOperator`\n\n:param flow_name: The flow name\n:param poll_interval: how often in seconds to check the query status\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is None or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.790500",
      "relevance_keywords": [
        "operator",
        "appflowrunoperator",
        "appflow",
        "run",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_appflowrunfulloperator_20260128_160330",
      "component_name": "AppflowRunFullOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AppflowRunFullOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AppflowBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source",
          "type": "str",
          "required": true
        },
        {
          "name": "flow_name",
          "type": "str",
          "required": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AppflowBaseOperator"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, cast\nfrom airflow.providers.amazon.aws.hooks.appflow import AppflowHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils import datetime_to_epoch_ms\nfrom airflow.providers.amazon.aws.utils.mixins import AwsBaseHookMixin, AwsHookParams, aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.common.compat.standard.operators import ShortCircuitOperator",
        "class_attributes": [],
        "methods": [
          "__init__"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Execute an AppFlow full run removing any filter.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AppflowRunFullOperator`\n\n:param source: The source name (Supported: salesforce, zendesk)\n:param flow_name: The flow name\n:param poll_interval: how often in seconds to check the query status\n:param wait_for_completion: whether to wait for the run to end to return",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.790823",
      "relevance_keywords": [
        "operator",
        "appflow",
        "appflowrunfulloperator",
        "run",
        "amazon",
        "s3",
        "full",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_appflowrunbeforeoperator_20260128_160330",
      "component_name": "AppflowRunBeforeOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AppflowRunBeforeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AppflowBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source",
          "type": "str",
          "required": true
        },
        {
          "name": "flow_name",
          "type": "str",
          "required": true
        },
        {
          "name": "source_field",
          "type": "str",
          "required": true
        },
        {
          "name": "filter_date",
          "type": "str",
          "required": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AppflowBaseOperator"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, cast\nfrom airflow.providers.amazon.aws.hooks.appflow import AppflowHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils import datetime_to_epoch_ms\nfrom airflow.providers.amazon.aws.utils.mixins import AwsBaseHookMixin, AwsHookParams, aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.common.compat.standard.operators import ShortCircuitOperator",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_update_flow"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Execute an AppFlow run after updating the filters to select only previous data.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AppflowRunBeforeOperator`\n\n:param source: The source name (Supported: salesforce)\n:param flow_name: The flow name\n:param source_field: The field name to apply filters\n:param filter_date: The date value (or template) to be used in filters.\n:param poll_interval: how often in seconds to check the query ",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.791127",
      "relevance_keywords": [
        "operator",
        "before",
        "appflow",
        "appflowrunbeforeoperator",
        "run",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_appflowrunafteroperator_20260128_160330",
      "component_name": "AppflowRunAfterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AppflowRunAfterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AppflowBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source",
          "type": "str",
          "required": true
        },
        {
          "name": "flow_name",
          "type": "str",
          "required": true
        },
        {
          "name": "source_field",
          "type": "str",
          "required": true
        },
        {
          "name": "filter_date",
          "type": "str",
          "required": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AppflowBaseOperator"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, cast\nfrom airflow.providers.amazon.aws.hooks.appflow import AppflowHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils import datetime_to_epoch_ms\nfrom airflow.providers.amazon.aws.utils.mixins import AwsBaseHookMixin, AwsHookParams, aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.common.compat.standard.operators import ShortCircuitOperator",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_update_flow"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Execute an AppFlow run after updating the filters to select only future data.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AppflowRunAfterOperator`\n\n:param source: The source name (Supported: salesforce, zendesk)\n:param flow_name: The flow name\n:param source_field: The field name to apply filters\n:param filter_date: The date value (or template) to be used in filters.\n:param poll_interval: how often in seconds to check the ",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.791479",
      "relevance_keywords": [
        "operator",
        "after",
        "appflowrunafteroperator",
        "appflow",
        "run",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_appflowrundailyoperator_20260128_160330",
      "component_name": "AppflowRunDailyOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AppflowRunDailyOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AppflowBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source",
          "type": "str",
          "required": true
        },
        {
          "name": "flow_name",
          "type": "str",
          "required": true
        },
        {
          "name": "source_field",
          "type": "str",
          "required": true
        },
        {
          "name": "filter_date",
          "type": "str",
          "required": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AppflowBaseOperator"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, cast\nfrom airflow.providers.amazon.aws.hooks.appflow import AppflowHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils import datetime_to_epoch_ms\nfrom airflow.providers.amazon.aws.utils.mixins import AwsBaseHookMixin, AwsHookParams, aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.common.compat.standard.operators import ShortCircuitOperator",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_update_flow"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Execute an AppFlow run after updating the filters to select only a single day.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AppflowRunDailyOperator`\n\n:param source: The source name (Supported: salesforce)\n:param flow_name: The flow name\n:param source_field: The field name to apply filters\n:param filter_date: The date value (or template) to be used in filters.\n:param poll_interval: how often in seconds to check the query st",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.791821",
      "relevance_keywords": [
        "operator",
        "appflow",
        "run",
        "amazon",
        "s3",
        "appflowrundailyoperator",
        "cloud",
        "daily",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_appflowrecordsshortcircuitoperator_20260128_160330",
      "component_name": "AppflowRecordsShortCircuitOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AppflowRecordsShortCircuitOperator",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "ShortCircuitOperator",
          "AwsBaseHookMixin"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "flow_name",
          "type": "str",
          "required": false
        },
        {
          "name": "appflow_run_task_id",
          "type": "str",
          "required": false
        },
        {
          "name": "ignore_downstream_trigger_rules",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        },
        {
          "name": "region_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "verify",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "botocore_config",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ShortCircuitOperator, AwsBaseHookMixin",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, cast\nfrom airflow.providers.amazon.aws.hooks.appflow import AppflowHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils import datetime_to_epoch_ms\nfrom airflow.providers.amazon.aws.utils.mixins import AwsBaseHookMixin, AwsHookParams, aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.common.compat.standard.operators import ShortCircuitOperator",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "_get_target_execution_id",
          "_has_new_records_func"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Short-circuit in case of an empty AppFlow's run.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AppflowRecordsShortCircuitOperator`\n\n:param flow_name: The flow name\n:param appflow_run_task_id: Run task ID from where this operator should extract the execution ID\n:param ignore_downstream_trigger_rules: Ignore downstream trigger rules\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` o",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:19.792109",
      "relevance_keywords": [
        "operator",
        "appflow",
        "circuit",
        "appflowrecordsshortcircuitoperator",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "records",
        "hook",
        "ec2",
        "short"
      ]
    },
    {
      "id": "official_athenaoperator_20260128_160330",
      "component_name": "AthenaOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AthenaOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "query",
          "type": "str",
          "required": false
        },
        {
          "name": "database",
          "type": "str",
          "required": false
        },
        {
          "name": "output_location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "client_request_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "workgroup",
          "type": "str",
          "required": false,
          "default": "primary"
        },
        {
          "name": "query_execution_context",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "result_configuration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sleep_time",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "max_polling_attempts",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "log_query",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "catalog",
          "type": "str",
          "required": false,
          "default": "AwsDataCatalog"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import urlparse\nfrom airflow.providers.amazon.aws.hooks.athena import AthenaHook\nfrom airflow.providers.amazon.aws.links.athena import AthenaQueryResultsLink\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.athena import AthenaTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "ui_color",
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "execute",
          "execute_complete",
          "on_kill",
          "get_openlineage_facets_on_complete",
          "get_openlineage_dataset"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "An operator that submits a Trino/Presto query to Amazon Athena.\n\n.. note:: if the task is killed while it runs, it'll cancel the athena query that was launched,\n    EXCEPT if running in deferrable mode.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AthenaOperator`\n\n:param query: Trino/Presto query to be run on Amazon Athena. (templated)\n:param database: Database to select. (templated)\n:param catalog: Catalog to select. (tem",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.818982",
      "relevance_keywords": [
        "operator",
        "athenaoperator",
        "amazon",
        "s3",
        "cloud",
        "athena",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_awsbaseoperator_20260128_160330",
      "component_name": "AwsBaseOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AwsBaseOperator",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BaseOperator",
          "AwsBaseHookMixin"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        },
        {
          "name": "region_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "verify",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "botocore_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator, AwsBaseHookMixin",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom airflow.providers.amazon.aws.utils.mixins import (\nfrom airflow.providers.amazon.version_compat import NOTSET, ArgNotSet\nfrom airflow.providers.common.compat.sdk import BaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Base AWS (Amazon) Operator Class to build operators on top of AWS Hooks.\n\n.. warning::\n    Only for internal usage, this class might be changed, renamed or removed in the future\n    without any further notice.\n\nExamples:\n .. code-block:: python\n\n    from airflow.providers.amazon.aws.hooks.foo_bar import FooBarThinHook, FooBarThickHook\n\n\n    class AwsFooBarOperator(AwsBaseOperator[FooBarThinHook]):\n        aws_hook_class = FooBarThinHook\n\n        def execute(self, context):\n            pass\n\n\n   ",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:19.839465",
      "relevance_keywords": [
        "operator",
        "hook",
        "base",
        "amazon",
        "s3",
        "awsbaseoperator",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_batchoperator_20260128_160330",
      "component_name": "BatchOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BatchOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "job_definition",
          "type": "str",
          "required": false
        },
        {
          "name": "job_queue",
          "type": "str",
          "required": false
        },
        {
          "name": "container_overrides",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "array_properties",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ecs_properties_override",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "eks_properties_override",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "node_overrides",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "share_identifier",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "scheduling_priority_override",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "parameters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry_strategy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "waiters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 4200
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import timedelta",
        "class_attributes": [
          "aws_hook_class",
          "ui_color",
          "arn",
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "execute",
          "execute_complete",
          "on_kill",
          "submit_job",
          "monitor_job",
          "_get_batch_log_fetcher"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Execute a job on AWS Batch.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BatchOperator`\n\n:param job_name: the name for the job that will run on AWS Batch (templated)\n:param job_definition: the job definition name on AWS Batch\n:param job_queue: the queue name on AWS Batch\n:param container_overrides: the `containerOverrides` parameter for boto3 (templated)\n:param ecs_properties_override: the `ecsPropertiesOverride` parameter",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.861404",
      "relevance_keywords": [
        "batch",
        "operator",
        "batchoperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_batchcreatecomputeenvironmentoperator_20260128_160330",
      "component_name": "BatchCreateComputeEnvironmentOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BatchCreateComputeEnvironmentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "compute_environment_name",
          "type": "str",
          "required": true
        },
        {
          "name": "environment_type",
          "type": "str",
          "required": true
        },
        {
          "name": "state",
          "type": "str",
          "required": true
        },
        {
          "name": "compute_resources",
          "type": "dict",
          "required": true
        },
        {
          "name": "unmanaged_v_cpus",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "service_role",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "max_retries",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import timedelta",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create an AWS Batch compute environment.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BatchCreateComputeEnvironmentOperator`\n\n:param compute_environment_name: Name of the AWS batch compute\n    environment (templated).\n:param environment_type: Type of the compute-environment.\n:param state: State of the compute-environment.\n:param compute_resources: Details about the resources managed by the\n    compute-environment (template",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.861691",
      "relevance_keywords": [
        "batch",
        "operator",
        "compute",
        "create",
        "environment",
        "amazon",
        "s3",
        "batchcreatecomputeenvironmentoperator",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockinvokemodeloperator_20260128_160330",
      "component_name": "BedrockInvokeModelOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockInvokeModelOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "model_id",
          "type": "str",
          "required": true
        },
        {
          "name": "input_data",
          "type": "dict[...]",
          "required": true
        },
        {
          "name": "content_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "accept_type",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.bedrock import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Invoke the specified Bedrock model to run inference using the input provided.\n\nUse InvokeModel to run inference for text models, image models, and embedding models.\nTo see the format and content of the input_data field for different models, refer to\n`Inference parameters docs <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>`_.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BedrockInvokeModelOperat",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.899537",
      "relevance_keywords": [
        "model",
        "operator",
        "bedrockinvokemodeloperator",
        "bedrock",
        "amazon",
        "s3",
        "invoke",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockcustomizemodeloperator_20260128_160330",
      "component_name": "BedrockCustomizeModelOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockCustomizeModelOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "custom_model_name",
          "type": "str",
          "required": true
        },
        {
          "name": "role_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "base_model_id",
          "type": "str",
          "required": true
        },
        {
          "name": "training_data_uri",
          "type": "str",
          "required": true
        },
        {
          "name": "output_data_uri",
          "type": "str",
          "required": true
        },
        {
          "name": "hyperparameters",
          "type": "dict[...]",
          "required": true
        },
        {
          "name": "ensure_unique_job_name",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "customization_job_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 75
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.bedrock import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute_complete",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a fine-tuning job to customize a base model.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BedrockCustomizeModelOperator`\n\n:param job_name: A unique name for the fine-tuning job.\n:param custom_model_name: A name for the custom model being created.\n:param role_arn: The Amazon Resource Name (ARN) of an IAM role that Amazon Bedrock can assume\n    to perform tasks on your behalf.\n:param base_model_id: Name of the base mo",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.899888",
      "relevance_keywords": [
        "model",
        "operator",
        "customize",
        "bedrock",
        "amazon",
        "s3",
        "bedrockcustomizemodeloperator",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockcreateprovisionedmodelthroughputoperator_20260128_160330",
      "component_name": "BedrockCreateProvisionedModelThroughputOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockCreateProvisionedModelThroughputOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "model_units",
          "type": "int",
          "required": true
        },
        {
          "name": "provisioned_model_name",
          "type": "str",
          "required": true
        },
        {
          "name": "model_id",
          "type": "str",
          "required": true
        },
        {
          "name": "create_throughput_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.bedrock import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a fine-tuning job to customize a base model.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BedrockCreateProvisionedModelThroughputOperator`\n\n:param model_units: Number of model units to allocate. (templated)\n:param provisioned_model_name: Unique name for this provisioned throughput. (templated)\n:param model_id: Name or ARN of the model to associate with this provisioned throughput. (templated)\n:param create_throughpu",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.900347",
      "relevance_keywords": [
        "throughput",
        "model",
        "operator",
        "create",
        "bedrock",
        "amazon",
        "s3",
        "bedrockcreateprovisionedmodelthroughputoperator",
        "ec2",
        "cloud",
        "aws",
        "provisioned"
      ]
    },
    {
      "id": "official_bedrockcreateknowledgebaseoperator_20260128_160330",
      "component_name": "BedrockCreateKnowledgeBaseOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockCreateKnowledgeBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": true
        },
        {
          "name": "embedding_model_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "role_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "storage_config",
          "type": "dict[...]",
          "required": true
        },
        {
          "name": "create_knowledge_base_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_indexing",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "indexing_error_retry_delay",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "indexing_error_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.bedrock import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute_complete",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a knowledge base that contains data sources used by Amazon Bedrock LLMs and Agents.\n\nTo create a knowledge base, you must first set up your data sources and configure a supported vector store.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BedrockCreateKnowledgeBaseOperator`\n\n:param name: The name of the knowledge base. (templated)\n:param embedding_model_arn: ARN of the model used to create vector embeddings for the k",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.900796",
      "relevance_keywords": [
        "bedrockcreateknowledgebaseoperator",
        "operator",
        "create",
        "bedrock",
        "base",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "knowledge",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockcreatedatasourceoperator_20260128_160330",
      "component_name": "BedrockCreateDataSourceOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockCreateDataSourceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": true
        },
        {
          "name": "knowledge_base_id",
          "type": "str",
          "required": true
        },
        {
          "name": "bucket_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "create_data_source_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.bedrock import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Set up an Amazon Bedrock Data Source to be added to an Amazon Bedrock Knowledge Base.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BedrockCreateDataSourceOperator`\n\n:param name: name for the Amazon Bedrock Data Source being created. (templated).\n:param bucket_name: The name of the Amazon S3 bucket to use for data source storage. (templated)\n:param knowledge_base_id: The unique identifier of the knowledge base to which to a",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.901179",
      "relevance_keywords": [
        "operator",
        "bedrockcreatedatasourceoperator",
        "create",
        "bedrock",
        "data",
        "amazon",
        "source",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockingestdataoperator_20260128_160330",
      "component_name": "BedrockIngestDataOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockIngestDataOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "knowledge_base_id",
          "type": "str",
          "required": true
        },
        {
          "name": "data_source_id",
          "type": "str",
          "required": true
        },
        {
          "name": "ingest_data_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.bedrock import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute_complete",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Begin an ingestion job, in which an Amazon Bedrock data source is added to an Amazon Bedrock knowledge base.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BedrockIngestDataOperator`\n\n:param knowledge_base_id: The unique identifier of the knowledge base to which to add the data source. (templated)\n:param data_source_id: The unique identifier of the data source to ingest. (templated)\n:param ingest_data_kwargs: Any additional ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.901595",
      "relevance_keywords": [
        "operator",
        "ingest",
        "bedrock",
        "data",
        "amazon",
        "s3",
        "cloud",
        "bedrockingestdataoperator",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockragoperator_20260128_160330",
      "component_name": "BedrockRaGOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockRaGOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "input",
          "type": "str",
          "required": true
        },
        {
          "name": "source_type",
          "type": "str",
          "required": true
        },
        {
          "name": "model_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "prompt_template",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "knowledge_base_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "vector_search_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sources",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "rag_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.bedrock import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "validate_inputs",
          "build_rag_config",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Query a knowledge base and generate responses based on the retrieved results with sources citations.\n\nNOTE:  Support for EXTERNAL SOURCES was added in botocore 1.34.90\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BedrockRaGOperator`\n\n:param input: The query to be made to the knowledge base. (templated)\n:param source_type: The type of resource that is queried by the request. (templated)\n    Must be one of 'KNOWLEDGE_BASE' o",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.902040",
      "relevance_keywords": [
        "bedrockragoperator",
        "operator",
        "bedrock",
        "ra",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockretrieveoperator_20260128_160330",
      "component_name": "BedrockRetrieveOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockRetrieveOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "retrieval_query",
          "type": "str",
          "required": true
        },
        {
          "name": "knowledge_base_id",
          "type": "str",
          "required": true
        },
        {
          "name": "vector_search_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retrieve_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.bedrock import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Query a knowledge base and retrieve results with source citations.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BedrockRetrieveOperator`\n\n:param retrieval_query: The query to be made to the knowledge base. (templated)\n:param knowledge_base_id: The unique identifier of the knowledge base that is queried. (templated)\n:param vector_search_config: How the results from the vector search should be returned. (templated)\n    For m",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.902418",
      "relevance_keywords": [
        "operator",
        "bedrock",
        "amazon",
        "s3",
        "cloud",
        "retrieve",
        "bedrockretrieveoperator",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockbatchinferenceoperator_20260128_160330",
      "component_name": "BedrockBatchInferenceOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockBatchInferenceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "role_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "model_id",
          "type": "str",
          "required": true
        },
        {
          "name": "input_uri",
          "type": "str",
          "required": true
        },
        {
          "name": "output_uri",
          "type": "str",
          "required": true
        },
        {
          "name": "invoke_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.bedrock import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute_complete",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a batch inference job to invoke a model on multiple prompts.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BedrockBatchInferenceOperator`\n\n:param job_name: A name to give the batch inference job. (templated)\n:param role_arn: The ARN of the IAM role with permissions to create the knowledge base. (templated)\n:param model_id: Name or ARN of the model to associate with this provisioned throughput. (templated)\n:param inpu",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.902923",
      "relevance_keywords": [
        "batch",
        "operator",
        "bedrock",
        "bedrockbatchinferenceoperator",
        "inference",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_cloudformationcreatestackoperator_20260128_160330",
      "component_name": "CloudFormationCreateStackOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFormationCreateStackOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "stack_name",
          "type": "str",
          "required": false
        },
        {
          "name": "cloudformation_parameters",
          "type": "dict",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.cloud_formation import CloudFormationHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "An operator that creates an AWS CloudFormation stack.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudFormationCreateStackOperator`\n\n:param stack_name: stack name (templated)\n:param cloudformation_parameters: parameters to be passed to AWS CloudFormation.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow i",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.929591",
      "relevance_keywords": [
        "operator",
        "create",
        "stack",
        "formation",
        "cloudformationcreatestackoperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_cloudformationdeletestackoperator_20260128_160330",
      "component_name": "CloudFormationDeleteStackOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFormationDeleteStackOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "stack_name",
          "type": "str",
          "required": false
        },
        {
          "name": "cloudformation_parameters",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.cloud_formation import CloudFormationHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "An operator that deletes an AWS CloudFormation stack.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudFormationDeleteStackOperator`\n\n:param stack_name: stack name (templated)\n:param cloudformation_parameters: parameters to be passed to CloudFormation.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a ",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.929793",
      "relevance_keywords": [
        "operator",
        "cloudformationdeletestackoperator",
        "formation",
        "stack",
        "delete",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_comprehendbaseoperator_20260128_160330",
      "component_name": "ComprehendBaseOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComprehendBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "input_data_config",
          "type": "dict",
          "required": true
        },
        {
          "name": "output_data_config",
          "type": "dict",
          "required": true
        },
        {
          "name": "data_access_role_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "language_code",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.comprehend import ComprehendHook\nfrom airflow.providers.amazon.aws.links.comprehend import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.comprehend import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "client",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "This is the base operator for Comprehend Service operators (not supposed to be used directly in DAGs).\n\n:param input_data_config: The input properties for a PII entities detection job. (templated)\n:param output_data_config: Provides `conguration` parameters for the output of PII entity detection\n    jobs. (templated)\n:param data_access_role_arn: The Amazon Resource Name (ARN) of the IAM role that grants Amazon Comprehend\n    read access to your input data. (templated)\n:param language_code: The ",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.953330",
      "relevance_keywords": [
        "operator",
        "comprehend",
        "comprehendbaseoperator",
        "amazon",
        "base",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_comprehendstartpiientitiesdetectionjoboperator_20260128_160330",
      "component_name": "ComprehendStartPiiEntitiesDetectionJobOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComprehendStartPiiEntitiesDetectionJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "ComprehendBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "input_data_config",
          "type": "dict",
          "required": true
        },
        {
          "name": "output_data_config",
          "type": "dict",
          "required": true
        },
        {
          "name": "mode",
          "type": "str",
          "required": true
        },
        {
          "name": "data_access_role_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "language_code",
          "type": "str",
          "required": true
        },
        {
          "name": "start_pii_entities_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from ComprehendBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.comprehend import ComprehendHook\nfrom airflow.providers.amazon.aws.links.comprehend import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.comprehend import (",
        "class_attributes": [
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a comprehend pii entities detection job for a collection of documents.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComprehendStartPiiEntitiesDetectionJobOperator`\n\n:param input_data_config: The input properties for a PII entities detection job. (templated)\n:param output_data_config: Provides `conguration` parameters for the output of PII entity detection\n    jobs. (templated)\n:param mode: Specifies whether the out",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.953746",
      "relevance_keywords": [
        "detection",
        "operator",
        "start",
        "comprehend",
        "entities",
        "comprehendstartpiientitiesdetectionjoboperator",
        "amazon",
        "s3",
        "cloud",
        "job",
        "aws",
        "ec2",
        "pii"
      ]
    },
    {
      "id": "official_comprehendcreatedocumentclassifieroperator_20260128_160330",
      "component_name": "ComprehendCreateDocumentClassifierOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComprehendCreateDocumentClassifierOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "document_classifier_name",
          "type": "str",
          "required": true
        },
        {
          "name": "input_data_config",
          "type": "dict[...]",
          "required": true
        },
        {
          "name": "mode",
          "type": "str",
          "required": true
        },
        {
          "name": "data_access_role_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "language_code",
          "type": "str",
          "required": true
        },
        {
          "name": "fail_on_warnings",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "output_data_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "document_classifier_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.comprehend import ComprehendHook\nfrom airflow.providers.amazon.aws.links.comprehend import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.comprehend import (",
        "class_attributes": [
          "aws_hook_class",
          "operator_extra_links",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a comprehend document classifier that can categorize documents.\n\nProvide a set of training documents that are labeled with the categories.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComprehendCreateDocumentClassifierOperator`\n\n:param document_classifier_name: The name of the document classifier. (templated)\n:param input_data_config: Specifies the format and location of the input data for the job. (templated)\n:para",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.954069",
      "relevance_keywords": [
        "classifier",
        "operator",
        "create",
        "comprehend",
        "comprehendcreatedocumentclassifieroperator",
        "amazon",
        "s3",
        "cloud",
        "document",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_datasyncoperator_20260128_160330",
      "component_name": "DataSyncOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataSyncOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "wait_interval_seconds",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "max_iterations",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "task_arn",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "source_location_uri",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "destination_location_uri",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "allow_random_task_choice",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "allow_random_location_choice",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "create_task_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "create_source_location_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "create_destination_location_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "update_task_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "task_execution_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "delete_task_after_execution",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport logging\nimport random\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.datasync import DataSyncHook\nfrom airflow.providers.amazon.aws.links.datasync import DataSyncTaskExecutionLink, DataSyncTaskLink\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowTaskTimeout",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "execute",
          "_get_tasks_and_locations",
          "choose_task",
          "choose_location",
          "_create_datasync_task",
          "_update_datasync_task",
          "_execute_datasync_task",
          "_cancel_datasync_task_execution",
          "on_kill",
          "_delete_datasync_task",
          "_get_location_arns"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Find, Create, Update, Execute and Delete AWS DataSync Tasks.\n\nIf ``do_xcom_push`` is True, then the DataSync TaskArn and TaskExecutionArn\nwhich were executed will be pushed to an XCom.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataSyncOperator`\n\n.. note:: There may be 0, 1, or many existing DataSync Tasks defined in your AWS\n    environment. The default behavior is to create a new Task if there are 0, or\n    execute the",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:19.974523",
      "relevance_keywords": [
        "operator",
        "datasyncoperator",
        "data",
        "amazon",
        "sync",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmscreatetaskoperator_20260128_160330",
      "component_name": "DmsCreateTaskOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsCreateTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "replication_task_id",
          "type": "str",
          "required": false
        },
        {
          "name": "source_endpoint_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "target_endpoint_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "replication_instance_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "table_mappings",
          "type": "dict",
          "required": false
        },
        {
          "name": "migration_type",
          "type": "str",
          "required": false,
          "default": "full-load"
        },
        {
          "name": "create_task_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates AWS DMS replication task.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsCreateTaskOperator`\n\n:param replication_task_id: Replication task id\n:param source_endpoint_arn: Source endpoint ARN\n:param target_endpoint_arn: Target endpoint ARN\n:param replication_instance_arn: Replication instance ARN\n:param table_mappings: Table mappings\n:param migration_type: Migration type ('full-load'|'cdc'|'full-load-and-cdc'), full",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.004665",
      "relevance_keywords": [
        "operator",
        "task",
        "dmscreatetaskoperator",
        "create",
        "dms",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmsdeletetaskoperator_20260128_160330",
      "component_name": "DmsDeleteTaskOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsDeleteTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "replication_task_arn",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes AWS DMS replication task.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsDeleteTaskOperator`\n\n:param replication_task_arn: Replication task ARN\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configuration would be us",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.005114",
      "relevance_keywords": [
        "dmsdeletetaskoperator",
        "operator",
        "task",
        "dms",
        "delete",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmsdescribetasksoperator_20260128_160330",
      "component_name": "DmsDescribeTasksOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsDescribeTasksOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "describe_tasks_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Describes AWS DMS replication tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsDescribeTasksOperator`\n\n:param describe_tasks_kwargs: Describe tasks command arguments\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 config",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.005550",
      "relevance_keywords": [
        "tasks",
        "operator",
        "dms",
        "describe",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "dmsdescribetasksoperator"
      ]
    },
    {
      "id": "official_dmsstarttaskoperator_20260128_160330",
      "component_name": "DmsStartTaskOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsStartTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "replication_task_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "start_replication_task_type",
          "type": "str",
          "required": false,
          "default": "start-replication"
        },
        {
          "name": "start_task_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts AWS DMS replication task.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsStartTaskOperator`\n\n:param replication_task_arn: Replication task ARN\n:param start_replication_task_type: Replication task start type (default='start-replication')\n    ('start-replication'|'resume-processing'|'reload-target')\n:param start_task_kwargs: Extra start replication task arguments\n:param aws_conn_id: The Airflow connection used for AW",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.005894",
      "relevance_keywords": [
        "operator",
        "task",
        "dmsstarttaskoperator",
        "start",
        "dms",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmsstoptaskoperator_20260128_160330",
      "component_name": "DmsStopTaskOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsStopTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "replication_task_arn",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Stops AWS DMS replication task.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsStopTaskOperator`\n\n:param replication_task_arn: Replication task ARN\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configuration would be used (",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.006248",
      "relevance_keywords": [
        "operator",
        "stop",
        "task",
        "dms",
        "amazon",
        "s3",
        "cloud",
        "dmsstoptaskoperator",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmsdescribereplicationconfigsoperator_20260128_160330",
      "component_name": "DmsDescribeReplicationConfigsOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsDescribeReplicationConfigsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Describes AWS DMS Serverless replication configurations.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsDescribeReplicationConfigsOperator`\n\n:param describe_config_filter: Filters block for filtering results.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None o",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.006568",
      "relevance_keywords": [
        "dmsdescribereplicationconfigsoperator",
        "configs",
        "operator",
        "dms",
        "replication",
        "describe",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmscreatereplicationconfigoperator_20260128_160330",
      "component_name": "DmsCreateReplicationConfigOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsCreateReplicationConfigOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "replication_config_id",
          "type": "str",
          "required": false
        },
        {
          "name": "source_endpoint_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "target_endpoint_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "compute_config",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "replication_type",
          "type": "str",
          "required": false
        },
        {
          "name": "table_mappings",
          "type": "str",
          "required": false
        },
        {
          "name": "additional_config_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an AWS DMS Serverless replication configuration.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsCreateReplicationConfigOperator`\n\n:param replication_config_id: Unique identifier used to create a ReplicationConfigArn.\n:param source_endpoint_arn: ARN of the source endpoint\n:param target_endpoint_arn: ARN of the target endpoint\n:param compute_config: Parameters for provisioning an DMS Serverless replication.\n:param r",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.006914",
      "relevance_keywords": [
        "operator",
        "create",
        "dms",
        "replication",
        "amazon",
        "config",
        "s3",
        "cloud",
        "dmscreatereplicationconfigoperator",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmsdeletereplicationconfigoperator_20260128_160330",
      "component_name": "DmsDeleteReplicationConfigOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsDeleteReplicationConfigOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "replication_config_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "VALID_STATES",
          "DELETING_STATES",
          "TERMINAL_PROVISION_STATES"
        ],
        "methods": [
          "__init__",
          "execute",
          "handle_delete_wait",
          "execute_complete",
          "retry_execution"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes an AWS DMS Serverless replication configuration.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsDeleteReplicationConfigOperator`\n\n:param replication_config_arn: ARN of the replication config\n:param wait_for_completion: If True, waits for the replication config to be deleted before returning.\n    If False, the operator will return immediately after the request is made.\n:param deferrable: Run the operator in deferra",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.007288",
      "relevance_keywords": [
        "operator",
        "dms",
        "dmsdeletereplicationconfigoperator",
        "replication",
        "delete",
        "amazon",
        "config",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmsdescribereplicationsoperator_20260128_160330",
      "component_name": "DmsDescribeReplicationsOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsDescribeReplicationsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderer"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Describes AWS DMS Serverless replications.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsDescribeReplicationsOperator`\n\n:param filter: Filters block for filtering results.\n\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 con",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.007622",
      "relevance_keywords": [
        "operator",
        "dms",
        "describe",
        "replications",
        "dmsdescribereplicationsoperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmsstartreplicationoperator_20260128_160330",
      "component_name": "DmsStartReplicationOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsStartReplicationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "replication_config_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "replication_start_type",
          "type": "str",
          "required": false
        },
        {
          "name": "cdc_start_time",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cdc_start_pos",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cdc_stop_pos",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "RUNNING_STATES",
          "STARTABLE_STATES",
          "TERMINAL_STATES",
          "TERMINAL_PROVISION_STATES",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "retry_execution"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts an AWS DMS Serverless replication.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsStartReplicationOperator`\n\n:param replication_config_arn: ARN of the replication config\n:param replication_start_type: Type of replication.\n:param cdc_start_time: Start time of CDC\n:param cdc_start_pos: Indicates when to start CDC.\n:param cdc_stop_pos: Indicates when to stop CDC.\n:param aws_conn_id: The Airflow connection used for AWS",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.007948",
      "relevance_keywords": [
        "dmsstartreplicationoperator",
        "operator",
        "start",
        "dms",
        "replication",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmsstopreplicationoperator_20260128_160330",
      "component_name": "DmsStopReplicationOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsStopReplicationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "replication_config_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import datetime\nfrom typing import Any, ClassVar\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.dms import (",
        "class_attributes": [
          "STOPPED_STATES",
          "NON_STOPPABLE_STATES",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Stops an AWS DMS Serverless replication.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DmsStopReplicationOperator`\n\n:param replication_config_arn: ARN of the replication config\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 co",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.008268",
      "relevance_keywords": [
        "operator",
        "stop",
        "dms",
        "replication",
        "amazon",
        "s3",
        "cloud",
        "dmsstopreplicationoperator",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ec2startinstanceoperator_20260128_160330",
      "component_name": "EC2StartInstanceOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EC2StartInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "check_interval",
          "type": "float",
          "required": false,
          "default": 15
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.links.ec2 import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "operator_extra_links",
          "template_fields",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Start AWS EC2 instance using boto3.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EC2StartInstanceOperator`\n\n:param instance_id: id of the AWS EC2 instance\n    :param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configuration would",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.031276",
      "relevance_keywords": [
        "operator",
        "start",
        "ec2startinstanceoperator",
        "instance",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ec2stopinstanceoperator_20260128_160330",
      "component_name": "EC2StopInstanceOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EC2StopInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "check_interval",
          "type": "float",
          "required": false,
          "default": 15
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.links.ec2 import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "operator_extra_links",
          "template_fields",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Stop AWS EC2 instance using boto3.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EC2StopInstanceOperator`\n\n:param instance_id: id of the AWS EC2 instance\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configuration would be us",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.031521",
      "relevance_keywords": [
        "operator",
        "stop",
        "instance",
        "amazon",
        "s3",
        "cloud",
        "ec2stopinstanceoperator",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ec2createinstanceoperator_20260128_160330",
      "component_name": "EC2CreateInstanceOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EC2CreateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "image_id",
          "type": "str",
          "required": true
        },
        {
          "name": "max_count",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "min_count",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.links.ec2 import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "execute",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create and start a specified number of EC2 Instances using boto3.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EC2CreateInstanceOperator`\n\n:param image_id: ID of the AMI used to create the instance.\n:param max_count: Maximum number of instances to launch. Defaults to 1.\n:param min_count: Minimum number of instances to launch. Defaults to 1.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.031715",
      "relevance_keywords": [
        "operator",
        "create",
        "instance",
        "amazon",
        "ec2createinstanceoperator",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ec2terminateinstanceoperator_20260128_160330",
      "component_name": "EC2TerminateInstanceOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EC2TerminateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_ids",
          "type": "Union",
          "required": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.links.ec2 import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Terminate EC2 Instances using boto3.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EC2TerminateInstanceOperator`\n\n:param instance_id: ID of the instance to be terminated.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configur",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.031880",
      "relevance_keywords": [
        "operator",
        "terminate",
        "instance",
        "ec2terminateinstanceoperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ec2rebootinstanceoperator_20260128_160330",
      "component_name": "EC2RebootInstanceOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EC2RebootInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_ids",
          "type": "Union",
          "required": false
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.links.ec2 import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "operator_extra_links",
          "template_fields",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Reboot Amazon EC2 instances.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EC2RebootInstanceOperator`\n\n:param instance_ids: ID of the instance(s) to be rebooted.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configuration wou",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.032046",
      "relevance_keywords": [
        "operator",
        "instance",
        "reboot",
        "ec2rebootinstanceoperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ec2hibernateinstanceoperator_20260128_160330",
      "component_name": "EC2HibernateInstanceOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EC2HibernateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_ids",
          "type": "Union",
          "required": false
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.links.ec2 import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "operator_extra_links",
          "template_fields",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Hibernate Amazon EC2 instances.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EC2HibernateInstanceOperator`\n\n:param instance_ids: ID of the instance(s) to be hibernated.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configura",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.032205",
      "relevance_keywords": [
        "hibernate",
        "ec2hibernateinstanceoperator",
        "operator",
        "instance",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ecsbaseoperator_20260128_160330",
      "component_name": "EcsBaseOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom functools import cached_property\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.exceptions import EcsOperatorError, EcsTaskFailToStart\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsClusterStates, EcsHook\nfrom airflow.providers.amazon.aws.hooks.logs import AwsLogsHook",
        "class_attributes": [
          "aws_hook_class"
        ],
        "methods": [
          "client",
          "execute",
          "_complete_exec_with_cluster_desc"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "This is the base operator for all Elastic Container Service operators.",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.053225",
      "relevance_keywords": [
        "operator",
        "ecs",
        "base",
        "amazon",
        "s3",
        "ecsbaseoperator",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ecscreateclusteroperator_20260128_160330",
      "component_name": "EcsCreateClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsCreateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EcsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": false
        },
        {
          "name": "create_cluster_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 15
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from EcsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom functools import cached_property\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.exceptions import EcsOperatorError, EcsTaskFailToStart\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsClusterStates, EcsHook\nfrom airflow.providers.amazon.aws.hooks.logs import AwsLogsHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an AWS ECS cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EcsCreateClusterOperator`\n\n:param cluster_name: The name of your cluster. If you don't specify a name for your\n    cluster, you create a cluster that's named default.\n:param create_cluster_kwargs: Extra arguments for Cluster Creation.\n:param wait_for_completion: If True, waits for creation of the cluster to complete. (default: True)\n:param waiter_dela",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.053403",
      "relevance_keywords": [
        "operator",
        "cluster",
        "ecs",
        "create",
        "ecscreateclusteroperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ecsdeleteclusteroperator_20260128_160330",
      "component_name": "EcsDeleteClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsDeleteClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EcsBaseOperator"
        ],
        "template_fields": [
          "cluster_name",
          "wait_for_completion",
          "deferrable"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 15
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from EcsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom functools import cached_property\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.exceptions import EcsOperatorError, EcsTaskFailToStart\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsClusterStates, EcsHook\nfrom airflow.providers.amazon.aws.hooks.logs import AwsLogsHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes an AWS ECS cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EcsDeleteClusterOperator`\n\n:param cluster_name: The short name or full Amazon Resource Name (ARN) of the cluster to delete.\n:param wait_for_completion: If True, waits for creation of the cluster to complete. (default: True)\n:param waiter_delay: The amount of time in seconds to wait between attempts,\n    if not set then the default waiter value will be",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.053575",
      "relevance_keywords": [
        "operator",
        "cluster",
        "ecs",
        "delete",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "ecsdeleteclusteroperator"
      ]
    },
    {
      "id": "official_ecsderegistertaskdefinitionoperator_20260128_160330",
      "component_name": "EcsDeregisterTaskDefinitionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsDeregisterTaskDefinitionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EcsBaseOperator"
        ],
        "template_fields": [
          "task_definition"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "task_definition",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from EcsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom functools import cached_property\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.exceptions import EcsOperatorError, EcsTaskFailToStart\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsClusterStates, EcsHook\nfrom airflow.providers.amazon.aws.hooks.logs import AwsLogsHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deregister a task definition on AWS ECS.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EcsDeregisterTaskDefinitionOperator`\n\n:param task_definition: The family and revision (family:revision) or full Amazon Resource Name (ARN)\n    of the task definition to deregister. If you use a family name, you must specify a revision.",
      "success_score": 193,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.053706",
      "relevance_keywords": [
        "ecsderegistertaskdefinitionoperator",
        "operator",
        "deregister",
        "ecs",
        "task",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "definition",
        "ec2"
      ]
    },
    {
      "id": "official_ecsregistertaskdefinitionoperator_20260128_160330",
      "component_name": "EcsRegisterTaskDefinitionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsRegisterTaskDefinitionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EcsBaseOperator"
        ],
        "template_fields": [
          "family",
          "container_definitions",
          "register_task_kwargs"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "family",
          "type": "str",
          "required": false
        },
        {
          "name": "container_definitions",
          "type": "list[...]",
          "required": false
        },
        {
          "name": "register_task_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from EcsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom functools import cached_property\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.exceptions import EcsOperatorError, EcsTaskFailToStart\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsClusterStates, EcsHook\nfrom airflow.providers.amazon.aws.hooks.logs import AwsLogsHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Register a task definition on AWS ECS.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EcsRegisterTaskDefinitionOperator`\n\n:param family: The family name of a task definition to create.\n:param container_definitions: A list of container definitions in JSON format that describe\n    the different containers that make up your task.\n:param register_task_kwargs: Extra arguments for Register Task Definition.",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.053845",
      "relevance_keywords": [
        "operator",
        "task",
        "ecs",
        "ecsregistertaskdefinitionoperator",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "register",
        "definition",
        "ec2"
      ]
    },
    {
      "id": "official_ecsruntaskoperator_20260128_160330",
      "component_name": "EcsRunTaskOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsRunTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EcsBaseOperator"
        ],
        "template_fields": [
          "task_definition",
          "cluster",
          "overrides",
          "launch_type",
          "capacity_provider_strategy",
          "volume_configurations",
          "group",
          "placement_constraints",
          "placement_strategy",
          "platform_version",
          "network_configuration",
          "tags",
          "awslogs_group",
          "awslogs_region",
          "awslogs_stream_prefix",
          "awslogs_fetch_interval",
          "container_name",
          "propagate_tags",
          "reattach",
          "number_logs_exception",
          "wait_for_completion",
          "deferrable"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "task_definition",
          "type": "str",
          "required": false
        },
        {
          "name": "cluster",
          "type": "str",
          "required": false
        },
        {
          "name": "overrides",
          "type": "dict",
          "required": false
        },
        {
          "name": "launch_type",
          "type": "str",
          "required": false,
          "default": "EC2"
        },
        {
          "name": "capacity_provider_strategy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "volume_configurations",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "group",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "placement_constraints",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "placement_strategy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "platform_version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "network_configuration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "awslogs_group",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "awslogs_region",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "awslogs_stream_prefix",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from EcsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom functools import cached_property\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.exceptions import EcsOperatorError, EcsTaskFailToStart\nfrom airflow.providers.amazon.aws.hooks.ecs import EcsClusterStates, EcsHook\nfrom airflow.providers.amazon.aws.hooks.logs import AwsLogsHook",
        "class_attributes": [
          "ui_color",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "_get_ecs_task_id",
          "execute",
          "execute_complete",
          "_after_execution",
          "_start_task",
          "_try_reattach_task",
          "_wait_for_task_ended",
          "_aws_logs_enabled",
          "_get_logs_stream_name",
          "_get_task_log_fetcher",
          "_check_success_task",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Execute a task on AWS ECS (Elastic Container Service).\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EcsRunTaskOperator`\n\n:param task_definition: the task definition name on Elastic Container Service\n:param cluster: the cluster name on Elastic Container Service\n:param overrides: the same parameter that boto3 will receive (templated):\n    https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html#ECS",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.054026",
      "relevance_keywords": [
        "operator",
        "task",
        "ecs",
        "run",
        "ecsruntaskoperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ekscreateclusteroperator_20260128_160330",
      "component_name": "EksCreateClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksCreateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_role_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "resources_vpc_config",
          "type": "dict",
          "required": true
        },
        {
          "name": "compute",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT_COMPUTE_TYPE>"
        },
        {
          "name": "create_cluster_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "nodegroup_name",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_NODEGROUP_NAME>"
        },
        {
          "name": "nodegroup_role_arn",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "create_nodegroup_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "fargate_profile_name",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_FARGATE_PROFILE_NAME>"
        },
        {
          "name": "fargate_pod_execution_role_arn",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "fargate_selectors",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "create_fargate_profile_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport logging\nimport warnings\nfrom ast import literal_eval\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, cast\nfrom botocore.exceptions import ClientError, WaiterError\nfrom airflow.exceptions import AirflowProviderDeprecationWarning",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "deferrable_create_cluster_next",
          "execute_failed",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an Amazon EKS Cluster control plane.\n\nOptionally, can also create the supporting compute architecture:\n\n - If argument 'compute' is provided with a value of 'nodegroup', will also\n     attempt to create an Amazon EKS Managed Nodegroup for the cluster.\n     See :class:`~airflow.providers.amazon.aws.operators.EksCreateNodegroupOperator`\n     documentation for requirements.\n\n-  If argument 'compute' is provided with a value of 'fargate', will also attempt to create an AWS\n     Fargate profi",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.079040",
      "relevance_keywords": [
        "operator",
        "cluster",
        "create",
        "ekscreateclusteroperator",
        "eks",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ekscreatenodegroupoperator_20260128_160330",
      "component_name": "EksCreateNodegroupOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksCreateNodegroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "nodegroup_subnets",
          "type": "Union",
          "required": true
        },
        {
          "name": "nodegroup_role_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "nodegroup_name",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_NODEGROUP_NAME>"
        },
        {
          "name": "create_nodegroup_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 80
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport logging\nimport warnings\nfrom ast import literal_eval\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, cast\nfrom botocore.exceptions import ClientError, WaiterError\nfrom airflow.exceptions import AirflowProviderDeprecationWarning",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an Amazon EKS managed node group for an existing Amazon EKS Cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EksCreateNodegroupOperator`\n\n:param cluster_name: The name of the Amazon EKS Cluster to create the managed nodegroup in. (templated)\n:param nodegroup_name: The unique name to give your managed nodegroup. (templated)\n:param nodegroup_subnets:\n     The subnets to use for the Auto Scaling group that is cre",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.079311",
      "relevance_keywords": [
        "nodegroup",
        "ekscreatenodegroupoperator",
        "operator",
        "create",
        "eks",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ekscreatefargateprofileoperator_20260128_160330",
      "component_name": "EksCreateFargateProfileOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksCreateFargateProfileOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "pod_execution_role_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "selectors",
          "type": "list",
          "required": true
        },
        {
          "name": "fargate_profile_name",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_FARGATE_PROFILE_NAME>"
        },
        {
          "name": "create_fargate_profile_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport logging\nimport warnings\nfrom ast import literal_eval\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, cast\nfrom botocore.exceptions import ClientError, WaiterError\nfrom airflow.exceptions import AirflowProviderDeprecationWarning",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an AWS Fargate profile for an Amazon EKS cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EksCreateFargateProfileOperator`\n\n:param cluster_name: The name of the Amazon EKS cluster to apply the AWS Fargate profile to. (templated)\n:param pod_execution_role_arn: The Amazon Resource Name (ARN) of the pod execution role to\n     use for pods that match the selectors in the AWS Fargate profile. (templated)\n:param sel",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.079533",
      "relevance_keywords": [
        "operator",
        "create",
        "ekscreatefargateprofileoperator",
        "eks",
        "amazon",
        "s3",
        "fargate",
        "cloud",
        "profile",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_eksdeleteclusteroperator_20260128_160330",
      "component_name": "EksDeleteClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksDeleteClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "force_delete_compute",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 40
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport logging\nimport warnings\nfrom ast import literal_eval\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, cast\nfrom botocore.exceptions import ClientError, WaiterError\nfrom airflow.exceptions import AirflowProviderDeprecationWarning",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "delete_any_nodegroups",
          "delete_any_fargate_profiles",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes the Amazon EKS Cluster control plane and all nodegroups attached to it.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EksDeleteClusterOperator`\n\n:param cluster_name: The name of the Amazon EKS Cluster to delete. (templated)\n:param force_delete_compute: If True, will delete any attached resources. (templated)\n     Defaults to False.\n:param wait_for_completion: If True, waits for operator to complete. (default: False)",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.079743",
      "relevance_keywords": [
        "operator",
        "cluster",
        "eks",
        "delete",
        "amazon",
        "s3",
        "eksdeleteclusteroperator",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_eksdeletenodegroupoperator_20260128_160330",
      "component_name": "EksDeleteNodegroupOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksDeleteNodegroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "nodegroup_name",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 40
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport logging\nimport warnings\nfrom ast import literal_eval\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, cast\nfrom botocore.exceptions import ClientError, WaiterError\nfrom airflow.exceptions import AirflowProviderDeprecationWarning",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes an Amazon EKS managed node group from an Amazon EKS Cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EksDeleteNodegroupOperator`\n\n:param cluster_name: The name of the Amazon EKS Cluster associated with your nodegroup. (templated)\n:param nodegroup_name: The name of the nodegroup to delete. (templated)\n:param wait_for_completion: If True, waits for operator to complete. (default: False) (templated)\n:param aws_co",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.079944",
      "relevance_keywords": [
        "nodegroup",
        "operator",
        "eks",
        "delete",
        "amazon",
        "s3",
        "cloud",
        "eksdeletenodegroupoperator",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_eksdeletefargateprofileoperator_20260128_160330",
      "component_name": "EksDeleteFargateProfileOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksDeleteFargateProfileOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "fargate_profile_name",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport logging\nimport warnings\nfrom ast import literal_eval\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, cast\nfrom botocore.exceptions import ClientError, WaiterError\nfrom airflow.exceptions import AirflowProviderDeprecationWarning",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes an AWS Fargate profile from an Amazon EKS Cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EksDeleteFargateProfileOperator`\n\n:param cluster_name: The name of the Amazon EKS cluster associated with your Fargate profile. (templated)\n:param fargate_profile_name: The name of the AWS Fargate profile to delete. (templated)\n:param wait_for_completion: If True, waits for operator to complete. (default: False) (templat",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.080140",
      "relevance_keywords": [
        "operator",
        "eks",
        "delete",
        "eksdeletefargateprofileoperator",
        "amazon",
        "s3",
        "fargate",
        "cloud",
        "profile",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ekspodoperator_20260128_160330",
      "component_name": "EksPodOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksPodOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "KubernetesPodOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "in_cluster",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "namespace",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_NAMESPACE_NAME>"
        },
        {
          "name": "pod_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT_CONN_ID>"
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "on_finish_action",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from KubernetesPodOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport logging\nimport warnings\nfrom ast import literal_eval\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, cast\nfrom botocore.exceptions import ClientError, WaiterError\nfrom airflow.exceptions import AirflowProviderDeprecationWarning",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "trigger_reentry"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Executes a task in a Kubernetes pod on the specified Amazon EKS Cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EksPodOperator`\n\n:param cluster_name: The name of the Amazon EKS Cluster to execute the task on. (templated)\n:param in_cluster: If True, look for config inside the cluster; if False look for a local file path.\n:param namespace: The namespace in which to execute the pod. (templated)\n:param pod_name: The uniq",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.080341",
      "relevance_keywords": [
        "operator",
        "pod",
        "eks",
        "amazon",
        "s3",
        "ekspodoperator",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emraddstepsoperator_20260128_160330",
      "component_name": "EmrAddStepsOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrAddStepsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_flow_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_flow_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "steps",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "execution_role_arn",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "An operator that adds steps to an existing EMR job_flow.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrAddStepsOperator`\n\n:param job_flow_id: id of the JobFlow to add steps to. (templated)\n:param job_flow_name: name of the JobFlow to add steps to. Use as an alternative to passing\n    job_flow_id. will search for id of JobFlow with matching name in one of the states in\n    param cluster_states. Exactly one cluster like th",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.127602",
      "relevance_keywords": [
        "emr",
        "operator",
        "add",
        "emraddstepsoperator",
        "amazon",
        "steps",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrstartnotebookexecutionoperator_20260128_160330",
      "component_name": "EmrStartNotebookExecutionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrStartNotebookExecutionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "editor_id",
          "type": "str",
          "required": true
        },
        {
          "name": "relative_path",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "service_role",
          "type": "str",
          "required": true
        },
        {
          "name": "notebook_execution_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "notebook_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "notebook_instance_security_group_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "master_instance_security_group_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "waiter_max_attempts",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "waiter_delay",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "An operator that starts an EMR notebook execution.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrStartNotebookExecutionOperator`\n\n:param editor_id: The unique identifier of the EMR notebook to use for notebook execution.\n:param relative_path: The path and file name of the notebook file for this execution,\n    relative to the path specified for the EMR notebook.\n:param cluster_id: The unique identifier of the EMR cluster ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.127860",
      "relevance_keywords": [
        "emr",
        "execution",
        "operator",
        "start",
        "emrstartnotebookexecutionoperator",
        "notebook",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrstopnotebookexecutionoperator_20260128_160330",
      "component_name": "EmrStopNotebookExecutionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrStopNotebookExecutionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "notebook_execution_id",
          "type": "str",
          "required": true
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "waiter_max_attempts",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "waiter_delay",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "An operator that stops a running EMR notebook execution.\n\n .. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrStopNotebookExecutionOperator`\n\n:param notebook_execution_id: The unique identifier of the notebook execution.\n:param wait_for_completion: If True, the operator will wait for the notebook.\n    to be in a STOPPED or FINISHED state. Defaults to False.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.128051",
      "relevance_keywords": [
        "emr",
        "execution",
        "operator",
        "stop",
        "notebook",
        "amazon",
        "s3",
        "emrstopnotebookexecutionoperator",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrekscreateclusteroperator_20260128_160330",
      "component_name": "EmrEksCreateClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrEksCreateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "virtual_cluster_name",
          "type": "str",
          "required": false
        },
        {
          "name": "eks_cluster_name",
          "type": "str",
          "required": false
        },
        {
          "name": "eks_namespace",
          "type": "str",
          "required": false
        },
        {
          "name": "virtual_cluster_id",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "An operator that creates EMR on EKS virtual clusters.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrEksCreateClusterOperator`\n\n:param virtual_cluster_name: The name of the EMR EKS virtual cluster to create.\n:param eks_cluster_name: The EKS cluster used by the EMR virtual cluster.\n:param eks_namespace: namespace used by the EKS cluster.\n:param virtual_cluster_id: The EMR on EKS virtual cluster id.\n:param aws_conn_id: The ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.128253",
      "relevance_keywords": [
        "emr",
        "operator",
        "cluster",
        "create",
        "eks",
        "amazon",
        "s3",
        "cloud",
        "emrekscreateclusteroperator",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrcontaineroperator_20260128_160330",
      "component_name": "EmrContainerOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrContainerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "virtual_cluster_id",
          "type": "str",
          "required": false
        },
        {
          "name": "execution_role_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "release_label",
          "type": "str",
          "required": false
        },
        {
          "name": "job_driver",
          "type": "dict",
          "required": false
        },
        {
          "name": "configuration_overrides",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "client_request_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "max_polling_attempts",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_retry_max_attempts",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "execute",
          "check_failure",
          "execute_complete",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "An operator that submits jobs to EMR on EKS virtual clusters.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrContainerOperator`\n\n:param name: The name of the job run.\n:param virtual_cluster_id: The EMR on EKS virtual cluster ID\n:param execution_role_arn: The IAM role ARN associated with the job run.\n:param release_label: The Amazon EMR release version to use for the job run.\n:param job_driver: Job configuration details, e",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.128488",
      "relevance_keywords": [
        "emr",
        "operator",
        "emrcontaineroperator",
        "amazon",
        "container",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrcreatejobflowoperator_20260128_160330",
      "component_name": "EmrCreateJobFlowOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrCreateJobFlowOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "emr_conn_id",
          "type": "Union",
          "required": false,
          "default": "emr_default"
        },
        {
          "name": "job_flow_overrides",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_policy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "waiter_max_attempts",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "waiter_delay",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "execute",
          "execute_complete",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an EMR JobFlow, reading the config from the EMR connection.\n\nA dictionary of JobFlow overrides can be passed that override the config from the connection.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrCreateJobFlowOperator`\n\n:param emr_conn_id: :ref:`Amazon Elastic MapReduce Connection <howto/connection:emr>`.\n    Use to receive an initial Amazon EMR cluster configuration:\n    ``boto3.client('emr').run_job_flow``",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.128709",
      "relevance_keywords": [
        "emr",
        "operator",
        "create",
        "emrcreatejobflowoperator",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "job",
        "flow",
        "ec2"
      ]
    },
    {
      "id": "official_emrmodifyclusteroperator_20260128_160330",
      "component_name": "EmrModifyClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrModifyClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": false
        },
        {
          "name": "step_concurrency_level",
          "type": "int",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "An operator that modifies an existing EMR cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrModifyClusterOperator`\n\n:param cluster_id: cluster identifier\n:param step_concurrency_level: Concurrency of the cluster\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.128908",
      "relevance_keywords": [
        "emr",
        "operator",
        "cluster",
        "modify",
        "emrmodifyclusteroperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrterminatejobflowoperator_20260128_160330",
      "component_name": "EmrTerminateJobFlowOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrTerminateJobFlowOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_flow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Operator to terminate EMR JobFlows.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrTerminateJobFlowOperator`\n\n:param job_flow_id: id of the JobFlow to terminate. (templated)\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 con",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.129108",
      "relevance_keywords": [
        "emr",
        "operator",
        "terminate",
        "emrterminatejobflowoperator",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "job",
        "flow",
        "ec2"
      ]
    },
    {
      "id": "official_emrserverlesscreateapplicationoperator_20260128_160330",
      "component_name": "EmrServerlessCreateApplicationOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrServerlessCreateApplicationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "release_label",
          "type": "str",
          "required": true
        },
        {
          "name": "job_type",
          "type": "str",
          "required": true
        },
        {
          "name": "client_request_token",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_max_attempts",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "waiter_delay",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute",
          "start_application_deferred",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Operator to create Serverless EMR Application.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrServerlessCreateApplicationOperator`\n\n:param release_label: The EMR release version associated with the application.\n:param job_type: The type of application you want to start, such as Spark or Hive.\n:param wait_for_completion: If true, wait for the Application to start before returning. Defaults to True.\n    If set to False, ``w",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.129324",
      "relevance_keywords": [
        "emr",
        "emrserverlesscreateapplicationoperator",
        "operator",
        "create",
        "serverless",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "application"
      ]
    },
    {
      "id": "official_emrserverlessstartjoboperator_20260128_160330",
      "component_name": "EmrServerlessStartJobOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrServerlessStartJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_id",
          "type": "str",
          "required": true
        },
        {
          "name": "execution_role_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "job_driver",
          "type": "dict",
          "required": true
        },
        {
          "name": "configuration_overrides",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "client_request_token",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "waiter_max_attempts",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "waiter_delay",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "enable_application_ui_links",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "on_kill",
          "is_monitoring_in_job_override",
          "persist_links"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Operator to start EMR Serverless job.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrServerlessStartJobOperator`\n\n:param application_id: ID of the EMR Serverless application to start.\n:param execution_role_arn: ARN of role to perform action.\n:param job_driver: Driver that the job runs on.\n:param configuration_overrides: Configuration specifications to override existing configurations.\n:param client_request_token: The clie",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.129583",
      "relevance_keywords": [
        "emr",
        "emrserverlessstartjoboperator",
        "operator",
        "start",
        "serverless",
        "amazon",
        "s3",
        "cloud",
        "job",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrserverlessstopapplicationoperator_20260128_160330",
      "component_name": "EmrServerlessStopApplicationOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrServerlessStopApplicationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_id",
          "type": "str",
          "required": true
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_max_attempts",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "waiter_delay",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "force_stop",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "stop_application",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Operator to stop an EMR Serverless application.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrServerlessStopApplicationOperator`\n\n:param application_id: ID of the EMR Serverless application to stop.\n:param wait_for_completion: If true, wait for the Application to stop before returning. Default to True\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default bo",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.129826",
      "relevance_keywords": [
        "emr",
        "operator",
        "stop",
        "emrserverlessstopapplicationoperator",
        "serverless",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "application"
      ]
    },
    {
      "id": "official_emrserverlessdeleteapplicationoperator_20260128_160330",
      "component_name": "EmrServerlessDeleteApplicationOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrServerlessDeleteApplicationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EmrServerlessStopApplicationOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_id",
          "type": "str",
          "required": true
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_max_attempts",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "waiter_delay",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "force_stop",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from EmrServerlessStopApplicationOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import uuid4\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Operator to delete EMR Serverless application.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EmrServerlessDeleteApplicationOperator`\n\n:param application_id: ID of the EMR Serverless application to delete.\n:param wait_for_completion: If true, wait for the Application to be deleted before returning.\n    Defaults to True. Note that this operator will always wait for the application to be STOPPED first.\n:param aws_conn_id: The ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.130068",
      "relevance_keywords": [
        "emr",
        "operator",
        "emrserverlessdeleteapplicationoperator",
        "delete",
        "serverless",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "application"
      ]
    },
    {
      "id": "official_eventbridgeputeventsoperator_20260128_160330",
      "component_name": "EventBridgePutEventsOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EventBridgePutEventsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entries",
          "type": "list[...]",
          "required": false
        },
        {
          "name": "endpoint_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.eventbridge import EventBridgeHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.utils.helpers import prune_dict",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Put Events onto Amazon EventBridge.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EventBridgePutEventsOperator`\n\n:param entries: the list of events to be put onto EventBridge, each event is a dict (required)\n:param endpoint_id: the URL subdomain of the endpoint\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflo",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.168645",
      "relevance_keywords": [
        "event",
        "operator",
        "eventbridgeputeventsoperator",
        "put",
        "bridge",
        "amazon",
        "s3",
        "cloud",
        "events",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_eventbridgeputruleoperator_20260128_160330",
      "component_name": "EventBridgePutRuleOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EventBridgePutRuleOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "description",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "event_bus_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "event_pattern",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "role_arn",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "schedule_expression",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "state",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.eventbridge import EventBridgeHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.utils.helpers import prune_dict",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create or update a specified EventBridge rule.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EventBridgePutRuleOperator`\n\n:param name: name of the rule to create or update (required)\n:param description: description of the rule\n:param event_bus_name: name or ARN of the event bus to associate with this rule\n:param event_pattern: pattern of events to be matched to this rule\n:param role_arn: the Amazon Resource Name of the IAM ",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.168772",
      "relevance_keywords": [
        "event",
        "operator",
        "rule",
        "eventbridgeputruleoperator",
        "put",
        "bridge",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_eventbridgeenableruleoperator_20260128_160330",
      "component_name": "EventBridgeEnableRuleOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EventBridgeEnableRuleOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "event_bus_name",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.eventbridge import EventBridgeHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.utils.helpers import prune_dict",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Enable an EventBridge Rule.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EventBridgeEnableRuleOperator`\n\n:param name: the name of the rule to enable\n:param event_bus_name: the name or ARN of the event bus associated with the rule (default if omitted)\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a dis",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.168863",
      "relevance_keywords": [
        "event",
        "operator",
        "rule",
        "enable",
        "eventbridgeenableruleoperator",
        "bridge",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_eventbridgedisableruleoperator_20260128_160330",
      "component_name": "EventBridgeDisableRuleOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EventBridgeDisableRuleOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "event_bus_name",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.eventbridge import EventBridgeHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.utils.helpers import prune_dict",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Disable an EventBridge Rule.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:EventBridgeDisableRuleOperator`\n\n:param name: the name of the rule to disable\n:param event_bus_name: the name or ARN of the event bus associated with the rule (default if omitted)\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a ",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.168949",
      "relevance_keywords": [
        "event",
        "operator",
        "rule",
        "eventbridgedisableruleoperator",
        "bridge",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "disable",
        "ec2"
      ]
    },
    {
      "id": "official_glaciercreatejoboperator_20260128_160330",
      "component_name": "GlacierCreateJobOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlacierCreateJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "vault_name",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.glacier import GlacierHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Initiate an Amazon Glacier inventory-retrieval job.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GlacierCreateJobOperator`\n\n:param aws_conn_id: The reference to the AWS connection details\n:param vault_name: the Glacier vault on which job is executed",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.192627",
      "relevance_keywords": [
        "operator",
        "create",
        "glacier",
        "amazon",
        "job",
        "s3",
        "cloud",
        "glaciercreatejoboperator",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_glacieruploadarchiveoperator_20260128_160330",
      "component_name": "GlacierUploadArchiveOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlacierUploadArchiveOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "vault_name",
          "type": "str",
          "required": false
        },
        {
          "name": "body",
          "type": "object",
          "required": false
        },
        {
          "name": "checksum",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "archive_description",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "account_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.glacier import GlacierHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "This operator add an archive to an Amazon S3 Glacier vault.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GlacierUploadArchiveOperator`\n\n:param vault_name: The name of the vault\n:param body: A bytes or seekable file-like object. The data to upload.\n:param checksum: The SHA256 tree hash of the data being uploaded.\n    This parameter is automatically populated if it is not provided\n:param archive_description: The description ",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.192717",
      "relevance_keywords": [
        "glacieruploadarchiveoperator",
        "operator",
        "upload",
        "archive",
        "glacier",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_gluejoboperator_20260128_160330",
      "component_name": "GlueJobOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false,
          "default": "aws_glue_default_job"
        },
        {
          "name": "job_desc",
          "type": "str",
          "required": false,
          "default": "AWS Glue Job with Airflow"
        },
        {
          "name": "script_location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "concurrent_run_limit",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "script_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry_limit",
          "type": "int",
          "required": false,
          "default": 0
        },
        {
          "name": "num_of_dpus",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "s3_bucket",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "iam_role_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "iam_role_arn",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "create_job_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "run_job_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "verbose",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport urllib.parse\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.glue import GlueDataQualityHook, GlueJobHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.links.glue import GlueJobRunDetailsLink\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "upload_etl_script_to_s3",
          "execute",
          "execute_complete",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create an AWS Glue Job.\n\nAWS Glue is a serverless Spark ETL service for running Spark Jobs on the AWS\ncloud. Language support: Python and Scala.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GlueJobOperator`\n\n:param job_name: unique job name per AWS Account\n:param script_location: location of ETL script. Must be a local or S3 path\n:param job_desc: job description details\n:param concurrent_run_limit: The maximum number of co",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.209434",
      "relevance_keywords": [
        "operator",
        "gluejoboperator",
        "glue",
        "amazon",
        "s3",
        "cloud",
        "job",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_gluedataqualityoperator_20260128_160330",
      "component_name": "GlueDataQualityOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueDataQualityOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "ruleset",
          "type": "str",
          "required": false
        },
        {
          "name": "description",
          "type": "str",
          "required": false,
          "default": "AWS Glue Data Quality Rule Set With Airflow"
        },
        {
          "name": "update_rule_set",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "data_quality_ruleset_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport urllib.parse\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.glue import GlueDataQualityHook, GlueJobHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.links.glue import GlueJobRunDetailsLink\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a data quality ruleset with DQDL rules applied to a specified Glue table.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GlueDataQualityOperator`\n\n:param name: A unique name for the data quality ruleset.\n:param ruleset: A Data Quality Definition Language (DQDL) ruleset.\n    For more information, see the Glue developer guide.\n:param description: A description of the data quality ruleset.\n:param update_rule_set: To upd",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.209818",
      "relevance_keywords": [
        "quality",
        "operator",
        "data",
        "glue",
        "gluedataqualityoperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_gluedataqualityrulesetevaluationrunoperator_20260128_160330",
      "component_name": "GlueDataQualityRuleSetEvaluationRunOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueDataQualityRuleSetEvaluationRunOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "datasource",
          "type": "dict",
          "required": false
        },
        {
          "name": "role",
          "type": "str",
          "required": false
        },
        {
          "name": "rule_set_names",
          "type": "list[...]",
          "required": false
        },
        {
          "name": "number_of_workers",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "timeout",
          "type": "int",
          "required": false,
          "default": 2880
        },
        {
          "name": "verify_result_status",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "show_results",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "rule_set_evaluation_run_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport urllib.parse\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.glue import GlueDataQualityHook, GlueJobHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.links.glue import GlueJobRunDetailsLink\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "validate_inputs",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Evaluate a ruleset against a data source (Glue table).\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GlueDataQualityRuleSetEvaluationRunOperator`\n\n:param datasource: The data source (Glue table) associated with this run. (templated)\n:param role: IAM role supplied for job execution. (templated)\n:param rule_set_names: A list of ruleset names for evaluation. (templated)\n:param number_of_workers: The number of G.1X workers to b",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.210167",
      "relevance_keywords": [
        "quality",
        "operator",
        "set",
        "rule",
        "gluedataqualityrulesetevaluationrunoperator",
        "evaluation",
        "data",
        "run",
        "glue",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_gluedataqualityrulerecommendationrunoperator_20260128_160330",
      "component_name": "GlueDataQualityRuleRecommendationRunOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueDataQualityRuleRecommendationRunOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "datasource",
          "type": "dict",
          "required": false
        },
        {
          "name": "role",
          "type": "str",
          "required": false
        },
        {
          "name": "number_of_workers",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "timeout",
          "type": "int",
          "required": false,
          "default": 2880
        },
        {
          "name": "show_results",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "recommendation_run_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport urllib.parse\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.glue import GlueDataQualityHook, GlueJobHook\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.links.glue import GlueJobRunDetailsLink\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts a recommendation run that is used to generate rules, Glue Data Quality analyzes the data and comes up with recommendations for a potential ruleset.\n\nRecommendation runs are automatically deleted after 90 days.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GlueDataQualityRuleRecommendationRunOperator`\n\n:param datasource: The data source (Glue table) associated with this run. (templated)\n:param role: IAM role supplied ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.210518",
      "relevance_keywords": [
        "gluedataqualityrulerecommendationrunoperator",
        "quality",
        "operator",
        "rule",
        "data",
        "run",
        "glue",
        "amazon",
        "s3",
        "cloud",
        "recommendation",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_gluecrawleroperator_20260128_160330",
      "component_name": "GlueCrawlerOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueCrawlerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config",
          "type": "Any",
          "required": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.glue_crawler import GlueCrawlerHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.glue_crawler import GlueCrawlerCompleteTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates, updates and triggers an AWS Glue Crawler.\n\nAWS Glue Crawler is a serverless service that manages a catalog of\nmetadata tables that contain the inferred schema, format and data\ntypes of data stores within the AWS cloud.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GlueCrawlerOperator`\n\n:param config: Configurations for the AWS Glue crawler\n:param poll_interval: Time (in seconds) to wait between two consecutive call",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.227959",
      "relevance_keywords": [
        "gluecrawleroperator",
        "operator",
        "glue",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "crawler"
      ]
    },
    {
      "id": "official_gluedatabrewstartjoboperator_20260128_160330",
      "component_name": "GlueDataBrewStartJobOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueDataBrewStartJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "delay",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.glue_databrew import GlueDataBrewHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.glue_databrew import GlueDataBrewJobCompleteTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Start an AWS Glue DataBrew job.\n\nAWS Glue DataBrew is a visual data preparation tool that makes it easier\nfor data analysts and data scientists to clean and normalize data\nto prepare it for analytics and machine learning (ML).\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GlueDataBrewStartJobOperator`\n\n:param job_name: unique job name per AWS Account\n:param wait_for_completion: Whether to wait for job run completion. (defau",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.241222",
      "relevance_keywords": [
        "operator",
        "gluedatabrewstartjoboperator",
        "start",
        "data",
        "glue",
        "amazon",
        "s3",
        "cloud",
        "brew",
        "job",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_kinesisanalyticsv2createapplicationoperator_20260128_160330",
      "component_name": "KinesisAnalyticsV2CreateApplicationOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KinesisAnalyticsV2CreateApplicationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_name",
          "type": "str",
          "required": true
        },
        {
          "name": "runtime_environment",
          "type": "str",
          "required": true
        },
        {
          "name": "service_execution_role",
          "type": "str",
          "required": true
        },
        {
          "name": "create_application_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "application_description",
          "type": "str",
          "required": false,
          "default": "Managed Service for Apache Flink application created from Airflow"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.kinesis_analytics import KinesisAnalyticsV2Hook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.kinesis_analytics import (\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "ui_color",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an AWS Managed Service for Apache Flink application.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:KinesisAnalyticsV2CreateApplicationOperator`\n\n:param application_name: The name of application. (templated)\n:param runtime_environment: The runtime environment for the application. (templated)\n:param service_execution_role: The IAM role used by the application to access services. (templated)\n:param create_application_k",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.257181",
      "relevance_keywords": [
        "kinesis",
        "analytics",
        "operator",
        "create",
        "kinesisanalyticsv2createapplicationoperator",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "application"
      ]
    },
    {
      "id": "official_kinesisanalyticsv2startapplicationoperator_20260128_160330",
      "component_name": "KinesisAnalyticsV2StartApplicationOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KinesisAnalyticsV2StartApplicationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_name",
          "type": "str",
          "required": true
        },
        {
          "name": "run_configuration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.kinesis_analytics import KinesisAnalyticsV2Hook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.kinesis_analytics import (\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "ui_color",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts an AWS Managed Service for Apache Flink application.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:KinesisAnalyticsV2StartApplicationOperator`\n\n:param application_name: The name of application. (templated)\n:param run_configuration: Application properties to start Apache Flink Job. (templated)\n\n:param wait_for_completion: Whether to wait for job to stop. (default: True)\n:param waiter_delay: Time in seconds to wait bet",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.257311",
      "relevance_keywords": [
        "kinesis",
        "analytics",
        "operator",
        "start",
        "kinesisanalyticsv2startapplicationoperator",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "application"
      ]
    },
    {
      "id": "official_kinesisanalyticsv2stopapplicationoperator_20260128_160330",
      "component_name": "KinesisAnalyticsV2StopApplicationOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KinesisAnalyticsV2StopApplicationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_name",
          "type": "str",
          "required": true
        },
        {
          "name": "force",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.kinesis_analytics import KinesisAnalyticsV2Hook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.kinesis_analytics import (\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "ui_color",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Stop an AWS Managed Service for Apache Flink application.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:KinesisAnalyticsV2StopApplicationOperator`\n\n:param application_name: The name of your application. (templated)\n:param force: Set to true to force the application to stop. If you set Force to true, Managed Service for\n    Apache Flink stops the application without taking a snapshot. (templated)\n\n:param wait_for_completion:",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.257413",
      "relevance_keywords": [
        "kinesis",
        "analytics",
        "operator",
        "stop",
        "amazon",
        "kinesisanalyticsv2stopapplicationoperator",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "application"
      ]
    },
    {
      "id": "official_lambdacreatefunctionoperator_20260128_160330",
      "component_name": "LambdaCreateFunctionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "LambdaCreateFunctionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "function_name",
          "type": "str",
          "required": false
        },
        {
          "name": "runtime",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "role",
          "type": "str",
          "required": false
        },
        {
          "name": "handler",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "code",
          "type": "dict",
          "required": false
        },
        {
          "name": "description",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 15
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.lambda_function import LambdaCreateFunctionCompleteTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an AWS Lambda function.\n\nMore information regarding parameters of this operator can be found here\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/lambda.html#Lambda.Client.create_function\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:LambdaCreateFunctionOperator`\n\n:param function_name: The name of the AWS Lambda function, version, or alias.\n:param runtime: The identifier of the function's ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.287409",
      "relevance_keywords": [
        "operator",
        "create",
        "function",
        "lambdacreatefunctionoperator",
        "lambda",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_lambdainvokefunctionoperator_20260128_160330",
      "component_name": "LambdaInvokeFunctionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "LambdaInvokeFunctionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "function_name",
          "type": "str",
          "required": false
        },
        {
          "name": "log_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "keep_empty_log_lines",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "qualifier",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "invocation_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "client_context",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "payload",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.lambda_function import LambdaCreateFunctionCompleteTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Invokes an AWS Lambda function.\n\nYou can invoke a function synchronously (and wait for the response), or asynchronously.\nTo invoke a function asynchronously, set `invocation_type` to `Event`. For more details,\nreview the boto3 Lambda invoke docs.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:LambdaInvokeFunctionOperator`\n\n:param function_name: The name of the AWS Lambda function, version, or alias.\n:param log_type: Set to T",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.287529",
      "relevance_keywords": [
        "operator",
        "lambdainvokefunctionoperator",
        "function",
        "lambda",
        "amazon",
        "s3",
        "invoke",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_mwaatriggerdagrunoperator_20260128_160330",
      "component_name": "MwaaTriggerDagRunOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "MwaaTriggerDagRunOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "env_name",
          "type": "str",
          "required": false
        },
        {
          "name": "trigger_dag_id",
          "type": "str",
          "required": false
        },
        {
          "name": "trigger_run_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "logical_date",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "data_interval_start",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "data_interval_end",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "conf",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "note",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "airflow_version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, Literal\nfrom airflow.providers.amazon.aws.hooks.mwaa import MwaaHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.mwaa import MwaaDagRunCompletedTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute_complete",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Trigger a Dag Run for a Dag in an Amazon MWAA environment.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:MwaaTriggerDagRunOperator`\n\n:param env_name: The MWAA environment name (templated)\n:param trigger_dag_id: The ID of the DAG to be triggered (templated)\n:param trigger_run_id: The Run ID. This together with trigger_dag_id are a unique key. (templated)\n:param logical_date: The logical date (previously called execution date",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.315571",
      "relevance_keywords": [
        "dag",
        "operator",
        "trigger",
        "mwaa",
        "run",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "mwaatriggerdagrunoperator",
        "ec2"
      ]
    },
    {
      "id": "official_neptunestartdbclusteroperator_20260128_160330",
      "component_name": "NeptuneStartDbClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "NeptuneStartDbClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.neptune import NeptuneHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.neptune import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts an Amazon Neptune DB cluster.\n\nAmazon Neptune Database is a serverless graph database designed for superior scalability\nand availability. Neptune Database provides built-in security, continuous backups, and\nintegrations with other AWS services\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:NeptuneStartDbClusterOperator`\n\n:param db_cluster_id: The DB cluster identifier of the Neptune DB cluster to be started.\n:param wa",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.328041",
      "relevance_keywords": [
        "db",
        "operator",
        "neptunestartdbclusteroperator",
        "cluster",
        "start",
        "neptune",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_neptunestopdbclusteroperator_20260128_160330",
      "component_name": "NeptuneStopDbClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "NeptuneStopDbClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.neptune import NeptuneHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.neptune import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Stops an Amazon Neptune DB cluster.\n\nAmazon Neptune Database is a serverless graph database designed for superior scalability\nand availability. Neptune Database provides built-in security, continuous backups, and\nintegrations with other AWS services\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:NeptuneStartDbClusterOperator`\n\n:param db_cluster_id: The DB cluster identifier of the Neptune DB cluster to be stopped.\n:param wai",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.328163",
      "relevance_keywords": [
        "db",
        "operator",
        "stop",
        "cluster",
        "neptunestopdbclusteroperator",
        "neptune",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_quicksightcreateingestionoperator_20260128_160330",
      "component_name": "QuickSightCreateIngestionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "QuickSightCreateIngestionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "data_set_id",
          "type": "str",
          "required": true
        },
        {
          "name": "ingestion_id",
          "type": "str",
          "required": true
        },
        {
          "name": "ingestion_type",
          "type": "str",
          "required": false,
          "default": "FULL_REFRESH"
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "check_interval",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.quicksight import QuickSightHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates and starts a new SPICE ingestion for a dataset;  also helps to Refresh existing SPICE datasets.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:QuickSightCreateIngestionOperator`\n\n:param data_set_id:  ID of the dataset used in the ingestion.\n:param ingestion_id: ID for the ingestion.\n:param ingestion_type: Type of ingestion. Values Can be  INCREMENTAL_REFRESH or FULL_REFRESH.\n    Default FULL_REFRESH.\n:param wait_for_",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.339801",
      "relevance_keywords": [
        "operator",
        "quick",
        "create",
        "quicksightcreateingestionoperator",
        "amazon",
        "ingestion",
        "sight",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdsbaseoperator_20260128_160330",
      "component_name": "RdsBaseOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "aws_hook_class",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "execute",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Base operator that implements common functions for all operators.",
      "success_score": 165,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.356740",
      "relevance_keywords": [
        "operator",
        "base",
        "amazon",
        "rdsbaseoperator",
        "s3",
        "cloud",
        "rds",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdscreatedbsnapshotoperator_20260128_160330",
      "component_name": "RdsCreateDbSnapshotOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsCreateDbSnapshotOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_type",
          "type": "str",
          "required": false
        },
        {
          "name": "db_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "db_snapshot_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a snapshot of a DB instance or DB cluster.\n\nThe source DB instance or cluster must be in the available or storage-optimization state.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsCreateDbSnapshotOperator`\n\n:param db_type: Type of the DB - either \"instance\" or \"cluster\"\n:param db_identifier: The identifier of the instance or cluster that you want to create the snapshot of\n:param db_snapshot_identifier: The identi",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.356942",
      "relevance_keywords": [
        "db",
        "operator",
        "create",
        "snapshot",
        "amazon",
        "s3",
        "rdscreatedbsnapshotoperator",
        "cloud",
        "rds",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdscopydbsnapshotoperator_20260128_160330",
      "component_name": "RdsCopyDbSnapshotOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsCopyDbSnapshotOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_type",
          "type": "str",
          "required": false
        },
        {
          "name": "source_db_snapshot_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "target_db_snapshot_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "kms_key_id",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "copy_tags",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "pre_signed_url",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "option_group_name",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "target_custom_availability_zone",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "source_region",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Copies the specified DB instance or DB cluster snapshot.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsCopyDbSnapshotOperator`\n\n:param db_type: Type of the DB - either \"instance\" or \"cluster\"\n:param source_db_snapshot_identifier: The identifier of the source snapshot\n:param target_db_snapshot_identifier: The identifier of the target snapshot\n:param kms_key_id: The AWS KMS key identifier for an encrypted DB snapshot\n:para",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.357135",
      "relevance_keywords": [
        "db",
        "operator",
        "snapshot",
        "rdscopydbsnapshotoperator",
        "amazon",
        "s3",
        "copy",
        "cloud",
        "rds",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdsdeletedbsnapshotoperator_20260128_160330",
      "component_name": "RdsDeleteDbSnapshotOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsDeleteDbSnapshotOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_type",
          "type": "str",
          "required": false
        },
        {
          "name": "db_snapshot_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a DB instance or cluster snapshot or terminating the copy operation.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsDeleteDbSnapshotOperator`\n\n:param db_type: Type of the DB - either \"instance\" or \"cluster\"\n:param db_snapshot_identifier: The identifier for the DB instance or DB cluster snapshot\n:param region_name: AWS region_name. If not specified then the default boto3 behaviour is used.\n:param verify: Whether or",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.357296",
      "relevance_keywords": [
        "db",
        "operator",
        "rdsdeletedbsnapshotoperator",
        "delete",
        "snapshot",
        "amazon",
        "s3",
        "cloud",
        "rds",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdsstartexporttaskoperator_20260128_160330",
      "component_name": "RdsStartExportTaskOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsStartExportTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "export_task_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "source_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "s3_bucket_name",
          "type": "str",
          "required": false
        },
        {
          "name": "iam_role_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "kms_key_id",
          "type": "str",
          "required": false
        },
        {
          "name": "s3_prefix",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "export_only",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 40
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts an export of a snapshot to Amazon S3. The provided IAM role must have access to the S3 bucket.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsStartExportTaskOperator`\n\n:param export_task_identifier: A unique identifier for the snapshot export task.\n:param source_arn: The Amazon Resource Name (ARN) of the snapshot to export to Amazon S3.\n:param s3_bucket_name: The name of the Amazon S3 bucket to export the snapshot ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.357458",
      "relevance_keywords": [
        "operator",
        "task",
        "start",
        "rdsstartexporttaskoperator",
        "export",
        "amazon",
        "s3",
        "cloud",
        "rds",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdscancelexporttaskoperator_20260128_160330",
      "component_name": "RdsCancelExportTaskOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsCancelExportTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "export_task_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "check_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 40
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Cancels an export task in progress that is exporting a snapshot to Amazon S3.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsCancelExportTaskOperator`\n\n:param export_task_identifier: The identifier of the snapshot export task to cancel\n:param wait_for_completion:  If True, waits for DB snapshot export to cancel. (default: True)\n:param check_interval: The amount of time in seconds to wait between attempts\n:param max_attemp",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.357614",
      "relevance_keywords": [
        "operator",
        "task",
        "export",
        "cancel",
        "rdscancelexporttaskoperator",
        "amazon",
        "s3",
        "cloud",
        "rds",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdscreateeventsubscriptionoperator_20260128_160330",
      "component_name": "RdsCreateEventSubscriptionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsCreateEventSubscriptionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "subscription_name",
          "type": "str",
          "required": false
        },
        {
          "name": "sns_topic_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "source_type",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "event_categories",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "source_ids",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "enabled",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an RDS event notification subscription.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsCreateEventSubscriptionOperator`\n\n:param subscription_name: The name of the subscription (must be less than 255 characters)\n:param sns_topic_arn: The ARN of the SNS topic created for event notification\n:param source_type: The type of source that is generating the events. Valid values: db-instance |\n    db-cluster | db-parameter-",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.357773",
      "relevance_keywords": [
        "event",
        "operator",
        "rdscreateeventsubscriptionoperator",
        "create",
        "amazon",
        "s3",
        "cloud",
        "rds",
        "subscription",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdsdeleteeventsubscriptionoperator_20260128_160330",
      "component_name": "RdsDeleteEventSubscriptionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsDeleteEventSubscriptionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "subscription_name",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes an RDS event notification subscription.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsDeleteEventSubscriptionOperator`\n\n:param subscription_name: The name of the RDS event notification subscription you want to delete\n:param region_name: AWS region_name. If not specified then the default boto3 behaviour is used.\n:param verify: Whether or not to verify SSL certificates. See:\n    https://boto3.amazonaws.com/v1/docum",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.357929",
      "relevance_keywords": [
        "event",
        "operator",
        "delete",
        "amazon",
        "rdsdeleteeventsubscriptionoperator",
        "s3",
        "cloud",
        "rds",
        "subscription",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdscreatedbinstanceoperator_20260128_160330",
      "component_name": "RdsCreateDbInstanceOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsCreateDbInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_instance_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "db_instance_class",
          "type": "str",
          "required": false
        },
        {
          "name": "engine",
          "type": "str",
          "required": false
        },
        {
          "name": "rds_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an RDS DB instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsCreateDbInstanceOperator`\n\n:param db_instance_identifier: The DB instance identifier, must start with a letter and\n    contain from 1 to 63 letters, numbers, or hyphens\n:param db_instance_class: The compute and memory capacity of the DB instance, for example db.m5.large\n:param engine: The name of the database engine to be used for this instance\n:pa",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.358089",
      "relevance_keywords": [
        "db",
        "operator",
        "create",
        "rdscreatedbinstanceoperator",
        "instance",
        "amazon",
        "s3",
        "cloud",
        "rds",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdsdeletedbinstanceoperator_20260128_160330",
      "component_name": "RdsDeleteDbInstanceOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsDeleteDbInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_instance_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "rds_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes an RDS DB Instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsDeleteDbInstanceOperator`\n\n:param db_instance_identifier: The DB instance identifier for the DB instance to be deleted\n:param rds_kwargs: Named arguments to pass to boto3 RDS client function ``delete_db_instance``\n    https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rds.html#RDS.Client.delete_db_instance\n:param wait_for_co",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.358246",
      "relevance_keywords": [
        "db",
        "operator",
        "instance",
        "delete",
        "rdsdeletedbinstanceoperator",
        "amazon",
        "s3",
        "cloud",
        "rds",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdsstartdboperator_20260128_160330",
      "component_name": "RdsStartDbOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsStartDbOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "db_type",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 40
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "_start_db",
          "_wait_until_db_available"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts an RDS DB instance / cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsStartDbOperator`\n\n:param db_identifier: The AWS identifier of the DB to start\n:param db_type: Type of the DB - either \"instance\" or \"cluster\" (default: \"instance\")\n:param wait_for_completion:  If True, waits for DB to start. (default: True)\n:param waiter_delay: Time (in seconds) to wait between two consecutive calls to check DB instance st",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.358413",
      "relevance_keywords": [
        "db",
        "rdsstartdboperator",
        "operator",
        "start",
        "amazon",
        "s3",
        "cloud",
        "rds",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdsstopdboperator_20260128_160330",
      "component_name": "RdsStopDbOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsStopDbOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "db_type",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "db_snapshot_identifier",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 40
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.rds import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "_stop_db"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Stops an RDS DB instance / cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RdsStopDbOperator`\n\n:param db_identifier: The AWS identifier of the DB to stop\n:param db_type: Type of the DB - either \"instance\" or \"cluster\" (default: \"instance\")\n:param db_snapshot_identifier: The instance identifier of the DB Snapshot to create before\n    stopping the DB instance. The default value (None) skips snapshot creation. This\n    ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.358573",
      "relevance_keywords": [
        "db",
        "operator",
        "stop",
        "rdsstopdboperator",
        "amazon",
        "s3",
        "cloud",
        "rds",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_redshiftcreateclusteroperator_20260128_160330",
      "component_name": "RedshiftCreateClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftCreateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "node_type",
          "type": "str",
          "required": false
        },
        {
          "name": "master_username",
          "type": "str",
          "required": false
        },
        {
          "name": "master_user_password",
          "type": "str",
          "required": false
        },
        {
          "name": "cluster_type",
          "type": "str",
          "required": false,
          "default": "multi-node"
        },
        {
          "name": "db_name",
          "type": "str",
          "required": false,
          "default": "dev"
        },
        {
          "name": "number_of_nodes",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "cluster_security_groups",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "vpc_security_group_ids",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_subnet_group_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "availability_zone",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "preferred_maintenance_window",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_parameter_group_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "automated_snapshot_retention_period",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "manual_snapshot_retention_period",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.redshift_cluster import (",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "ui_fgcolor",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a new cluster with the specified parameters.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RedshiftCreateClusterOperator`\n\n:param cluster_identifier:  A unique identifier for the cluster.\n:param node_type: The node type to be provisioned for the cluster. Refer\n        https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-clusters.html#rs-node-type-info\n        for the list of available node types.\n:param mas",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.377673",
      "relevance_keywords": [
        "redshiftcreateclusteroperator",
        "operator",
        "cluster",
        "create",
        "amazon",
        "s3",
        "cloud",
        "redshift",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_redshiftcreateclustersnapshotoperator_20260128_160330",
      "component_name": "RedshiftCreateClusterSnapshotOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftCreateClusterSnapshotOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "snapshot_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "cluster_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "retention_period",
          "type": "int",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 15
        },
        {
          "name": "max_attempt",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.redshift_cluster import (",
        "class_attributes": [
          "template_fields",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a manual snapshot of the specified cluster. The cluster must be in the available state.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RedshiftCreateClusterSnapshotOperator`\n\n:param snapshot_identifier: A unique identifier for the snapshot that you are requesting\n:param cluster_identifier: The cluster identifier for which you want a snapshot\n:param retention_period: The number of days that a manual snapshot is retain",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.377852",
      "relevance_keywords": [
        "operator",
        "cluster",
        "create",
        "snapshot",
        "amazon",
        "redshiftcreateclustersnapshotoperator",
        "s3",
        "cloud",
        "redshift",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_redshiftdeleteclustersnapshotoperator_20260128_160330",
      "component_name": "RedshiftDeleteClusterSnapshotOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftDeleteClusterSnapshotOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "snapshot_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "cluster_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.redshift_cluster import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_status"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes the specified manual snapshot.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RedshiftDeleteClusterSnapshotOperator`\n\n:param snapshot_identifier: A unique identifier for the snapshot that you are requesting\n:param cluster_identifier: The unique identifier of the cluster the snapshot was created from\n:param wait_for_completion: Whether wait for cluster deletion or not\n    The default value is ``True``\n:param aws_conn_",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.378128",
      "relevance_keywords": [
        "operator",
        "cluster",
        "redshiftdeleteclustersnapshotoperator",
        "delete",
        "snapshot",
        "amazon",
        "s3",
        "cloud",
        "redshift",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_redshiftresumeclusteroperator_20260128_160330",
      "component_name": "RedshiftResumeClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftResumeClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.redshift_cluster import (",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "ui_fgcolor",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Resume a paused AWS Redshift Cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RedshiftResumeClusterOperator`\n\n:param cluster_identifier:  Unique identifier of the AWS Redshift cluster\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n     If this is ``None`` or empty then the default boto3 behaviour is used. If\n     running Airflow in a distributed manner and aws_conn_id is None or\n     empty, then ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.378293",
      "relevance_keywords": [
        "operator",
        "cluster",
        "resume",
        "amazon",
        "s3",
        "cloud",
        "redshift",
        "aws",
        "ec2",
        "redshiftresumeclusteroperator"
      ]
    },
    {
      "id": "official_redshiftpauseclusteroperator_20260128_160330",
      "component_name": "RedshiftPauseClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftPauseClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.redshift_cluster import (",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "ui_fgcolor",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Pause an AWS Redshift Cluster if it has status `available`.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RedshiftPauseClusterOperator`\n\n:param cluster_identifier: id of the AWS Redshift Cluster\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n     If this is ``None`` or empty then the default boto3 behaviour is used. If\n     running Airflow in a distributed manner and aws_conn_id is None or\n     empty, ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.378568",
      "relevance_keywords": [
        "operator",
        "cluster",
        "amazon",
        "redshiftpauseclusteroperator",
        "pause",
        "s3",
        "cloud",
        "redshift",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_redshiftdeleteclusteroperator_20260128_160330",
      "component_name": "RedshiftDeleteClusterOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftDeleteClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "skip_final_cluster_snapshot",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "final_cluster_snapshot_identifier",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.redshift_cluster import (",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "ui_fgcolor",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete an AWS Redshift cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RedshiftDeleteClusterOperator`\n\n:param cluster_identifier: unique identifier of a cluster\n:param skip_final_cluster_snapshot: determines cluster snapshot creation\n:param final_cluster_snapshot_identifier: name of final cluster snapshot\n:param wait_for_completion: Whether wait for cluster deletion or not\n    The default value is ``True``\n:param aws",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.378743",
      "relevance_keywords": [
        "redshiftdeleteclusteroperator",
        "operator",
        "cluster",
        "delete",
        "amazon",
        "s3",
        "cloud",
        "redshift",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_redshiftdataoperator_20260128_160330",
      "component_name": "RedshiftDataOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftDataOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "sql",
          "type": "Union",
          "required": true
        },
        {
          "name": "database",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_identifier",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "db_user",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "parameters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "secret_arn",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "statement_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "with_event",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "return_sql_result",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "workgroup_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "session_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "session_keep_alive_seconds",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.redshift_data import RedshiftDataHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.redshift_data import RedshiftDataTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "get_sql_results",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Executes SQL Statements against an Amazon Redshift cluster using Redshift Data.\n\n... see also::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:RedshiftDataOperator`\n\n:param database: the name of the database\n:param sql: the SQL statement or list of  SQL statement to run\n:param cluster_identifier: unique identifier of a cluster\n:param db_user: the database username\n:param parameters: the parameters for the SQL statement\n:param secret_arn: ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.394267",
      "relevance_keywords": [
        "operator",
        "redshiftdataoperator",
        "data",
        "amazon",
        "s3",
        "cloud",
        "redshift",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3createbucketoperator_20260128_160330",
      "component_name": "S3CreateBucketOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3CreateBucketOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket_name",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "This operator creates an S3 bucket.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S3CreateBucketOperator`\n\n:param bucket_name: This is bucket name you want to create\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configuration",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.399046",
      "relevance_keywords": [
        "operator",
        "create",
        "bucket",
        "s3createbucketoperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3deletebucketoperator_20260128_160330",
      "component_name": "S3DeleteBucketOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3DeleteBucketOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket_name",
          "type": "str",
          "required": true
        },
        {
          "name": "force_delete",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "This operator deletes an S3 bucket.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S3DeleteBucketOperator`\n\n:param bucket_name: This is bucket name you want to delete\n:param force_delete: Forcibly delete all objects in the bucket before deleting the bucket\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.399329",
      "relevance_keywords": [
        "operator",
        "delete",
        "bucket",
        "amazon",
        "s3deletebucketoperator",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3getbuckettaggingoperator_20260128_160330",
      "component_name": "S3GetBucketTaggingOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3GetBucketTaggingOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket_name",
          "type": "str",
          "required": true
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "This operator gets tagging from an S3 bucket.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S3GetBucketTaggingOperator`\n\n:param bucket_name: This is bucket name you want to reference\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default bo",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.399583",
      "relevance_keywords": [
        "operator",
        "bucket",
        "amazon",
        "s3",
        "s3getbuckettaggingoperator",
        "get",
        "cloud",
        "tagging",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3putbuckettaggingoperator_20260128_160330",
      "component_name": "S3PutBucketTaggingOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3PutBucketTaggingOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket_name",
          "type": "str",
          "required": true
        },
        {
          "name": "key",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "value",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tag_set",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "This operator puts tagging for an S3 bucket.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S3PutBucketTaggingOperator`\n\n:param bucket_name: The name of the bucket to add tags to.\n:param key: The key portion of the key/value pair for a tag to be added.\n    If a key is provided, a value must be provided as well.\n:param value: The value portion of the key/value pair for a tag to be added.\n    If a value is provided, a key must",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.399834",
      "relevance_keywords": [
        "operator",
        "put",
        "s3putbuckettaggingoperator",
        "bucket",
        "amazon",
        "s3",
        "tagging",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3deletebuckettaggingoperator_20260128_160330",
      "component_name": "S3DeleteBucketTaggingOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3DeleteBucketTaggingOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket_name",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "This operator deletes tagging from an S3 bucket.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S3DeleteBucketTaggingOperator`\n\n:param bucket_name: This is the name of the bucket to delete tags from.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty,",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.400080",
      "relevance_keywords": [
        "operator",
        "delete",
        "s3deletebuckettaggingoperator",
        "bucket",
        "amazon",
        "s3",
        "tagging",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3copyobjectoperator_20260128_160330",
      "component_name": "S3CopyObjectOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3CopyObjectOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source_bucket_key",
          "type": "str",
          "required": false
        },
        {
          "name": "dest_bucket_key",
          "type": "str",
          "required": false
        },
        {
          "name": "source_bucket_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "dest_bucket_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "source_version_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "acl_policy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "meta_data_directive",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_start"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a copy of an object that is already stored in S3.\n\nNote: the S3 connection used here needs to have access to both\nsource and destination bucket/key.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S3CopyObjectOperator`\n\n:param source_bucket_key: The key of the source object. (templated)\n\n    It can be either full s3:// style url or relative path from root level.\n\n    When it's specified as a full s3:// url, please omi",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.400327",
      "relevance_keywords": [
        "operator",
        "object",
        "amazon",
        "s3",
        "copy",
        "s3copyobjectoperator",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3createobjectoperator_20260128_160330",
      "component_name": "S3CreateObjectOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3CreateObjectOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "s3_bucket",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "s3_key",
          "type": "str",
          "required": false
        },
        {
          "name": "data",
          "type": "Union",
          "required": false
        },
        {
          "name": "replace",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "encrypt",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "acl_policy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "encoding",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "compression",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_start"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new object from `data` as string or bytes.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S3CreateObjectOperator`\n\n:param s3_bucket: Name of the S3 bucket where to save the object. (templated)\n    It should be omitted when ``s3_key`` is provided as a full s3:// url.\n:param s3_key: The key of the object to be created. (templated)\n    It can be either full s3:// style url or relative path from root level.\n    When it",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.400654",
      "relevance_keywords": [
        "s3createobjectoperator",
        "operator",
        "object",
        "create",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3deleteobjectsoperator_20260128_160330",
      "component_name": "S3DeleteObjectsOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3DeleteObjectsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "keys",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "prefix",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "from_datetime",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "to_datetime",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "To enable users to delete single object or multiple objects from a bucket using a single HTTP request.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S3DeleteObjectsOperator`\n\n:param bucket: Name of the bucket in which you are going to delete object(s). (templated)\n:param keys: The key(s) to delete from S3 bucket. (templated)\n\n    When ``keys`` is a string, it's supposed to be the key name of\n    the single object to delete.",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.400896",
      "relevance_keywords": [
        "s3deleteobjectsoperator",
        "operator",
        "objects",
        "delete",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3filetransformoperator_20260128_160330",
      "component_name": "S3FileTransformOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3FileTransformOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source_s3_key",
          "type": "str",
          "required": false
        },
        {
          "name": "dest_s3_key",
          "type": "str",
          "required": false
        },
        {
          "name": "transform_script",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "select_expression",
          "type": "Any",
          "required": false,
          "default": null
        },
        {
          "name": "select_expr_serialization_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "script_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "source_aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        },
        {
          "name": "source_verify",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "dest_aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        },
        {
          "name": "dest_verify",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "replace",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "ui_color",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_start"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Copies data from a source S3 location to a temporary location on the local filesystem.\n\nRuns a transformation on this file as specified by the transformation\nscript and uploads the output to a destination S3 location.\n\nThe locations of the source and the destination files in the local\nfilesystem is provided as a first and second arguments to the\ntransformation script. The transformation script is expected to read the\ndata from source, transform it and write the output to the local\ndestination fi",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.401123",
      "relevance_keywords": [
        "s3filetransformoperator",
        "transform",
        "file",
        "operator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3listoperator_20260128_160330",
      "component_name": "S3ListOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3ListOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "prefix",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "delimiter",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "apply_wildcard",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "List all objects from the bucket with the given string prefix in name.\n\nThis operator returns a python list with the name of objects which can be\nused by `xcom` in the downstream task.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S3ListOperator`\n\n:param bucket: The S3 bucket where to find the objects. (templated)\n:param prefix: Prefix string to filters the objects whose name begin with\n    such prefix. (templated)\n:param d",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.401333",
      "relevance_keywords": [
        "operator",
        "s3listoperator",
        "amazon",
        "list",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3listprefixesoperator_20260128_160330",
      "component_name": "S3ListPrefixesOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3ListPrefixesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "prefix",
          "type": "str",
          "required": false
        },
        {
          "name": "delimiter",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport subprocess\nimport sys\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nimport pytz\nfrom dateutil import parser",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "List all subfolders from the bucket with the given string prefix in name.\n\nThis operator returns a python list with the name of all subfolders which\ncan be used by `xcom` in the downstream task.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S3ListPrefixesOperator`\n\n:param bucket: The S3 bucket where to find the subfolders. (templated)\n:param prefix: Prefix string to filter the subfolders whose name begin with\n    such prefi",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.401542",
      "relevance_keywords": [
        "operator",
        "s3listprefixesoperator",
        "amazon",
        "list",
        "aws",
        "s3",
        "cloud",
        "prefixes",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerbaseoperator_20260128_160330",
      "component_name": "SageMakerBaseOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config",
          "type": "dict",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "ui_color",
          "integer_fields"
        ],
        "methods": [
          "__init__",
          "parse_integer",
          "parse_config_integers",
          "expand_role",
          "preprocess_config",
          "_create_integer_fields",
          "_get_unique_job_name",
          "_get_unique_name",
          "_check_resource_type",
          "_check_if_job_exists",
          "_check_if_resource_exists",
          "execute",
          "path_to_s3_dataset"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "This is the base operator for all SageMaker operators.\n\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configuration would be used (and must be\n    maintained on each worker node).\n:param region_name: AWS region_name. If not specified then the default boto3 behaviour is used.\n:param verify: Whether",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.430046",
      "relevance_keywords": [
        "operator",
        "sagemakerbaseoperator",
        "sage",
        "maker",
        "base",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerprocessingoperator_20260128_160330",
      "component_name": "SageMakerProcessingOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerProcessingOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config",
          "type": "dict",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "print_log",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "check_interval",
          "type": "int",
          "required": false,
          "default": "<CHECK_INTERVAL_SECOND>"
        },
        {
          "name": "max_attempts",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "max_ingestion_time",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "action_if_job_exists",
          "type": "str",
          "required": false,
          "default": "timestamp"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_create_integer_fields",
          "expand_role",
          "execute",
          "execute_complete",
          "get_openlineage_facets_on_complete",
          "_extract_s3_dataset_identifiers"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Use Amazon SageMaker Processing to analyze data and evaluate machine learning models on Amazon SageMaker.\n\nWith Processing, you can use a simplified, managed experience on SageMaker\nto run your data processing workloads, such as feature engineering, data\nvalidation, model evaluation, and model interpretation.\n\n .. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerProcessingOperator`\n\n:param config: The configuration necess",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.430373",
      "relevance_keywords": [
        "operator",
        "sagemakerprocessingoperator",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "processing",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerendpointconfigoperator_20260128_160330",
      "component_name": "SageMakerEndpointConfigOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerEndpointConfigOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config",
          "type": "dict",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_create_integer_fields",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an endpoint configuration that Amazon SageMaker hosting services uses to deploy models.\n\nIn the configuration, you identify one or more models, created using the CreateModel API, to deploy and\nthe resources that you want Amazon SageMaker to provision.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerEndpointConfigOperator`\n\n:param config: The configuration necessary to create an endpoint config.\n\n    For detai",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.430621",
      "relevance_keywords": [
        "operator",
        "endpoint",
        "sagemakerendpointconfigoperator",
        "sage",
        "maker",
        "amazon",
        "config",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerendpointoperator_20260128_160330",
      "component_name": "SageMakerEndpointOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerEndpointOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config",
          "type": "dict",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "check_interval",
          "type": "int",
          "required": false,
          "default": "<CHECK_INTERVAL_SECOND>"
        },
        {
          "name": "max_ingestion_time",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "operation",
          "type": "str",
          "required": false,
          "default": "create"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_create_integer_fields",
          "expand_role",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "When you create a serverless endpoint, SageMaker provisions and manages the compute resources for you.\n\nThen, you can make inference requests to the endpoint and receive model predictions\nin response. SageMaker scales the compute resources up and down as needed to handle\nyour request traffic.\n\nRequires an Endpoint Config.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerEndpointOperator`\n\n:param config:\n    The config",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.430883",
      "relevance_keywords": [
        "operator",
        "endpoint",
        "sage",
        "sagemakerendpointoperator",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakertransformoperator_20260128_160330",
      "component_name": "SageMakerTransformOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerTransformOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config",
          "type": "dict",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "check_interval",
          "type": "int",
          "required": false,
          "default": "<CHECK_INTERVAL_SECOND>"
        },
        {
          "name": "max_attempts",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "max_ingestion_time",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "check_if_job_exists",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "action_if_job_exists",
          "type": "str",
          "required": false,
          "default": "timestamp"
        },
        {
          "name": "check_if_model_exists",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "action_if_model_exists",
          "type": "str",
          "required": false,
          "default": "timestamp"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_create_integer_fields",
          "expand_role",
          "execute",
          "_get_unique_model_name",
          "_check_if_model_exists",
          "execute_complete",
          "serialize_result",
          "get_openlineage_facets_on_complete",
          "_get_model_data_urls"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts a transform job.\n\nA transform job uses a trained model to get inferences on a dataset\nand saves these results to an Amazon S3 location that you specify.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerTransformOperator`\n\n:param config: The configuration necessary to start a transform job (templated).\n\n    If you need to create a SageMaker transform job based on an existed SageMaker model::\n\n        config = tr",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.431137",
      "relevance_keywords": [
        "transform",
        "operator",
        "sagemakertransformoperator",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakertuningoperator_20260128_160330",
      "component_name": "SageMakerTuningOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerTuningOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config",
          "type": "dict",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "check_interval",
          "type": "int",
          "required": false,
          "default": "<CHECK_INTERVAL_SECOND>"
        },
        {
          "name": "max_ingestion_time",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "expand_role",
          "_create_integer_fields",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts a hyperparameter tuning job.\n\nA hyperparameter tuning job finds the best version of a model by running\nmany training jobs on your dataset using the algorithm you choose and\nvalues for hyperparameters within ranges that you specify. It then chooses\nthe hyperparameter values that result in a model that performs the best,\nas measured by an objective metric that you choose.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:S",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.431509",
      "relevance_keywords": [
        "sagemakertuningoperator",
        "operator",
        "sage",
        "maker",
        "tuning",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakermodeloperator_20260128_160330",
      "component_name": "SageMakerModelOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerModelOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config",
          "type": "dict",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "expand_role",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a model in Amazon SageMaker.\n\nIn the request, you name the model and describe a primary container. For the\nprimary container, you specify the Docker image that contains inference code,\nartifacts (from prior training), and a custom environment map that the inference\ncode uses when you deploy the model for predictions.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerModelOperator`\n\n:param config: The configurat",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.431847",
      "relevance_keywords": [
        "model",
        "operator",
        "sagemakermodeloperator",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakertrainingoperator_20260128_160330",
      "component_name": "SageMakerTrainingOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerTrainingOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config",
          "type": "dict",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "print_log",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "check_interval",
          "type": "int",
          "required": false,
          "default": "<CHECK_INTERVAL_SECOND>"
        },
        {
          "name": "max_attempts",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "max_ingestion_time",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "check_if_job_exists",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "action_if_job_exists",
          "type": "str",
          "required": false,
          "default": "timestamp"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "expand_role",
          "_create_integer_fields",
          "execute",
          "execute_complete",
          "serialize_result",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts a model training job.\n\nAfter training completes, Amazon SageMaker saves the resulting\nmodel artifacts to an Amazon S3 location that you specify.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerTrainingOperator`\n\n:param config: The configuration necessary to start a training job (templated).\n\n    For details of the configuration parameter see :py:meth:`SageMaker.Client.create_training_job`\n:param aws_conn_id: T",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.432196",
      "relevance_keywords": [
        "operator",
        "training",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2",
        "sagemakertrainingoperator"
      ]
    },
    {
      "id": "official_sagemakerdeletemodeloperator_20260128_160330",
      "component_name": "SageMakerDeleteModelOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerDeleteModelOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config",
          "type": "dict",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a SageMaker model.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerDeleteModelOperator`\n\n:param config: The configuration necessary to delete the model.\n    For details of the configuration parameter see :py:meth:`SageMaker.Client.delete_model`\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.432670",
      "relevance_keywords": [
        "model",
        "operator",
        "sage",
        "delete",
        "sagemakerdeletemodeloperator",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerstartpipelineoperator_20260128_160330",
      "component_name": "SageMakerStartPipelineOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerStartPipelineOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "pipeline_name",
          "type": "str",
          "required": false
        },
        {
          "name": "display_name",
          "type": "str",
          "required": false,
          "default": "airflow-triggered-execution"
        },
        {
          "name": "pipeline_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "check_interval",
          "type": "int",
          "required": false,
          "default": "<CHECK_INTERVAL_SECOND>"
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 9999
        },
        {
          "name": "verbose",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts a SageMaker pipeline execution.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerStartPipelineOperator`\n\n:param config: The configuration to start the pipeline execution.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default b",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.433011",
      "relevance_keywords": [
        "operator",
        "start",
        "sage",
        "maker",
        "pipeline",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "sagemakerstartpipelineoperator"
      ]
    },
    {
      "id": "official_sagemakerstoppipelineoperator_20260128_160330",
      "component_name": "SageMakerStopPipelineOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerStopPipelineOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "pipeline_exec_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "check_interval",
          "type": "int",
          "required": false,
          "default": "<CHECK_INTERVAL_SECOND>"
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 9999
        },
        {
          "name": "verbose",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "fail_if_not_running",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Stops a SageMaker pipeline execution.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerStopPipelineOperator`\n\n:param config: The configuration to start the pipeline execution.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default bot",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.433277",
      "relevance_keywords": [
        "operator",
        "sagemakerstoppipelineoperator",
        "stop",
        "sage",
        "maker",
        "pipeline",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerregistermodelversionoperator_20260128_160330",
      "component_name": "SageMakerRegisterModelVersionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerRegisterModelVersionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "image_uri",
          "type": "str",
          "required": false
        },
        {
          "name": "model_url",
          "type": "str",
          "required": false
        },
        {
          "name": "package_group_name",
          "type": "str",
          "required": false
        },
        {
          "name": "package_group_desc",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "package_desc",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "model_approval",
          "type": "ApprovalStatus",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "extras",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Register a SageMaker model by creating a model version that specifies the model group to which it belongs.\n\nWill create the model group if it does not exist already.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerRegisterModelVersionOperator`\n\n:param image_uri: The Amazon EC2 Container Registry (Amazon ECR) path where inference code is stored.\n:param model_url: The Amazon S3 path where the model artifacts (the train",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.433533",
      "relevance_keywords": [
        "sagemakerregistermodelversionoperator",
        "model",
        "operator",
        "version",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "register",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerautomloperator_20260128_160330",
      "component_name": "SageMakerAutoMLOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerAutoMLOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "s3_input",
          "type": "str",
          "required": false
        },
        {
          "name": "target_attribute",
          "type": "str",
          "required": false
        },
        {
          "name": "s3_output",
          "type": "str",
          "required": false
        },
        {
          "name": "role_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "compressed_input",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "time_limit",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "autodeploy_endpoint_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "extras",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "check_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an auto ML job, learning to predict the given column from the data provided through S3.\n\nThe learning output is written to the specified S3 location.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerAutoMLOperator`\n\n:param job_name: Name of the job to create, needs to be unique within the account.\n:param s3_input: The S3 location (folder or file) where to fetch the data.\n    By default, it expects csv with hea",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.433795",
      "relevance_keywords": [
        "sagemakerautomloperator",
        "auto",
        "operator",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakercreateexperimentoperator_20260128_160330",
      "component_name": "SageMakerCreateExperimentOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerCreateExperimentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "description",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a SageMaker experiment, to be then associated to jobs etc.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerCreateExperimentOperator`\n\n:param name: name of the experiment, must be unique within the AWS account\n:param description: description of the experiment, optional\n:param tags: tags to attach to the experiment, optional\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``N",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.434054",
      "relevance_keywords": [
        "operator",
        "create",
        "sage",
        "maker",
        "sagemakercreateexperimentoperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2",
        "experiment"
      ]
    },
    {
      "id": "official_sagemakercreatenotebookoperator_20260128_160330",
      "component_name": "SageMakerCreateNotebookOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerCreateNotebookOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "instance_type",
          "type": "str",
          "required": false
        },
        {
          "name": "role_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "volume_size_in_gb",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "volume_kms_key_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "lifecycle_config_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "direct_internet_access",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "root_access",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "create_instance_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a SageMaker notebook.\n\nMore information regarding parameters of this operator can be found here\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_notebook_instance.html.\n\n.. seealso:\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerCreateNotebookOperator`\n\n:param instance_name: The name of the notebook instance.\n:param instance_type: The type of instance to create.\n:param role",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.434319",
      "relevance_keywords": [
        "operator",
        "create",
        "notebook",
        "sagemakercreatenotebookoperator",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerstopnotebookoperator_20260128_160330",
      "component_name": "SageMakerStopNotebookOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerStopNotebookOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_name",
          "type": "str",
          "required": true
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Stop a notebook instance.\n\n.. seealso:\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerStopNotebookOperator`\n\n:param instance_name: The name of the notebook instance to stop.\n:param wait_for_completion: Whether or not to wait for the notebook to be stopped before returning\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running A",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.434612",
      "relevance_keywords": [
        "operator",
        "stop",
        "sagemakerstopnotebookoperator",
        "notebook",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerdeletenotebookoperator_20260128_160330",
      "component_name": "SageMakerDeleteNotebookOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerDeleteNotebookOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_name",
          "type": "str",
          "required": true
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "template_fields",
          "aws_hook_class",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a notebook instance.\n\n.. seealso:\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerDeleteNotebookOperator`\n\n:param instance_name: The name of the notebook instance to delete.\n:param wait_for_completion: Whether or not to wait for the notebook to delete before returning.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    runnin",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.434997",
      "relevance_keywords": [
        "operator",
        "sagemakerdeletenotebookoperator",
        "notebook",
        "sage",
        "delete",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerstartnotebookoperator_20260128_160330",
      "component_name": "SageMakerStartNoteBookOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerStartNoteBookOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_name",
          "type": "str",
          "required": true
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport time\nimport urllib\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sagemaker import (",
        "class_attributes": [
          "template_fields",
          "aws_hook_class",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Start a notebook instance.\n\n.. seealso:\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SageMakerStartNotebookOperator`\n\n:param instance_name: The name of the notebook instance to start.\n:param wait_for_completion: Whether or not to wait for notebook to be InService before returning\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.435264",
      "relevance_keywords": [
        "sagemakerstartnotebookoperator",
        "note",
        "book",
        "operator",
        "start",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakernotebookoperator_20260128_160330",
      "component_name": "SageMakerNotebookOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerNotebookOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "task_id",
          "type": "str",
          "required": true
        },
        {
          "name": "input_config",
          "type": "dict",
          "required": true
        },
        {
          "name": "output_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "compute",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "termination_condition",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 1440
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sagemaker_unified_studio import (\nfrom airflow.providers.amazon.aws.links.sagemaker_unified_studio import (",
        "class_attributes": [
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "notebook_execution_hook",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Provides Artifact execution functionality for Sagemaker Unified Studio Workflows.\n\nExamples:\n .. code-block:: python\n\n    from airflow.providers.amazon.aws.operators.sagemaker_unified_studio import SageMakerNotebookOperator\n\n    notebook_operator = SageMakerNotebookOperator(\n        task_id=\"notebook_task\",\n        input_config={\"input_path\": \"path/to/notebook.ipynb\", \"input_params\": \"\"},\n        output_config={\"output_format\": \"ipynb\"},\n        wait_for_completion=True,\n        waiter_delay=10,",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.454844",
      "relevance_keywords": [
        "sagemakernotebookoperator",
        "operator",
        "notebook",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_snspublishoperator_20260128_160330",
      "component_name": "SnsPublishOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SnsPublishOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "target_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "message",
          "type": "str",
          "required": false
        },
        {
          "name": "subject",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "message_attributes",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "message_deduplication_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "message_group_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sns import SnsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Publish a message to Amazon SNS.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SnsPublishOperator`\n\n:param target_arn: either a TopicArn or an EndpointArn\n:param message: the default message you want to send (templated)\n:param subject: the message subject you want to send (templated)\n:param message_attributes: the message attributes you want to send as a flat dict (data type will be\n    determined automatically)\n:param aws_",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.467325",
      "relevance_keywords": [
        "operator",
        "sns",
        "publish",
        "snspublishoperator",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sqspublishoperator_20260128_160330",
      "component_name": "SqsPublishOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SqsPublishOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "sqs_queue",
          "type": "str",
          "required": false
        },
        {
          "name": "message_content",
          "type": "str",
          "required": false
        },
        {
          "name": "message_attributes",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "delay_seconds",
          "type": "int",
          "required": false,
          "default": 0
        },
        {
          "name": "message_group_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "message_deduplication_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sqs import SqsHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_fields_renderers",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Publish a message to an Amazon SQS queue.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SqsPublishOperator`\n\n:param sqs_queue: The SQS queue url (templated)\n:param message_content: The message content (templated)\n:param message_attributes: additional attributes for the message (default: None)\n    For details of the attributes parameter see :py:meth:`botocore.client.SQS.send_message`\n:param delay_seconds: message delay (temp",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.479702",
      "relevance_keywords": [
        "operator",
        "publish",
        "sqspublishoperator",
        "amazon",
        "s3",
        "cloud",
        "sqs",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ssmruncommandoperator_20260128_160330",
      "component_name": "SsmRunCommandOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SsmRunCommandOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "document_name",
          "type": "str",
          "required": false
        },
        {
          "name": "run_command_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 75
        },
        {
          "name": "fail_on_nonzero_exit",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import WaiterError\nfrom airflow.providers.amazon.aws.hooks.ssm import SsmHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.ssm import SsmRunCommandTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute_complete",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Executes the SSM Run Command to perform actions on managed instances.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the\n    guide:\n    :ref:`howto/operator:SsmRunCommandOperator`\n\n:param document_name: The name of the Amazon Web Services Systems Manager\n    document (SSM document) to run.\n:param run_command_kwargs: Optional parameters to pass to the send_command\n    API.\n\n:param wait_for_completion: Whether to wait for cluster to stop.\n    (default: True)\n:pa",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.491576",
      "relevance_keywords": [
        "operator",
        "run",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "ssmruncommandoperator",
        "ssm",
        "aws",
        "command"
      ]
    },
    {
      "id": "official_ssmgetcommandinvocationoperator_20260128_160330",
      "component_name": "SsmGetCommandInvocationOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SsmGetCommandInvocationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "command_id",
          "type": "str",
          "required": false
        },
        {
          "name": "instance_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import WaiterError\nfrom airflow.providers.amazon.aws.hooks.ssm import SsmHook\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.ssm import SsmRunCommandTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Retrieves the output and execution details of an SSM command invocation.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the\n    guide:\n    :ref:`howto/operator:SsmGetCommandInvocationOperator`\n\n:param command_id: The ID of the SSM command to retrieve output for.\n:param instance_id: The ID of the specific instance to retrieve output\n    for. If not provided, retrieves output from all instances that\n    executed the command.\n:param aws_conn_id: The Airflow conne",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.491708",
      "relevance_keywords": [
        "operator",
        "invocation",
        "ssmgetcommandinvocationoperator",
        "amazon",
        "s3",
        "get",
        "ec2",
        "cloud",
        "ssm",
        "aws",
        "command"
      ]
    },
    {
      "id": "official_stepfunctionstartexecutionoperator_20260128_160330",
      "component_name": "StepFunctionStartExecutionOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StepFunctionStartExecutionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "state_machine_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "is_redrive_execution",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "state_machine_input",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.step_function import StepFunctionHook\nfrom airflow.providers.amazon.aws.links.step_function import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.step_function import StepFunctionsExecutionCompleteTrigger",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "An Operator that begins execution of an AWS Step Function State Machine.\n\nAdditional arguments may be specified and are passed down to the underlying BaseOperator.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StepFunctionStartExecutionOperator`\n\n:param state_machine_arn: ARN of the Step Function State Machine\n:param name: The name of the execution.\n:param is_redrive_execution: Restarts unsuccessful executions of Standard w",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.506137",
      "relevance_keywords": [
        "stepfunctionstartexecutionoperator",
        "execution",
        "operator",
        "step",
        "start",
        "function",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_stepfunctiongetexecutionoutputoperator_20260128_160330",
      "component_name": "StepFunctionGetExecutionOutputOperator",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StepFunctionGetExecutionOutputOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "execution_arn",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.step_function import StepFunctionHook\nfrom airflow.providers.amazon.aws.links.step_function import (\nfrom airflow.providers.amazon.aws.operators.base_aws import AwsBaseOperator\nfrom airflow.providers.amazon.aws.triggers.step_function import StepFunctionsExecutionCompleteTrigger",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "An Operator that returns the output of an AWS Step Function State Machine execution.\n\nAdditional arguments may be specified and are passed down to the underlying BaseOperator.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StepFunctionGetExecutionOutputOperator`\n\n:param execution_arn: ARN of the Step Function State Machine Execution\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:20.506244",
      "relevance_keywords": [
        "execution",
        "operator",
        "step",
        "output",
        "function",
        "amazon",
        "s3",
        "get",
        "stepfunctiongetexecutionoutputoperator",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_athenasensor_20260128_160330",
      "component_name": "AthenaSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AthenaSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "query_execution_id",
          "type": "str",
          "required": false
        },
        {
          "name": "max_retries",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sleep_time",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.athena import AthenaHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the state of the Query until it reaches a terminal state; fails if the query fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:AthenaSensor`\n\n\n:param query_execution_id: query_execution_id to check the state of\n:param max_retries: Number of times to poll for query state before\n    returning the current state, defaults to None\n:param sleep_time: Time in seconds to wait between two consecutive call to\n    check query stat",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.520967",
      "relevance_keywords": [
        "amazon",
        "aws",
        "athena",
        "s3",
        "cloud",
        "sensor",
        "athenasensor",
        "ec2"
      ]
    },
    {
      "id": "official_awsbasesensor_20260128_160330",
      "component_name": "AwsBaseSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AwsBaseSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BaseSensorOperator",
          "AwsBaseHookMixin"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        },
        {
          "name": "region_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "verify",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "botocore_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator, AwsBaseHookMixin",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom airflow.providers.amazon.aws.utils.mixins import (\nfrom airflow.providers.amazon.version_compat import NOTSET, ArgNotSet\nfrom airflow.providers.common.compat.sdk import BaseSensorOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Base AWS (Amazon) Sensor Class for build sensors in top of AWS Hooks.\n\n.. warning::\n    Only for internal usage, this class might be changed, renamed or removed in the future\n    without any further notice.\n\nExamples:\n .. code-block:: python\n\n    from airflow.providers.amazon.aws.hooks.foo_bar import FooBarThinHook, FooBarThickHook\n\n\n    class AwsFooBarSensor(AwsBaseSensor[FooBarThinHook]):\n        aws_hook_class = FooBarThinHook\n\n        def poke(self, context):\n            pass\n\n\n    class Aws",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.532038",
      "relevance_keywords": [
        "base",
        "amazon",
        "s3",
        "awsbasesensor",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_batchsensor_20260128_160330",
      "component_name": "BatchSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BatchSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poke_interval",
          "type": "float",
          "required": false,
          "default": 30
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 4200
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.batch_client import BatchClientHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.batch import BatchJobTrigger\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the Batch Job until it reaches a terminal state; fails if the job fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:BatchSensor`\n\n:param job_id: Batch job_id to check the state for\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, ",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.556242",
      "relevance_keywords": [
        "batch",
        "batchsensor",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_batchcomputeenvironmentsensor_20260128_160330",
      "component_name": "BatchComputeEnvironmentSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BatchComputeEnvironmentSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "compute_environment",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.batch_client import BatchClientHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.batch import BatchJobTrigger\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the Batch environment until it reaches a terminal state; fails if the environment fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:BatchComputeEnvironmentSensor`\n\n:param compute_environment: Batch compute environment name\n\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed ma",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.556339",
      "relevance_keywords": [
        "batchcomputeenvironmentsensor",
        "batch",
        "compute",
        "environment",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_batchjobqueuesensor_20260128_160330",
      "component_name": "BatchJobQueueSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BatchJobQueueSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_queue",
          "type": "str",
          "required": true
        },
        {
          "name": "treat_non_existing_as_deleted",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.batch_client import BatchClientHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.batch import BatchJobTrigger\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the Batch job queue until it reaches a terminal state; fails if the queue fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:BatchJobQueueSensor`\n\n:param job_queue: Batch job queue name\n\n:param treat_non_existing_as_deleted: If True, a non-existing Batch job queue is considered as a deleted\n    queue and as such a valid case.\n\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this i",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.556410",
      "relevance_keywords": [
        "batch",
        "queue",
        "amazon",
        "job",
        "s3",
        "batchjobqueuesensor",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockbasesensor_20260128_160330",
      "component_name": "BedrockBaseSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockBaseSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport abc\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, TypeVar\nfrom airflow.providers.amazon.aws.hooks.bedrock import BedrockAgentHook, BedrockHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.bedrock import (",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "aws_hook_class",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "get_state"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "General sensor behavior for Amazon Bedrock.\n\nSubclasses must implement following methods:\n    - ``get_state()``\n\nSubclasses must set the following fields:\n    - ``INTERMEDIATE_STATES``\n    - ``FAILURE_STATES``\n    - ``SUCCESS_STATES``\n    - ``FAILURE_MESSAGE``\n\n:param deferrable: If True, the sensor will operate in deferrable mode. This mode requires aiobotocore\n    module to be installed.\n    (default: False, but can be overridden in config file by setting default_deferrable to True)",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.587712",
      "relevance_keywords": [
        "bedrock",
        "base",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "sensor",
        "aws",
        "bedrockbasesensor"
      ]
    },
    {
      "id": "official_bedrockcustomizemodelcompletedsensor_20260128_160330",
      "component_name": "BedrockCustomizeModelCompletedSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockCustomizeModelCompletedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BedrockBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 75
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 120
        }
      ],
      "success_factors": [
        "Inherits from BedrockBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport abc\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, TypeVar\nfrom airflow.providers.amazon.aws.hooks.bedrock import BedrockAgentHook, BedrockHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.bedrock import (",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_state"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the model customization job until it reaches a terminal state; fails if the job fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:BedrockCustomizeModelCompletedSensor`\n\n:param job_name: The name of the Bedrock model customization job.\n\n:param deferrable: If True, the sensor will operate in deferrable mode. This mode requires aiobotocore\n    module to be installed.\n    (default: False, but can be overridden ",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.587880",
      "relevance_keywords": [
        "model",
        "completed",
        "customize",
        "bedrock",
        "amazon",
        "s3",
        "cloud",
        "bedrockcustomizemodelcompletedsensor",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockprovisionmodelthroughputcompletedsensor_20260128_160330",
      "component_name": "BedrockProvisionModelThroughputCompletedSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockProvisionModelThroughputCompletedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BedrockBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "model_id",
          "type": "str",
          "required": false
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 20
        }
      ],
      "success_factors": [
        "Inherits from BedrockBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport abc\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, TypeVar\nfrom airflow.providers.amazon.aws.hooks.bedrock import BedrockAgentHook, BedrockHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.bedrock import (",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "get_state",
          "execute"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the provisioned model throughput job until it reaches a terminal state; fails if the job fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:BedrockProvisionModelThroughputCompletedSensor`\n\n:param model_id: The ARN or name of the provisioned throughput.\n\n:param deferrable: If True, the sensor will operate in deferrable more. This mode requires aiobotocore\n    module to be installed.\n    (default: False, but can be overrid",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.587995",
      "relevance_keywords": [
        "throughput",
        "model",
        "completed",
        "bedrockprovisionmodelthroughputcompletedsensor",
        "bedrock",
        "amazon",
        "s3",
        "provision",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockknowledgebaseactivesensor_20260128_160330",
      "component_name": "BedrockKnowledgeBaseActiveSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockKnowledgeBaseActiveSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BedrockBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "knowledge_base_id",
          "type": "str",
          "required": false
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 24
        }
      ],
      "success_factors": [
        "Inherits from BedrockBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport abc\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, TypeVar\nfrom airflow.providers.amazon.aws.hooks.bedrock import BedrockAgentHook, BedrockHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.bedrock import (",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "get_state",
          "execute"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the Knowledge Base status until it reaches a terminal state; fails if creation fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:BedrockKnowledgeBaseActiveSensor`\n\n:param knowledge_base_id: The unique identifier of the knowledge base for which to get information. (templated)\n\n:param deferrable: If True, the sensor will operate in deferrable more. This mode requires aiobotocore\n    module to be installed.\n    (default: F",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.588102",
      "relevance_keywords": [
        "bedrockknowledgebaseactivesensor",
        "bedrock",
        "base",
        "amazon",
        "aws",
        "s3",
        "active",
        "cloud",
        "sensor",
        "knowledge",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockingestionjobsensor_20260128_160330",
      "component_name": "BedrockIngestionJobSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockIngestionJobSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BedrockBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "knowledge_base_id",
          "type": "str",
          "required": false
        },
        {
          "name": "data_source_id",
          "type": "str",
          "required": false
        },
        {
          "name": "ingestion_job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from BedrockBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport abc\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, TypeVar\nfrom airflow.providers.amazon.aws.hooks.bedrock import BedrockAgentHook, BedrockHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.bedrock import (",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "get_state",
          "execute"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the ingestion job status until it reaches a terminal state; fails if creation fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:BedrockIngestionJobSensor`\n\n:param knowledge_base_id: The unique identifier of the knowledge base for which to get information. (templated)\n:param data_source_id: The unique identifier of the data source in the ingestion job. (templated)\n:param ingestion_job_id: The unique identifier of the ing",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.588216",
      "relevance_keywords": [
        "bedrockingestionjobsensor",
        "bedrock",
        "amazon",
        "ingestion",
        "job",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockbatchinferencesensor_20260128_160330",
      "component_name": "BedrockBatchInferenceSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockBatchInferenceSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BedrockBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "success_state",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 75
        }
      ],
      "success_factors": [
        "Inherits from BedrockBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport abc\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any, TypeVar\nfrom airflow.providers.amazon.aws.hooks.bedrock import BedrockAgentHook, BedrockHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.bedrock import (",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "INVALID_SUCCESS_STATE_MESSAGE",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "get_state",
          "execute"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the batch inference job status until it reaches a terminal state; fails if creation fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:BedrockBatchInferenceSensor`\n\n:param job_arn: The Amazon Resource Name (ARN) of the batch inference job. (templated)\n:param success_state: A BedrockBatchInferenceSensor.TargetState; defaults to 'SCHEDULED' (templated)\n\n:param deferrable: If True, the sensor will operate in deferrable more",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.588326",
      "relevance_keywords": [
        "batch",
        "bedrock",
        "inference",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "sensor",
        "bedrockbatchinferencesensor",
        "ec2"
      ]
    },
    {
      "id": "official_cloudformationcreatestacksensor_20260128_160330",
      "component_name": "CloudFormationCreateStackSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFormationCreateStackSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "stack_name",
          "type": "Any",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.cloud_formation import CloudFormationHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Waits for a stack to be created successfully on AWS CloudFormation.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:CloudFormationCreateStackSensor`\n\n:param stack_name: The name of the stack to wait for (templated)\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.601971",
      "relevance_keywords": [
        "create",
        "cloudformationcreatestacksensor",
        "formation",
        "stack",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_cloudformationdeletestacksensor_20260128_160330",
      "component_name": "CloudFormationDeleteStackSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFormationDeleteStackSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "stack_name",
          "type": "str",
          "required": false
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        },
        {
          "name": "region_name",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.cloud_formation import CloudFormationHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Waits for a stack to be deleted successfully on AWS CloudFormation.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:CloudFormationDeleteStackSensor`\n\n:param stack_name: The name of the stack to wait for (templated)\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.602057",
      "relevance_keywords": [
        "stack",
        "formation",
        "delete",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "sensor",
        "cloudformationdeletestacksensor",
        "ec2"
      ]
    },
    {
      "id": "official_comprehendbasesensor_20260128_160330",
      "component_name": "ComprehendBaseSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComprehendBaseSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport abc\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.comprehend import ComprehendHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.comprehend import (\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "get_state"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "General sensor behavior for Amazon Comprehend.\n\nSubclasses must implement following methods:\n    - ``get_state()``\n\nSubclasses must set the following fields:\n    - ``INTERMEDIATE_STATES``\n    - ``FAILURE_STATES``\n    - ``SUCCESS_STATES``\n    - ``FAILURE_MESSAGE``\n\n:param deferrable: If True, the sensor will operate in deferrable mode. This mode requires aiobotocore\n    module to be installed.\n    (default: False, but can be overridden in config file by setting default_deferrable to True)",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.615232",
      "relevance_keywords": [
        "comprehendbasesensor",
        "comprehend",
        "base",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_comprehendstartpiientitiesdetectionjobcompletedsensor_20260128_160330",
      "component_name": "ComprehendStartPiiEntitiesDetectionJobCompletedSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComprehendStartPiiEntitiesDetectionJobCompletedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "ComprehendBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 75
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 120
        }
      ],
      "success_factors": [
        "Inherits from ComprehendBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport abc\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.comprehend import ComprehendHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.comprehend import (\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_state"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the pii entities detection job until it reaches a completed state; fails if the job fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:ComprehendStartPiiEntitiesDetectionJobCompletedSensor`\n\n:param job_id: The id of the Comprehend pii entities detection job.\n\n:param deferrable: If True, the sensor will operate in deferrable mode. This mode requires aiobotocore\n    module to be installed.\n    (default: False,",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.615345",
      "relevance_keywords": [
        "detection",
        "completed",
        "start",
        "comprehend",
        "entities",
        "amazon",
        "job",
        "s3",
        "comprehendstartpiientitiesdetectionjobcompletedsensor",
        "cloud",
        "sensor",
        "aws",
        "ec2",
        "pii"
      ]
    },
    {
      "id": "official_comprehendcreatedocumentclassifiercompletedsensor_20260128_160330",
      "component_name": "ComprehendCreateDocumentClassifierCompletedSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComprehendCreateDocumentClassifierCompletedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "document_classifier_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "fail_on_warnings",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 75
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport abc\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.comprehend import ComprehendHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.comprehend import (\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the document classifier until it reaches a completed state; fails if the job fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:ComprehendCreateDocumentClassifierCompletedSensor`\n\n:param document_classifier_arn: The arn of the Comprehend document classifier.\n:param fail_on_warnings: If set to True, the document classifier training job will throw an error when the\n    status is TRAINED_WITH_WARNING. (default ",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.615443",
      "relevance_keywords": [
        "classifier",
        "comprehendcreatedocumentclassifiercompletedsensor",
        "completed",
        "create",
        "comprehend",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "document",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmstaskbasesensor_20260128_160330",
      "component_name": "DmsTaskBaseSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsTaskBaseSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "replication_task_arn",
          "type": "str",
          "required": true
        },
        {
          "name": "target_statuses",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "termination_statuses",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Contains general sensor behavior for DMS task.\n\nSubclasses should set ``target_statuses`` and ``termination_statuses`` fields.\n\n:param replication_task_arn: AWS DMS replication task ARN\n:param target_statuses: the target statuses, sensor waits until\n    the task reaches any of these states\n:param termination_statuses: the termination statuses, sensor fails when\n    the task reaches any of these states\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.628361",
      "relevance_keywords": [
        "task",
        "dms",
        "base",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "dmstaskbasesensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dmstaskcompletedsensor_20260128_160330",
      "component_name": "DmsTaskCompletedSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsTaskCompletedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "DmsTaskBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from DmsTaskBaseSensor"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.dms import DmsHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [],
        "methods": [
          "__init__"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Pokes DMS task until it is completed.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:DmsTaskCompletedSensor`\n\n:param replication_task_arn: AWS DMS replication task ARN\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configuration wo",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.628448",
      "relevance_keywords": [
        "task",
        "completed",
        "dms",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "dmstaskcompletedsensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_dynamodbvaluesensor_20260128_160330",
      "component_name": "DynamoDBValueSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DynamoDBValueSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "table_name",
          "type": "str",
          "required": true
        },
        {
          "name": "partition_key_name",
          "type": "str",
          "required": true
        },
        {
          "name": "partition_key_value",
          "type": "str",
          "required": true
        },
        {
          "name": "attribute_name",
          "type": "str",
          "required": true
        },
        {
          "name": "attribute_value",
          "type": "Union",
          "required": true
        },
        {
          "name": "sort_key_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sort_key_value",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.dynamodb import DynamoDBHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Waits for an attribute value to be present for an item in a DynamoDB table.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:DynamoDBValueSensor`\n\n:param table_name: DynamoDB table name\n:param partition_key_name: DynamoDB partition key name\n:param partition_key_value: DynamoDB partition key value\n:param attribute_name: DynamoDB attribute name\n:param attribute_value: DynamoDB attribute value\n:param sort_key_name: (optional) DynamoD",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.640295",
      "relevance_keywords": [
        "dynamo",
        "value",
        "amazon",
        "s3",
        "dynamodbvaluesensor",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ec2instancestatesensor_20260128_160330",
      "component_name": "EC2InstanceStateSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EC2InstanceStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "target_state",
          "type": "str",
          "required": false
        },
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.ec2 import EC2Hook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.ec2 import EC2StateSensorTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color",
          "ui_fgcolor",
          "valid_states"
        ],
        "methods": [
          "__init__",
          "execute",
          "poke",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the AWS EC2 instance until the instance reaches the target state.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:EC2InstanceStateSensor`\n\n:param target_state: target state of instance\n:param instance_id: id of the AWS EC2 instance\n:param region_name: (optional) aws region name associated with the client\n:param deferrable: if True, the sensor will run in deferrable mode",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.652035",
      "relevance_keywords": [
        "ec2instancestatesensor",
        "instance",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2",
        "state"
      ]
    },
    {
      "id": "official_ecsbasesensor_20260128_160330",
      "component_name": "EcsBaseSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsBaseSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.ecs import (\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class"
        ],
        "methods": [
          "client"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Contains general sensor behavior for Elastic Container Service.",
      "success_score": 155,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.662703",
      "relevance_keywords": [
        "ecs",
        "ecsbasesensor",
        "base",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ecsclusterstatesensor_20260128_160330",
      "component_name": "EcsClusterStateSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsClusterStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EcsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": false
        },
        {
          "name": "target_state",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "failure_states",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from EcsBaseSensor",
        "Implements poke() method for polling",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.ecs import (\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the cluster state until it reaches a terminal state; raises AirflowException with the failure reason.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/sensor:EcsClusterStateSensor`\n\n:param cluster_name: The name of your cluster.\n:param target_state: Success state to watch for. (Default: \"ACTIVE\")\n:param failure_states: Fail if any of these states are reached before the\n     Success State. (Default: \"FAILED\" or \"INACTIVE\")",
      "success_score": 175,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.662812",
      "relevance_keywords": [
        "cluster",
        "ecs",
        "ecsclusterstatesensor",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2",
        "state"
      ]
    },
    {
      "id": "official_ecstaskdefinitionstatesensor_20260128_160330",
      "component_name": "EcsTaskDefinitionStateSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsTaskDefinitionStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EcsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "task_definition",
          "type": "str",
          "required": false
        },
        {
          "name": "target_state",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from EcsBaseSensor",
        "Implements poke() method for polling",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.ecs import (\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll task definition until it reaches a terminal state; raise AirflowException with the failure reason.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/sensor:EcsTaskDefinitionStateSensor`\n\n:param task_definition: The family for the latest ACTIVE revision, family and\n     revision (family:revision ) for a specific revision in the family, or full\n     Amazon Resource Name (ARN) of the task definition.\n:param target_state: Success state",
      "success_score": 175,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.662883",
      "relevance_keywords": [
        "task",
        "ecstaskdefinitionstatesensor",
        "ecs",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "sensor",
        "definition",
        "ec2",
        "state"
      ]
    },
    {
      "id": "official_ecstaskstatesensor_20260128_160330",
      "component_name": "EcsTaskStateSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsTaskStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EcsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster",
          "type": "str",
          "required": false
        },
        {
          "name": "task",
          "type": "str",
          "required": false
        },
        {
          "name": "target_state",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "failure_states",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from EcsBaseSensor",
        "Implements poke() method for polling",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.ecs import (\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the task state until it reaches a terminal state; raises AirflowException with the failure reason.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/sensor:EcsTaskStateSensor`\n\n:param cluster: The short name or full Amazon Resource Name (ARN) of the cluster that hosts the task.\n:param task: The task ID or full ARN of the task to poll.\n:param target_state: Success state to watch for. (Default: \"ACTIVE\")\n:param failure_states: Fail i",
      "success_score": 175,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.662949",
      "relevance_keywords": [
        "task",
        "ecs",
        "amazon",
        "ecstaskstatesensor",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2",
        "state"
      ]
    },
    {
      "id": "official_eksbasesensor_20260128_160330",
      "component_name": "EksBaseSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksBaseSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": false
        },
        {
          "name": "target_state",
          "type": "Union",
          "required": false
        },
        {
          "name": "target_state_type",
          "type": "type",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom abc import abstractmethod\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.eks import (",
        "class_attributes": [
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "poke",
          "get_state",
          "get_terminal_states"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Base class to check various EKS states.\n\nSubclasses need to implement get_state and get_terminal_states methods.\n\n:param cluster_name: The name of the Cluster\n:param target_state: Will return successfully when that state is reached.\n:param target_state_type: The enum containing the states,\n    will be used to convert the target state if it has to be converted from a string\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.678023",
      "relevance_keywords": [
        "eksbasesensor",
        "eks",
        "base",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_eksclusterstatesensor_20260128_160330",
      "component_name": "EksClusterStateSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksClusterStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EksBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "target_state",
          "type": "ClusterStates",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from EksBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom abc import abstractmethod\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.eks import (",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "get_state",
          "get_terminal_states"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check the state of an Amazon EKS Cluster until it reaches the target state or another terminal state.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:EksClusterStateSensor`\n\n:param cluster_name: The name of the Cluster to watch. (templated)\n:param target_state: Target state of the Cluster. (templated)\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behav",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.678160",
      "relevance_keywords": [
        "cluster",
        "eksclusterstatesensor",
        "eks",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2",
        "state"
      ]
    },
    {
      "id": "official_eksfargateprofilestatesensor_20260128_160330",
      "component_name": "EksFargateProfileStateSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksFargateProfileStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EksBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "fargate_profile_name",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "target_state",
          "type": "FargateProfileStates",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from EksBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom abc import abstractmethod\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.eks import (",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "get_state",
          "get_terminal_states"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check the state of an AWS Fargate profile until it reaches the target state or another terminal state.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:EksFargateProfileStateSensor`\n\n:param cluster_name: The name of the Cluster which the AWS Fargate profile is attached to. (templated)\n:param fargate_profile_name: The name of the Fargate profile to watch. (templated)\n:param target_state: Target state of the Fargate profile. (templa",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.678265",
      "relevance_keywords": [
        "eksfargateprofilestatesensor",
        "eks",
        "amazon",
        "s3",
        "fargate",
        "cloud",
        "profile",
        "sensor",
        "aws",
        "ec2",
        "state"
      ]
    },
    {
      "id": "official_eksnodegroupstatesensor_20260128_160330",
      "component_name": "EksNodegroupStateSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksNodegroupStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EksBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "nodegroup_name",
          "type": "str",
          "required": false
        },
        {
          "name": "target_state",
          "type": "NodegroupStates",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "region",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from EksBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom abc import abstractmethod\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.eks import (",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "get_state",
          "get_terminal_states"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check the state of an EKS managed node group until it reaches the target state or another terminal state.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:EksNodegroupStateSensor`\n\n:param cluster_name: The name of the Cluster which the Nodegroup is attached to. (templated)\n:param nodegroup_name: The name of the Nodegroup to watch. (templated)\n:param target_state: Target state of the Nodegroup. (templated)\n:param aws_conn_id: The A",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.678362",
      "relevance_keywords": [
        "nodegroup",
        "eksnodegroupstatesensor",
        "eks",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2",
        "state"
      ]
    },
    {
      "id": "official_emrbasesensor_20260128_160330",
      "component_name": "EmrBaseSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrBaseSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import EmrClusterLink, EmrLogsLink, get_log_uri\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "get_emr_response",
          "state_from_response",
          "failure_message_from_response"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Contains general sensor behavior for EMR.\n\nSubclasses should implement following methods:\n    - ``get_emr_response()``\n    - ``state_from_response()``\n    - ``failure_message_from_response()``\n\nSubclasses should set ``target_states`` and ``failed_states`` fields.\n\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then d",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.724525",
      "relevance_keywords": [
        "emr",
        "emrbasesensor",
        "base",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrserverlessjobsensor_20260128_160330",
      "component_name": "EmrServerlessJobSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrServerlessJobSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_id",
          "type": "str",
          "required": false
        },
        {
          "name": "job_run_id",
          "type": "str",
          "required": false
        },
        {
          "name": "target_states",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import EmrClusterLink, EmrLogsLink, get_log_uri\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "failure_message_from_response"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the job run until it reaches a terminal state; fails if the job run fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:EmrServerlessJobSensor`\n\n:param application_id: application_id to check the state of\n:param job_run_id: job_run_id to check the state of\n:param target_states: a set of states to wait for, defaults to 'SUCCESS'\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this i",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.724717",
      "relevance_keywords": [
        "emr",
        "serverless",
        "emrserverlessjobsensor",
        "amazon",
        "job",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrserverlessapplicationsensor_20260128_160330",
      "component_name": "EmrServerlessApplicationSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrServerlessApplicationSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_id",
          "type": "str",
          "required": false
        },
        {
          "name": "target_states",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import EmrClusterLink, EmrLogsLink, get_log_uri\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.emr import (",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "failure_message_from_response"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the application until it reaches a terminal state; fails if the application fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:EmrServerlessApplicationSensor`\n\n:param application_id: application_id to check the state of\n:param target_states: a set of states to wait for, defaults to {'CREATED', 'STARTED'}\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty the",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.725151",
      "relevance_keywords": [
        "emr",
        "serverless",
        "amazon",
        "s3",
        "emrserverlessapplicationsensor",
        "ec2",
        "cloud",
        "sensor",
        "aws",
        "application"
      ]
    },
    {
      "id": "official_emrcontainersensor_20260128_160330",
      "component_name": "EmrContainerSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrContainerSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "virtual_cluster_id",
          "type": "str",
          "required": false
        },
        {
          "name": "job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "max_retries",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import EmrClusterLink, EmrLogsLink, get_log_uri\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.emr import (",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "aws_hook_class",
          "template_fields",
          "template_ext",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "_hook_parameters",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the job run until it reaches a terminal state; fail if the job run fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:EmrContainerSensor`\n\n:param job_id: job_id to check the state of\n:param max_retries: Number of times to poll for query state before\n    returning the current state, defaults to None\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the ",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.725524",
      "relevance_keywords": [
        "emr",
        "amazon",
        "emrcontainersensor",
        "container",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrnotebookexecutionsensor_20260128_160330",
      "component_name": "EmrNotebookExecutionSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrNotebookExecutionSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EmrBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "notebook_execution_id",
          "type": "str",
          "required": true
        },
        {
          "name": "target_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "failed_states",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from EmrBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import EmrClusterLink, EmrLogsLink, get_log_uri\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.emr import (",
        "class_attributes": [
          "template_fields",
          "FAILURE_STATES",
          "COMPLETED_STATES"
        ],
        "methods": [
          "__init__",
          "get_emr_response",
          "state_from_response",
          "failure_message_from_response"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the EMR notebook until it reaches any of the target states; raise AirflowException on failure.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:EmrNotebookExecutionSensor`\n\n:param notebook_execution_id: Unique id of the notebook execution to be poked.\n:target_states: the states the sensor will wait for the execution to reach.\n    Default target_states is ``FINISHED``.\n:failed_states: if the execution reaches any of the failed",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.725843",
      "relevance_keywords": [
        "emr",
        "execution",
        "notebook",
        "emrnotebookexecutionsensor",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_emrjobflowsensor_20260128_160330",
      "component_name": "EmrJobFlowSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrJobFlowSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EmrBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_flow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "target_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "failed_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from EmrBaseSensor",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import EmrClusterLink, EmrLogsLink, get_log_uri\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.emr import (",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "get_emr_response",
          "state_from_response",
          "failure_message_from_response",
          "execute",
          "execute_complete"
        ],
        "has_poke": false,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the EMR JobFlow Cluster until it reaches any of the target states; raise AirflowException on failure.\n\nWith the default target states, sensor waits cluster to be terminated.\nWhen target_states is set to ['RUNNING', 'WAITING'] sensor waits\nuntil job flow to be ready (after 'STARTING' and 'BOOTSTRAPPING' states)\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:EmrJobFlowSensor`\n\n:param job_flow_id: job_flow_id to check the stat",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.726137",
      "relevance_keywords": [
        "emr",
        "emrjobflowsensor",
        "amazon",
        "job",
        "aws",
        "s3",
        "cloud",
        "sensor",
        "flow",
        "ec2"
      ]
    },
    {
      "id": "official_emrstepsensor_20260128_160330",
      "component_name": "EmrStepSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrStepSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "EmrBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_flow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "step_id",
          "type": "str",
          "required": false
        },
        {
          "name": "target_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "failed_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "max_attempts",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from EmrBaseSensor",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.emr import EmrContainerHook, EmrHook, EmrServerlessHook\nfrom airflow.providers.amazon.aws.links.emr import EmrClusterLink, EmrLogsLink, get_log_uri\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.emr import (",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "get_emr_response",
          "state_from_response",
          "failure_message_from_response",
          "execute",
          "execute_complete"
        ],
        "has_poke": false,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the step until it reaches any of the target states; raise AirflowException on failure.\n\nWith the default target states, sensor waits step to be completed.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:EmrStepSensor`\n\n:param job_flow_id: job_flow_id which contains the step check the state of\n:param step_id: step to check the state of\n:param target_states: the target states, sensor waits until\n    step reaches a",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.726409",
      "relevance_keywords": [
        "emr",
        "step",
        "emrstepsensor",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_glacierjoboperationsensor_20260128_160330",
      "component_name": "GlacierJobOperationSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlacierJobOperationSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "vault_name",
          "type": "str",
          "required": false
        },
        {
          "name": "job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "mode",
          "type": "str",
          "required": false,
          "default": "reschedule"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom enum import Enum\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.glacier import GlacierHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Glacier sensor for checking job state. This operator runs only in reschedule mode.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:GlacierJobOperationSensor`\n\n:param aws_conn_id: The reference to the AWS connection details\n:param vault_name: name of Glacier vault on which job is executed\n:param job_id: the job ID was returned by retrieve_inventory()\n:param poke_interval: Time in seconds that the job should wait in\n    between eac",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.754380",
      "relevance_keywords": [
        "operation",
        "glacier",
        "glacierjoboperationsensor",
        "amazon",
        "job",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_gluejobsensor_20260128_160330",
      "component_name": "GlueJobSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueJobSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "run_id",
          "type": "str",
          "required": false
        },
        {
          "name": "verbose",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.glue import GlueDataQualityHook, GlueJobHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.glue import (\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event",
        "class_attributes": [
          "SUCCESS_STATES",
          "FAILURE_STATES",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for an AWS Glue Job to reach any of the status below.\n\n'FAILED', 'STOPPED', 'SUCCEEDED'\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:GlueJobSensor`\n\n:param job_name: The AWS Glue Job unique name\n:param run_id: The AWS Glue current running job identifier\n:param verbose: If True, more Glue Job Run logs show in the Airflow Task Logs.  (default: False)\n:param deferrable: If True, the sensor will operate in deferrable mode. T",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.771658",
      "relevance_keywords": [
        "gluejobsensor",
        "glue",
        "amazon",
        "job",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_gluedataqualityrulesetevaluationrunsensor_20260128_160330",
      "component_name": "GlueDataQualityRuleSetEvaluationRunSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueDataQualityRuleSetEvaluationRunSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "evaluation_run_id",
          "type": "str",
          "required": false
        },
        {
          "name": "show_results",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "verify_result_status",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.glue import GlueDataQualityHook, GlueJobHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.glue import (\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event",
        "class_attributes": [
          "SUCCESS_STATES",
          "FAILURE_STATES",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for an AWS Glue data quality ruleset evaluation run to reach any of the status below.\n\n'FAILED', 'STOPPED', 'STOPPING', 'TIMEOUT', 'SUCCEEDED'\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:GlueDataQualityRuleSetEvaluationRunSensor`\n\n:param evaluation_run_id: The AWS Glue data quality ruleset evaluation run identifier.\n:param verify_result_status: Validate all the ruleset rules evaluation run results,\n    If any of the rul",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.771833",
      "relevance_keywords": [
        "quality",
        "set",
        "rule",
        "evaluation",
        "data",
        "run",
        "glue",
        "amazon",
        "s3",
        "cloud",
        "gluedataqualityrulesetevaluationrunsensor",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_gluedataqualityrulerecommendationrunsensor_20260128_160330",
      "component_name": "GlueDataQualityRuleRecommendationRunSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueDataQualityRuleRecommendationRunSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "recommendation_run_id",
          "type": "str",
          "required": false
        },
        {
          "name": "show_results",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.glue import GlueDataQualityHook, GlueJobHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.glue import (\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event",
        "class_attributes": [
          "SUCCESS_STATES",
          "FAILURE_STATES",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for an AWS Glue data quality recommendation run to reach any of the status below.\n\n'FAILED', 'STOPPED', 'STOPPING', 'TIMEOUT', 'SUCCEEDED'\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:GlueDataQualityRuleRecommendationRunSensor`\n\n:param recommendation_run_id: The AWS Glue data quality rule recommendation run identifier.\n:param show_results: Displays the recommended ruleset (a set of rules), when recommendation run complet",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.771964",
      "relevance_keywords": [
        "quality",
        "rule",
        "data",
        "run",
        "glue",
        "amazon",
        "recommendation",
        "s3",
        "cloud",
        "sensor",
        "gluedataqualityrulerecommendationrunsensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_gluecatalogpartitionsensor_20260128_160330",
      "component_name": "GlueCatalogPartitionSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueCatalogPartitionSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "table_name",
          "type": "str",
          "required": false
        },
        {
          "name": "expression",
          "type": "str",
          "required": false,
          "default": "ds='{{ ds }}'"
        },
        {
          "name": "database_name",
          "type": "str",
          "required": false,
          "default": "default"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.glue_catalog import GlueCatalogHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.glue import GlueCatalogPartitionTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute",
          "poke",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for a partition to show up in AWS Glue Catalog.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:GlueCatalogPartitionSensor`\n\n:param table_name: The name of the table to wait for, supports the dot\n    notation (my_database.my_table)\n:param expression: The partition clause to wait for. This is passed as\n    is to the AWS Glue Catalog API's get_partitions function,\n    and supports SQL like notation as in ``ds='2015-01-01'\n   ",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.788987",
      "relevance_keywords": [
        "gluecatalogpartitionsensor",
        "partition",
        "glue",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "sensor",
        "aws",
        "catalog"
      ]
    },
    {
      "id": "official_gluecrawlersensor_20260128_160330",
      "component_name": "GlueCrawlerSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueCrawlerSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "crawler_name",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.glue_crawler import GlueCrawlerHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Waits for an AWS Glue crawler to reach any of the statuses below.\n\n'FAILED', 'CANCELLED', 'SUCCEEDED'\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:GlueCrawlerSensor`\n\n:param crawler_name: The AWS Glue crawler unique name\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.801948",
      "relevance_keywords": [
        "gluecrawlersensor",
        "glue",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "sensor",
        "aws",
        "crawler"
      ]
    },
    {
      "id": "official_kinesisanalyticsv2basesensor_20260128_160330",
      "component_name": "KinesisAnalyticsV2BaseSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KinesisAnalyticsV2BaseSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_name",
          "type": "str",
          "required": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.kinesis_analytics import KinesisAnalyticsV2Hook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.kinesis_analytics import (\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "aws_hook_class",
          "ui_color",
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "SUCCESS_MESSAGE"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": " General sensor behaviour for AWS Managed Service for Apache Flink.\n\nSubclasses must set the following fields:\n    - ``INTERMEDIATE_STATES``\n    - ``FAILURE_STATES``\n    - ``SUCCESS_STATES``\n    - ``FAILURE_MESSAGE``\n    - ``SUCCESS_MESSAGE``\n\n:param application_name: Application name.\n:param deferrable: If True, the sensor will operate in deferrable mode. This mode requires aiobotocore\n    module to be installed.\n    (default: False, but can be overridden in config file by setting default_defer",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.820876",
      "relevance_keywords": [
        "kinesis",
        "analytics",
        "kinesisanalyticsv2basesensor",
        "base",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_kinesisanalyticsv2startapplicationcompletedsensor_20260128_160330",
      "component_name": "KinesisAnalyticsV2StartApplicationCompletedSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KinesisAnalyticsV2StartApplicationCompletedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "KinesisAnalyticsV2BaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_name",
          "type": "str",
          "required": false
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 75
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 120
        }
      ],
      "success_factors": [
        "Inherits from KinesisAnalyticsV2BaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.kinesis_analytics import KinesisAnalyticsV2Hook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.kinesis_analytics import (\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "SUCCESS_MESSAGE",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for AWS Managed Service for Apache Flink application to start.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:KinesisAnalyticsV2StartApplicationCompletedSensor`\n\n:param application_name: Application name.\n\n:param deferrable: If True, the sensor will operate in deferrable mode. This mode requires aiobotocore\n    module to be installed.\n    (default: False, but can be overridden in config file by setting default_deferrable t",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.821169",
      "relevance_keywords": [
        "kinesis",
        "analytics",
        "completed",
        "kinesisanalyticsv2startapplicationcompletedsensor",
        "start",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "sensor",
        "aws",
        "application"
      ]
    },
    {
      "id": "official_kinesisanalyticsv2stopapplicationcompletedsensor_20260128_160330",
      "component_name": "KinesisAnalyticsV2StopApplicationCompletedSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KinesisAnalyticsV2StopApplicationCompletedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "KinesisAnalyticsV2BaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_name",
          "type": "str",
          "required": false
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 75
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 120
        }
      ],
      "success_factors": [
        "Inherits from KinesisAnalyticsV2BaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.kinesis_analytics import KinesisAnalyticsV2Hook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.kinesis_analytics import (\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "SUCCESS_MESSAGE",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for AWS Managed Service for Apache Flink application to stop.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:KinesisAnalyticsV2StopApplicationCompletedSensor`\n\n:param application_name: Application name.\n\n:param deferrable: If True, the sensor will operate in deferrable mode. This mode requires aiobotocore\n    module to be installed.\n    (default: False, but can be overridden in config file by setting default_deferrable to ",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.821337",
      "relevance_keywords": [
        "kinesis",
        "analytics",
        "stop",
        "completed",
        "kinesisanalyticsv2stopapplicationcompletedsensor",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "sensor",
        "aws",
        "application"
      ]
    },
    {
      "id": "official_lambdafunctionstatesensor_20260128_160330",
      "component_name": "LambdaFunctionStateSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "LambdaFunctionStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "function_name",
          "type": "str",
          "required": false
        },
        {
          "name": "qualifier",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "target_states",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.lambda_function import LambdaHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils import trim_none_values\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "FAILURE_STATES",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the deployment state of the AWS Lambda function until it reaches a target state.\n\nFails if the query fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:LambdaFunctionStateSensor`\n\n:param function_name: The name of the AWS Lambda function, version, or alias.\n:param qualifier: Specify a version or alias to get details about a published version of the function.\n:param target_states: The Lambda states desired.\n:param aws_con",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.838245",
      "relevance_keywords": [
        "function",
        "lambda",
        "amazon",
        "lambdafunctionstatesensor",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2",
        "state"
      ]
    },
    {
      "id": "official_mwaadagrunsensor_20260128_160330",
      "component_name": "MwaaDagRunSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "MwaaDagRunSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "external_env_name",
          "type": "str",
          "required": false
        },
        {
          "name": "external_dag_id",
          "type": "str",
          "required": false
        },
        {
          "name": "external_dag_run_id",
          "type": "str",
          "required": false
        },
        {
          "name": "success_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "failure_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "airflow_version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 720
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING, Any, Literal\nfrom airflow.providers.amazon.aws.hooks.mwaa import MwaaHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.mwaa import MwaaDagRunCompletedTrigger, MwaaTaskCompletedTrigger\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.utils.state import DagRunState, TaskInstanceState",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute_complete",
          "execute"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for a DAG Run in an MWAA Environment to complete.\n\nIf the DAG Run fails, an AirflowException is thrown.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:MwaaDagRunSensor`\n\n:param external_env_name: The external MWAA environment name that contains the DAG Run you want to wait for\n    (templated)\n:param external_dag_id: The DAG ID in the external MWAA environment that contains the DAG Run you want to wait for\n    (templated)\n:",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.855384",
      "relevance_keywords": [
        "dag",
        "mwaa",
        "mwaadagrunsensor",
        "run",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_mwaatasksensor_20260128_160330",
      "component_name": "MwaaTaskSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "MwaaTaskSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "external_env_name",
          "type": "str",
          "required": false
        },
        {
          "name": "external_dag_id",
          "type": "str",
          "required": false
        },
        {
          "name": "external_dag_run_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "external_task_id",
          "type": "str",
          "required": false
        },
        {
          "name": "success_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "failure_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "airflow_version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 720
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING, Any, Literal\nfrom airflow.providers.amazon.aws.hooks.mwaa import MwaaHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.mwaa import MwaaDagRunCompletedTrigger, MwaaTaskCompletedTrigger\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.utils.state import DagRunState, TaskInstanceState",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute_complete",
          "execute"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for a task in an MWAA Environment to complete.\n\nIf the task fails, an AirflowException is thrown.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:MwaaTaskSensor`\n\n:param external_env_name: The external MWAA environment name that contains the Task Instance you want to wait for\n    (templated)\n:param external_dag_id: The DAG ID in the external MWAA environment that contains the Task Instance you want to wait for\n    (template",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.855554",
      "relevance_keywords": [
        "task",
        "mwaa",
        "amazon",
        "s3",
        "cloud",
        "mwaatasksensor",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_opensearchserverlesscollectionactivesensor_20260128_160330",
      "component_name": "OpenSearchServerlessCollectionActiveSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "OpenSearchServerlessCollectionActiveSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "collection_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "collection_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 60
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.opensearch_serverless import OpenSearchServerlessHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.opensearch_serverless import (\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.utils.helpers import exactly_one",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of the Collection until it reaches a terminal state; fails if the query fails.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:OpenSearchServerlessCollectionAvailableSensor`\n\n:param collection_id: A collection ID. You can't provide a name and an ID in the same request.\n:param collection_name: A collection name. You can't provide a name and an ID in the same request.\n\n:param deferrable: If True, the sensor will oper",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.888504",
      "relevance_keywords": [
        "opensearchserverlesscollectionactivesensor",
        "open",
        "collection",
        "serverless",
        "search",
        "amazon",
        "s3",
        "active",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_quicksightsensor_20260128_160330",
      "component_name": "QuickSightSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "QuickSightSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [
          "data_set_id",
          "ingestion_id",
          "aws_conn_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "data_set_id",
          "type": "str",
          "required": false
        },
        {
          "name": "ingestion_id",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.quicksight import QuickSightHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Watches for the status of an Amazon QuickSight Ingestion.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:QuickSightSensor`\n\n:param data_set_id:  ID of the dataset used in the ingestion.\n:param ingestion_id: ID for the ingestion.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is ``None`` or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn",
      "success_score": 179,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.903889",
      "relevance_keywords": [
        "quick",
        "quicksightsensor",
        "amazon",
        "s3",
        "sight",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdsbasesensor_20260128_160330",
      "component_name": "RdsBaseSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsBaseSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "hook_params",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.amazon.aws.utils.rds import RdsDbType\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowNotFoundException",
        "class_attributes": [
          "aws_hook_class",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Base operator that implements common functions for all sensors.",
      "success_score": 150,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.922314",
      "relevance_keywords": [
        "base",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "rds",
        "sensor",
        "rdsbasesensor",
        "ec2"
      ]
    },
    {
      "id": "official_rdssnapshotexistencesensor_20260128_160330",
      "component_name": "RdsSnapshotExistenceSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsSnapshotExistenceSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_type",
          "type": "str",
          "required": false
        },
        {
          "name": "db_snapshot_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "target_statuses",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.amazon.aws.utils.rds import RdsDbType\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowNotFoundException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Waits for RDS snapshot with a specific status.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:RdsSnapshotExistenceSensor`\n\n:param db_type: Type of the DB - either \"instance\" or \"cluster\"\n:param db_snapshot_identifier: The identifier for the DB snapshot\n:param target_statuses: Target status of snapshot\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n     If this is ``None`` or empty then the default boto3 beh",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.922452",
      "relevance_keywords": [
        "existence",
        "snapshot",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "rds",
        "sensor",
        "aws",
        "rdssnapshotexistencesensor"
      ]
    },
    {
      "id": "official_rdsexporttaskexistencesensor_20260128_160330",
      "component_name": "RdsExportTaskExistenceSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsExportTaskExistenceSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "export_task_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "target_statuses",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "error_statuses",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.amazon.aws.utils.rds import RdsDbType\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowNotFoundException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Waits for RDS export task with a specific status.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:RdsExportTaskExistenceSensor`\n\n:param export_task_identifier: A unique identifier for the snapshot export task.\n:param target_statuses: Target status of export task\n:param error_statuses: Target error status of export task to fail the sensor",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.922546",
      "relevance_keywords": [
        "existence",
        "task",
        "export",
        "rdsexporttaskexistencesensor",
        "amazon",
        "s3",
        "cloud",
        "rds",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_rdsdbsensor_20260128_160330",
      "component_name": "RdsDbSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsDbSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "RdsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "db_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "db_type",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "target_statuses",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from RdsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.rds import RdsHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.amazon.aws.utils.rds import RdsDbType\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowNotFoundException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Waits for an RDS instance or cluster to enter one of a number of states.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:RdsDbSensor`\n\n:param db_type: Type of the DB - either \"instance\" or \"cluster\" (default: 'instance')\n:param db_identifier: The AWS identifier for the DB\n:param target_statuses: Target status of DB",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.922641",
      "relevance_keywords": [
        "db",
        "rdsdbsensor",
        "amazon",
        "s3",
        "cloud",
        "rds",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_redshiftclustersensor_20260128_160330",
      "component_name": "RedshiftClusterSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftClusterSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_identifier",
          "type": "str",
          "required": false
        },
        {
          "name": "target_status",
          "type": "str",
          "required": false,
          "default": "available"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.redshift_cluster import RedshiftHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.redshift_cluster import RedshiftClusterTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "template_fields",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for a Redshift cluster to reach a specific status.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:RedshiftClusterSensor`\n\n:param cluster_identifier: The identifier for the cluster being pinged.\n:param target_status: The cluster status desired.\n:param deferrable: Run operator in the deferrable mode.\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n     If this is ``None`` or empty then the default boto3 ",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.939482",
      "relevance_keywords": [
        "redshiftclustersensor",
        "cluster",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "redshift",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_s3keysunchangedsensor_20260128_160330",
      "component_name": "S3KeysUnchangedSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3KeysUnchangedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket_name",
          "type": "str",
          "required": false
        },
        {
          "name": "prefix",
          "type": "str",
          "required": false
        },
        {
          "name": "verify",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "inactivity_period",
          "type": "float",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "min_objects",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "previous_objects",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "allow_delete",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport fnmatch\nimport inspect\nimport os\nimport re\nfrom collections.abc import Callable, Sequence\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, Any, cast\nfrom airflow.providers.amazon.aws.hooks.s3 import S3Hook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor",
        "class_attributes": [
          "template_fields",
          "aws_hook_class"
        ],
        "methods": [
          "__init__",
          "is_keys_unchanged",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Return True if inactivity_period has passed with no increase in the number of objects matching prefix.\n\nNote, this sensor will not behave correctly in reschedule mode, as the state of the listed\nobjects in the S3 bucket will be lost between rescheduled invocations.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:S3KeysUnchangedSensor`\n\n:param bucket_name: Name of the S3 bucket\n:param prefix: The prefix being waited on. Relative p",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.944334",
      "relevance_keywords": [
        "keys",
        "s3keysunchangedsensor",
        "amazon",
        "s3",
        "cloud",
        "unchanged",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerbasesensor_20260128_160330",
      "component_name": "SageMakerBaseSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerBaseSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "resource_type",
          "type": "str",
          "required": false,
          "default": "job"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sagemaker import LogState, SageMakerHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "aws_hook_class",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "non_terminal_states",
          "failed_states",
          "get_sagemaker_response",
          "get_failed_reason_from_response",
          "state_from_response"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Contains general sensor behavior for SageMaker.\n\nSubclasses should implement get_sagemaker_response() and state_from_response() methods.\nSubclasses should also implement NON_TERMINAL_STATES and FAILED_STATE methods.",
      "success_score": 160,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.964367",
      "relevance_keywords": [
        "sage",
        "maker",
        "sagemakerbasesensor",
        "amazon",
        "base",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerendpointsensor_20260128_160330",
      "component_name": "SageMakerEndpointSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerEndpointSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "endpoint_name",
          "type": "Any",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sagemaker import LogState, SageMakerHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "template_ext"
        ],
        "methods": [
          "__init__",
          "non_terminal_states",
          "failed_states",
          "get_sagemaker_response",
          "get_failed_reason_from_response",
          "state_from_response"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the endpoint state until it reaches a terminal state; raise AirflowException with the failure reason.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:SageMakerEndpointSensor`\n\n:param endpoint_name: Name of the endpoint instance to watch.",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.964543",
      "relevance_keywords": [
        "sagemakerendpointsensor",
        "endpoint",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakertransformsensor_20260128_160330",
      "component_name": "SageMakerTransformSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerTransformSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sagemaker import LogState, SageMakerHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "template_ext"
        ],
        "methods": [
          "__init__",
          "non_terminal_states",
          "failed_states",
          "get_sagemaker_response",
          "get_failed_reason_from_response",
          "state_from_response"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the transform job until it reaches a terminal state; raise AirflowException with the failure reason.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:SageMakerTransformSensor`\n\n:param job_name: Name of the transform job to watch.",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.964784",
      "relevance_keywords": [
        "transform",
        "sage",
        "maker",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "sagemakertransformsensor",
        "sensor",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakertuningsensor_20260128_160330",
      "component_name": "SageMakerTuningSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerTuningSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sagemaker import LogState, SageMakerHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "template_ext"
        ],
        "methods": [
          "__init__",
          "non_terminal_states",
          "failed_states",
          "get_sagemaker_response",
          "get_failed_reason_from_response",
          "state_from_response"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the tuning state until it reaches a terminal state; raise AirflowException with the failure reason.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:SageMakerTuningSensor`\n\n:param job_name: Name of the tuning instance to watch.",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.965159",
      "relevance_keywords": [
        "sagemakertuningsensor",
        "sage",
        "maker",
        "tuning",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakertrainingsensor_20260128_160330",
      "component_name": "SageMakerTrainingSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerTrainingSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "Any",
          "required": false
        },
        {
          "name": "print_log",
          "type": "Any",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sagemaker import LogState, SageMakerHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "template_ext"
        ],
        "methods": [
          "__init__",
          "init_log_resource",
          "non_terminal_states",
          "failed_states",
          "get_sagemaker_response",
          "get_failed_reason_from_response",
          "state_from_response"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the training job until it reaches a terminal state; raise AirflowException with the failure reason.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:SageMakerTrainingSensor`\n\n:param job_name: Name of the training job to watch.\n:param print_log: Prints the cloudwatch log if True; Defaults to True.",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.965412",
      "relevance_keywords": [
        "training",
        "sage",
        "maker",
        "sagemakertrainingsensor",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerpipelinesensor_20260128_160330",
      "component_name": "SageMakerPipelineSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerPipelineSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "pipeline_exec_arn",
          "type": "str",
          "required": false
        },
        {
          "name": "verbose",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sagemaker import LogState, SageMakerHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "non_terminal_states",
          "failed_states",
          "get_sagemaker_response",
          "state_from_response"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the pipeline until it reaches a terminal state; raise AirflowException with the failure reason.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:SageMakerPipelineSensor`\n\n:param pipeline_exec_arn: ARN of the pipeline to watch.\n:param verbose: Whether to print steps details while waiting for completion.\n        Defaults to true, consider turning off for pipelines that have thousands of steps.",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.965705",
      "relevance_keywords": [
        "sagemakerpipelinesensor",
        "sage",
        "maker",
        "pipeline",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerautomlsensor_20260128_160330",
      "component_name": "SageMakerAutoMLSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerAutoMLSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sagemaker import LogState, SageMakerHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "non_terminal_states",
          "failed_states",
          "get_sagemaker_response",
          "state_from_response"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the auto ML job until it reaches a terminal state; raise AirflowException with the failure reason.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:SageMakerAutoMLSensor`\n\n:param job_name: unique name of the AutoML job to watch.",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.965916",
      "relevance_keywords": [
        "auto",
        "sagemakerautomlsensor",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerprocessingsensor_20260128_160330",
      "component_name": "SageMakerProcessingSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerProcessingSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "SageMakerBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from SageMakerBaseSensor",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sagemaker import LogState, SageMakerHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "template_ext"
        ],
        "methods": [
          "__init__",
          "non_terminal_states",
          "failed_states",
          "get_sagemaker_response",
          "state_from_response"
        ],
        "has_poke": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the processing job until it reaches a terminal state; raise AirflowException with the failure reason.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:SageMakerProcessingSensor`\n\n:param job_name: Name of the processing job to watch.",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.966115",
      "relevance_keywords": [
        "sagemakerprocessingsensor",
        "sage",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "processing",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakernotebooksensor_20260128_160330",
      "component_name": "SageMakerNotebookSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerNotebookSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "execution_id",
          "type": "str",
          "required": false
        },
        {
          "name": "execution_name",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.sagemaker_unified_studio import (\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator",
        "class_attributes": [],
        "methods": [
          "__init__",
          "hook",
          "poke",
          "execute"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Waits for a Sagemaker Workflows Notebook execution to reach any of the status below.\n\n'FAILED', 'STOPPED', 'COMPLETED'\n\n:param execution_id: The Sagemaker Workflows Notebook running execution identifier\n:param execution_name: The Sagemaker Workflows Notebook unique execution name",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.983874",
      "relevance_keywords": [
        "notebook",
        "sage",
        "sagemakernotebooksensor",
        "maker",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sqssensor_20260128_160330",
      "component_name": "SqsSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SqsSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "sqs_queue",
          "type": "Any",
          "required": false
        },
        {
          "name": "max_messages",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "num_batches",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "wait_time_seconds",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "visibility_timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "message_filtering",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "message_filtering_match_values",
          "type": "Any",
          "required": false,
          "default": null
        },
        {
          "name": "message_filtering_config",
          "type": "Any",
          "required": false,
          "default": null
        },
        {
          "name": "delete_message_on_reception",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Collection, Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.sqs import SqsHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.sqs import SqsSensorTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields",
        "class_attributes": [
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "poll_sqs",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Get messages from an Amazon SQS queue and then delete the messages from the queue.\n\nIf deletion of messages fails, an AirflowException is thrown. Otherwise, the messages\nare pushed through XCom with the key ``messages``.\n\nBy default,the sensor performs one and only one SQS call per poke, which limits the result to\na maximum of 10 messages. However, the total number of SQS API calls per poke can be controlled\nby num_batches param.\n\n.. seealso::\n    For more information on how to use this sensor, ",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:20.997837",
      "relevance_keywords": [
        "sqs",
        "sqssensor",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_ssmruncommandcompletedsensor_20260128_160330",
      "component_name": "SsmRunCommandCompletedSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SsmRunCommandCompletedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "command_id",
          "type": "Any",
          "required": false
        },
        {
          "name": "fail_on_nonzero_exit",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 75
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.ssm import SsmHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.triggers.ssm import SsmRunCommandTrigger\nfrom airflow.providers.amazon.aws.utils import validate_execute_complete_event\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import conf",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "FAILURE_MESSAGE",
          "aws_hook_class",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Poll the state of an AWS SSM Run Command until completion.\n\nWaits until all instance jobs reach a terminal state. Fails if any\ninstance job ends in a failed state.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the\n    guide:\n    :ref:`howto/sensor:SsmRunCommandCompletedSensor`\n\n:param command_id: The ID of the AWS SSM Run Command.\n:param fail_on_nonzero_exit: If True (default), the sensor will fail when the command\n    returns a non-zero exit code. If False, th",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:21.014524",
      "relevance_keywords": [
        "completed",
        "run",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "sensor",
        "ssm",
        "aws",
        "command",
        "ssmruncommandcompletedsensor"
      ]
    },
    {
      "id": "official_stepfunctionexecutionsensor_20260128_160330",
      "component_name": "StepFunctionExecutionSensor",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StepFunctionExecutionSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseSensor"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "execution_arn",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseSensor",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.step_function import StepFunctionHook\nfrom airflow.providers.amazon.aws.sensors.base_aws import AwsBaseSensor\nfrom airflow.providers.amazon.aws.utils.mixins import aws_template_fields\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "aws_hook_class",
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Poll the Step Function State Machine Execution until it reaches a terminal state; fails if the task fails.\n\nOn successful completion of the Execution the Sensor will do an XCom Push\nof the State Machine's output to `output`\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/sensor:StepFunctionExecutionSensor`\n\n:param execution_arn: execution_arn to check the state of\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    ",
      "success_score": 170,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:21.032389",
      "relevance_keywords": [
        "stepfunctionexecutionsensor",
        "execution",
        "step",
        "function",
        "amazon",
        "s3",
        "cloud",
        "sensor",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_appflowhook_20260128_160330",
      "component_name": "AppflowHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AppflowHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsGenericHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsGenericHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsGenericHook\nfrom airflow.providers.amazon.aws.utils.waiter_with_logging import wait",
        "class_attributes": [],
        "methods": [
          "__init__",
          "run_flow",
          "_log_execution_description",
          "update_flow_filter"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon AppFlow.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"appflow\") <Appflow.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n    - `Amazon Appflow API Reference <https://docs.aws.amazon.com/appflow/1.0/APIReference/Welcome.html>`__",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.060270",
      "relevance_keywords": [
        "appflowhook",
        "appflow",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_athenahook_20260128_160330",
      "component_name": "AthenaHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AthenaHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "log_query",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Collection\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "TERMINAL_STATES"
        ],
        "methods": [
          "__init__",
          "run_query",
          "get_query_info",
          "check_query_status",
          "get_state_change_reason",
          "get_query_results",
          "get_query_results_paginator",
          "poll_query_status",
          "get_output_location",
          "stop_query"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Athena.\n\nProvide thick wrapper around\n:external+boto3:py:class:`boto3.client(\"athena\") <Athena.Client>`.\n\n:param log_query: Whether to log athena query and other execution params\n    when it's executed. Defaults to *True*.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.081751",
      "relevance_keywords": [
        "athenahook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "athena",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_athenasqlhook_20260128_160330",
      "component_name": "AthenaSQLHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AthenaSQLHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook",
          "DbApiHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "athena_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook, DbApiHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nimport pyathena\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name",
          "supports_autocommit"
        ],
        "methods": [
          "__init__",
          "get_ui_field_behaviour",
          "conn_config",
          "conn",
          "_get_conn_params",
          "get_uri",
          "get_conn"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Athena.\n\nProvide wrapper around PyAthena library.\n\n:param athena_conn_id: :ref:`Amazon Athena Connection <howto/connection:athena>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\nYou can specify ``driver`` in ``extra`` of your connection in order to use\na different driver than the default ``rest``.\n\nAlso, aws_domain could be specified in ``extra`` of your connection.\n\nPyAthena and AWS Authentication param",
      "success_score": 175,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.101736",
      "relevance_keywords": [
        "athenasqlhook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "athena",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_awsgenerichook_20260128_160330",
      "component_name": "AwsGenericHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AwsGenericHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BaseHook",
          "Generic"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "verify",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "region_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "client_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "resource_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseHook, Generic",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport inspect\nimport json",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "_get_provider_version",
          "_find_operator_class_name",
          "_find_executor_class_name",
          "_get_caller",
          "_generate_dag_key",
          "_get_airflow_version",
          "_generate_user_agent_extra_field",
          "conn_config",
          "_resolve_service_name",
          "service_name",
          "service_config",
          "region_name",
          "config",
          "verify",
          "account_id",
          "get_session",
          "_get_config",
          "get_client_type",
          "get_resource_type",
          "conn",
          "async_conn",
          "_get_async_conn",
          "_client",
          "conn_client_meta",
          "conn_region_name",
          "conn_partition",
          "get_conn",
          "get_credentials",
          "expand_role",
          "retry",
          "get_ui_field_behaviour",
          "test_connection",
          "waiter_path",
          "get_waiter",
          "_apply_parameters_value",
          "list_waiters",
          "_list_official_waiters",
          "_list_custom_waiters"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Generic class for interact with AWS.\n\nThis class provide a thin wrapper around the boto3 Python library.\n\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is None or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configuration would be used (and must be\n    maintained on each worker node).\n:param verify: Whether or not to verify SSL certificates. See:\n    https",
      "success_score": 175,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.131723",
      "relevance_keywords": [
        "awsgenerichook",
        "hook",
        "amazon",
        "generic",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_awsbasehook_20260128_160330",
      "component_name": "AwsBaseHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AwsBaseHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsGenericHook"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsGenericHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport inspect\nimport json",
        "class_attributes": [],
        "methods": [],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Base class for interact with AWS.\n\nThis class provide a thin wrapper around the boto3 Python library.\n\n:param aws_conn_id: The Airflow connection used for AWS credentials.\n    If this is None or empty then the default boto3 behaviour is used. If\n    running Airflow in a distributed manner and aws_conn_id is None or\n    empty, then default boto3 configuration would be used (and must be\n    maintained on each worker node).\n:param verify: Whether or not to verify SSL certificates. See:\n    https://",
      "success_score": 175,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.132077",
      "relevance_keywords": [
        "hook",
        "base",
        "awsbasehook",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_batchclienthook_20260128_160330",
      "component_name": "BatchClientHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BatchClientHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "max_retries",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "status_retries",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport itertools",
        "class_attributes": [
          "MAX_RETRIES",
          "STATUS_RETRIES",
          "DEFAULT_DELAY_MIN",
          "DEFAULT_DELAY_MAX",
          "FAILURE_STATE",
          "SUCCESS_STATE",
          "RUNNING_STATE",
          "INTERMEDIATE_STATES",
          "COMPUTE_ENVIRONMENT_TERMINAL_STATUS",
          "COMPUTE_ENVIRONMENT_INTERMEDIATE_STATUS"
        ],
        "methods": [
          "__init__",
          "client",
          "terminate_job",
          "check_job_success",
          "wait_for_job",
          "poll_for_job_running",
          "poll_for_job_complete",
          "poll_job_status",
          "get_job_description",
          "parse_job_description",
          "get_job_awslogs_info",
          "get_job_all_awslogs_info",
          "add_jitter",
          "delay",
          "exponential_delay"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS Batch.\n\nProvide thick wrapper around :external+boto3:py:class:`boto3.client(\"batch\") <Batch.Client>`.\n\n:param max_retries: exponential back-off retries, 4200 = 48 hours;\n    polling is only used when waiters is None\n:param status_retries: number of HTTP retries to get job status, 10;\n    polling is only used when waiters is None\n\n.. note::\n    Several methods use a default random delay to check or poll for job status, i.e.\n    ``random.uniform(DEFAULT_DELAY_MIN, DEFAULT_DELAY_M",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.169715",
      "relevance_keywords": [
        "batch",
        "batchclienthook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "client",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_batchwaitershook_20260128_160330",
      "component_name": "BatchWaitersHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BatchWaitersHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BatchClientHook"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "waiter_config",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BatchClientHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport sys",
        "class_attributes": [],
        "methods": [
          "__init__",
          "default_config",
          "waiter_config",
          "waiter_model",
          "get_waiter",
          "list_waiters",
          "wait_for_job"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "A utility to manage waiters for AWS Batch services.\n\n.. code-block:: python\n\n    import random\n    from airflow.providers.amazon.aws.operators.batch_waiters import BatchWaiters\n\n    # to inspect default waiters\n    waiters = BatchWaiters()\n    config = waiters.default_config  # type: Dict\n    waiter_names = waiters.list_waiters()  # -> [\"JobComplete\", \"JobExists\", \"JobRunning\"]\n\n    # The default_config is a useful stepping stone to creating custom waiters, e.g.\n    custom_config = waiters.defau",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.209392",
      "relevance_keywords": [
        "batch",
        "waiters",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "batchwaitershook",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockhook_20260128_160330",
      "component_name": "BedrockHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [
          "client_type"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Bedrock.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"bedrock\") <Bedrock.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.223079",
      "relevance_keywords": [
        "bedrock",
        "bedrockhook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockruntimehook_20260128_160330",
      "component_name": "BedrockRuntimeHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockRuntimeHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [
          "client_type"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with the Amazon Bedrock Runtime.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"bedrock-runtime\") <BedrockRuntime.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.223163",
      "relevance_keywords": [
        "runtime",
        "bedrock",
        "amazon",
        "aws",
        "bedrockruntimehook",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockagenthook_20260128_160330",
      "component_name": "BedrockAgentHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockAgentHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [
          "client_type"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with the Amazon Agents for Bedrock API.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"bedrock-agent\") <AgentsforBedrock.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.223217",
      "relevance_keywords": [
        "bedrockagenthook",
        "bedrock",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "agent",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_bedrockagentruntimehook_20260128_160330",
      "component_name": "BedrockAgentRuntimeHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BedrockAgentRuntimeHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [
          "client_type"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with the Amazon Agents for Bedrock API.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"bedrock-agent-runtime\") <AgentsforBedrockRuntime.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.223267",
      "relevance_keywords": [
        "runtime",
        "bedrock",
        "bedrockagentruntimehook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "agent",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_chimewebhookhook_20260128_160330",
      "component_name": "ChimeWebhookHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ChimeWebhookHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "HttpHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "chime_conn_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from HttpHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom functools import cached_property\nfrom typing import Any\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.http.hooks.http import HttpHook",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "webhook_endpoint",
          "_get_webhook_endpoint",
          "_build_chime_payload",
          "send_message",
          "get_ui_field_behaviour"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Chime Webhooks to create notifications.\n\n.. warning:: This hook is only designed to work with web hooks and not chatbots.\n\n:param chime_conn_id: :ref:`Amazon Chime Connection ID <howto/connection:chime>`\n    with Endpoint as `https://hooks.chime.aws` and the webhook token\n    in the form of ``{webhook.id}?token{webhook.token}``",
      "success_score": 175,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.241070",
      "relevance_keywords": [
        "webhook",
        "chimewebhookhook",
        "chime",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_cloudformationhook_20260128_160330",
      "component_name": "CloudFormationHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFormationHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_stack_status",
          "create_stack",
          "delete_stack"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS CloudFormation.\n\nProvide thin wrapper around\n:external+boto3:py:class:`boto3.client(\"cloudformation\") <CloudFormation.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.253587",
      "relevance_keywords": [
        "formation",
        "amazon",
        "cloudformationhook",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_comprehendhook_20260128_160330",
      "component_name": "ComprehendHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComprehendHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [],
        "methods": [
          "__init__",
          "validate_document_classifier_training_status"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS Comprehend.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"comprehend\") <Comprehend.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.266252",
      "relevance_keywords": [
        "comprehend",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "comprehendhook",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_datasynchook_20260128_160330",
      "component_name": "DataSyncHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataSyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "wait_interval_seconds",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom urllib.parse import urlsplit\nfrom airflow.exceptions import AirflowBadRequest\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowTaskTimeout",
        "class_attributes": [
          "TASK_EXECUTION_INTERMEDIATE_STATES",
          "TASK_EXECUTION_FAILURE_STATES",
          "TASK_EXECUTION_SUCCESS_STATES"
        ],
        "methods": [
          "__init__",
          "create_location",
          "get_location_arns",
          "_refresh_locations",
          "create_task",
          "update_task",
          "delete_task",
          "_refresh_tasks",
          "get_task_arns_for_location_arns",
          "start_task_execution",
          "cancel_task_execution",
          "get_task_description",
          "describe_task_execution",
          "get_current_task_execution_arn",
          "wait_for_task_execution"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS DataSync.\n\nProvide thick wrapper around :external+boto3:py:class:`boto3.client(\"datasync\") <DataSync.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n:param wait_interval_seconds: Time to wait between two\n    consecutive calls to check TaskExecution status. Defaults to 30 seconds.\n:raises ValueError: If wait_interval_seconds is not between 0 and 15*60 seconds.\n\n.. seealso::\n    - :class:`airflow.provi",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.280551",
      "relevance_keywords": [
        "datasynchook",
        "data",
        "amazon",
        "sync",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_dmshook_20260128_160330",
      "component_name": "DmsHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DmsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any\nfrom botocore.exceptions import ClientError\nfrom dateutil import parser\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "describe_replication_tasks",
          "find_replication_tasks_by_arn",
          "get_task_status",
          "create_replication_task",
          "start_replication_task",
          "stop_replication_task",
          "delete_replication_task",
          "wait_for_task_status",
          "describe_replication_configs",
          "create_replication_config",
          "describe_replications",
          "delete_replication_config",
          "start_replication",
          "stop_replication",
          "get_provision_status"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS Database Migration Service (DMS).\n\nProvide thin wrapper around\n:external+boto3:py:class:`boto3.client(\"dms\") <DatabaseMigrationService.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.301258",
      "relevance_keywords": [
        "dms",
        "dmshook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_dynamodbhook_20260128_160330",
      "component_name": "DynamoDBHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DynamoDBHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "table_keys",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "table_name",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [],
        "methods": [
          "__init__",
          "client",
          "write_batch_data",
          "get_import_status"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon DynamoDB.\n\nProvide thick wrapper around\n:external+boto3:py:class:`boto3.resource(\"dynamodb\") <DynamoDB.ServiceResource>`.\n\n:param table_keys: partition key and sort key\n:param table_name: target DynamoDB table\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 175,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.318753",
      "relevance_keywords": [
        "dynamo",
        "dynamodbhook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_ec2hook_20260128_160330",
      "component_name": "EC2Hook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EC2Hook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_type",
          "type": "Any",
          "required": false,
          "default": "resource_type"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport functools\nimport time\nfrom collections.abc import Callable\nfrom typing import ParamSpec, TypeVar\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "API_TYPES"
        ],
        "methods": [
          "__init__",
          "get_instance",
          "stop_instances",
          "start_instances",
          "terminate_instances",
          "describe_instances",
          "get_instances",
          "get_instance_ids",
          "get_instance_state",
          "wait_for_state"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Elastic Compute Cloud (EC2).\n\nProvide thick wrapper around :external+boto3:py:class:`boto3.client(\"ec2\") <EC2.Client>`\nor :external+boto3:py:class:`boto3.resource(\"ec2\") <EC2.ServiceResource>`.\n\n:param api_type: If set to ``client_type`` then hook use ``boto3.client(\"ec2\")`` capabilities,\n    If set to ``resource_type`` then hook use ``boto3.resource(\"ec2\")`` capabilities.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying ",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.336595",
      "relevance_keywords": [
        "hook",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2",
        "ec2hook"
      ]
    },
    {
      "id": "official_ecrhook_20260128_160330",
      "component_name": "EcrHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcrHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport base64\nimport logging\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_temporary_credentials"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Elastic Container Registry (ECR).\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"ecr\") <ECR.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.364884",
      "relevance_keywords": [
        "ecrhook",
        "ecr",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_ecshook_20260128_160330",
      "component_name": "EcsHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EcsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsGenericHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsGenericHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import TYPE_CHECKING, Protocol, runtime_checkable\nfrom airflow.providers.amazon.aws.exceptions import EcsOperatorError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsGenericHook\nfrom airflow.providers.amazon.aws.utils import _StringCompareEnum",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_cluster_state",
          "get_task_definition_state",
          "get_task_state"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Elastic Container Service (ECS).\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"ecs\") <ECS.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n    - `Amazon Elastic Container Service         <https://docs.aws.amazon.com/AmazonECS/latest/APIReference/Welcome.html>`__",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.380889",
      "relevance_keywords": [
        "ecs",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "ecshook",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_ekshook_20260128_160330",
      "component_name": "EksHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EksHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport contextlib\nimport json\nimport os\nimport stat\nimport sys\nimport tempfile\nfrom collections.abc import Callable, Generator\nfrom contextlib import contextmanager\nfrom enum import Enum",
        "class_attributes": [
          "client_type"
        ],
        "methods": [
          "__init__",
          "create_cluster",
          "create_nodegroup",
          "create_fargate_profile",
          "delete_cluster",
          "delete_nodegroup",
          "delete_fargate_profile",
          "describe_cluster",
          "describe_nodegroup",
          "describe_fargate_profile",
          "get_cluster_state",
          "get_fargate_profile_state",
          "get_nodegroup_state",
          "list_clusters",
          "list_nodegroups",
          "list_fargate_profiles",
          "_list_all",
          "_secure_credential_context",
          "generate_config_file"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Elastic Kubernetes Service (EKS).\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"eks\") <EKS.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.398103",
      "relevance_keywords": [
        "ekshook",
        "eks",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_elasticachereplicationgrouphook_20260128_160330",
      "component_name": "ElastiCacheReplicationGroupHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ElastiCacheReplicationGroupHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "exponential_back_off_factor",
          "type": "float",
          "required": false,
          "default": 1
        },
        {
          "name": "initial_poke_interval",
          "type": "float",
          "required": false,
          "default": 60
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "TERMINAL_STATES"
        ],
        "methods": [
          "__init__",
          "create_replication_group",
          "delete_replication_group",
          "describe_replication_group",
          "get_replication_group_status",
          "is_replication_group_available",
          "wait_for_availability",
          "wait_for_deletion",
          "ensure_delete_replication_group"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon ElastiCache.\n\nProvide thick wrapper around :external+boto3:py:class:`boto3.client(\"elasticache\") <ElastiCache.Client>`.\n\n:param max_retries: Max retries for checking availability of and deleting replication group\n        If this is not supplied then this is defaulted to 10\n:param exponential_back_off_factor: Multiplication factor for deciding next sleep time\n        If this is not supplied then this is defaulted to 1\n:param initial_poke_interval: Initial sleep time in second",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.415702",
      "relevance_keywords": [
        "elasti",
        "elasticachereplicationgrouphook",
        "replication",
        "amazon",
        "cache",
        "group",
        "aws",
        "cloud",
        "s3",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_emrhook_20260128_160330",
      "component_name": "EmrHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "emr_conn_id",
          "type": "Union",
          "required": false,
          "default": "<default_conn_name>"
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport time\nimport warnings\nfrom typing import Any\nimport tenacity\nfrom botocore.exceptions import ClientError\nfrom tenacity import retry_if_exception, stop_after_attempt, wait_fixed\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.utils.waiter_with_logging import wait",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "get_cluster_id_by_name",
          "create_job_flow",
          "add_job_flow_steps",
          "test_connection",
          "get_ui_field_behaviour"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Elastic MapReduce Service (EMR).\n\nProvide thick wrapper around :external+boto3:py:class:`boto3.client(\"emr\") <EMR.Client>`.\n\n:param emr_conn_id: :ref:`Amazon Elastic MapReduce Connection <howto/connection:emr>`.\n    This attribute is only necessary when using\n    the :meth:`airflow.providers.amazon.aws.hooks.emr.EmrHook.create_job_flow`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    :clas",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.442053",
      "relevance_keywords": [
        "emrhook",
        "emr",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_emrserverlesshook_20260128_160330",
      "component_name": "EmrServerlessHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrServerlessHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport time\nimport warnings\nfrom typing import Any\nimport tenacity\nfrom botocore.exceptions import ClientError\nfrom tenacity import retry_if_exception, stop_after_attempt, wait_fixed\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.utils.waiter_with_logging import wait",
        "class_attributes": [
          "JOB_INTERMEDIATE_STATES",
          "JOB_FAILURE_STATES",
          "JOB_SUCCESS_STATES",
          "JOB_TERMINAL_STATES",
          "APPLICATION_INTERMEDIATE_STATES",
          "APPLICATION_FAILURE_STATES",
          "APPLICATION_SUCCESS_STATES"
        ],
        "methods": [
          "__init__",
          "cancel_running_jobs"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon EMR Serverless.\n\nProvide thin wrapper around :py:class:`boto3.client(\"emr-serverless\") <EMRServerless.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.442498",
      "relevance_keywords": [
        "emr",
        "serverless",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2",
        "emrserverlesshook"
      ]
    },
    {
      "id": "official_emrcontainerhook_20260128_160330",
      "component_name": "EmrContainerHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmrContainerHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "virtual_cluster_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport time\nimport warnings\nfrom typing import Any\nimport tenacity\nfrom botocore.exceptions import ClientError\nfrom tenacity import retry_if_exception, stop_after_attempt, wait_fixed\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.utils.waiter_with_logging import wait",
        "class_attributes": [
          "INTERMEDIATE_STATES",
          "FAILURE_STATES",
          "SUCCESS_STATES",
          "TERMINAL_STATES"
        ],
        "methods": [
          "__init__",
          "create_emr_on_eks_cluster",
          "submit_job",
          "get_job_failure_reason",
          "check_query_status",
          "poll_query_status",
          "stop_query"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon EMR Containers (Amazon EMR on EKS).\n\nProvide thick wrapper around :py:class:`boto3.client(\"emr-containers\") <EMRContainers.Client>`.\n\n:param virtual_cluster_id: Cluster ID of the EMR on EKS virtual cluster\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.442888",
      "relevance_keywords": [
        "emr",
        "amazon",
        "container",
        "aws",
        "s3",
        "cloud",
        "emrcontainerhook",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_eventbridgehook_20260128_160330",
      "component_name": "EventBridgeHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EventBridgeHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.utils.helpers import prune_dict",
        "class_attributes": [],
        "methods": [
          "__init__",
          "put_rule"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Amazon EventBridge Hook.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.475551",
      "relevance_keywords": [
        "event",
        "eventbridgehook",
        "bridge",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_firehosehook_20260128_160330",
      "component_name": "FirehoseHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "FirehoseHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "delivery_stream",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "put_records"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Kinesis Firehose.\n\nProvide thick wrapper around :external+boto3:py:class:`boto3.client(\"firehose\") <Firehose.Client>`.\n\n:param delivery_stream: Name of the delivery stream\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.503395",
      "relevance_keywords": [
        "firehose",
        "firehosehook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_glacierhook_20260128_160330",
      "component_name": "GlacierHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlacierHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "retrieve_inventory",
          "retrieve_inventory_results",
          "describe_job"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Glacier.\n\nThis is a thin wrapper around\n:external+boto3:py:class:`boto3.client(\"glacier\") <Glacier.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.515736",
      "relevance_keywords": [
        "glacierhook",
        "glacier",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_gluejobhook_20260128_160330",
      "component_name": "GlueJobHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueJobHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "s3_bucket",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "desc",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "concurrent_run_limit",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "script_location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry_limit",
          "type": "int",
          "required": false,
          "default": 0
        },
        {
          "name": "num_of_dpus",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "iam_role_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "iam_role_arn",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "create_job_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "update_config",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "job_poll_interval",
          "type": "Union",
          "required": false,
          "default": 6
        },
        {
          "name": "api_retry_args",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport time\nimport warnings\nfrom functools import cached_property\nfrom typing import Any\nfrom botocore.exceptions import ClientError\nfrom tenacity import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_should_retry_on_error",
          "create_glue_job_config",
          "describe_jobs",
          "list_jobs",
          "get_iam_execution_role",
          "initialize_job",
          "get_job_state",
          "logs_hook",
          "print_job_logs",
          "job_completion",
          "_handle_state",
          "has_job",
          "update_job",
          "get_or_create_glue_job",
          "create_or_update_glue_job"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS Glue.\n\nProvide thick wrapper around :external+boto3:py:class:`boto3.client(\"glue\") <Glue.Client>`.\n\n:param s3_bucket: S3 bucket where logs and local etl script will be uploaded\n:param job_name: unique job name per AWS account\n:param desc: job description\n:param concurrent_run_limit: The maximum number of concurrent runs allowed for a job\n:param script_location: path to etl script on s3\n:param retry_limit: Maximum number of times to retry this job if it fails\n:param num_of_dpus:",
      "success_score": 175,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.531984",
      "relevance_keywords": [
        "gluejobhook",
        "glue",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "job",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_gluedataqualityhook_20260128_160330",
      "component_name": "GlueDataQualityHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueDataQualityHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport time\nimport warnings\nfrom functools import cached_property\nfrom typing import Any\nfrom botocore.exceptions import ClientError\nfrom tenacity import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "has_data_quality_ruleset",
          "_log_results",
          "get_evaluation_run_results",
          "validate_evaluation_run_results",
          "log_recommendation_results"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS Glue Data Quality.\n\nProvide thick wrapper around :external+boto3:py:class:`boto3.client(\"glue\") <Glue.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 165,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.532194",
      "relevance_keywords": [
        "quality",
        "gluedataqualityhook",
        "data",
        "glue",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_gluecataloghook_20260128_160330",
      "component_name": "GlueCatalogHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueCatalogHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import Any\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_partitions",
          "check_for_partition",
          "get_table",
          "get_table_location",
          "get_partition",
          "create_partition"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS Glue Data Catalog.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"glue\") <Glue.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n    - `AWS Glue Data Catalog         <https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#data-catalog-intro>`__",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.548659",
      "relevance_keywords": [
        "glue",
        "amazon",
        "aws",
        "gluecataloghook",
        "s3",
        "cloud",
        "ec2",
        "hook",
        "catalog"
      ]
    },
    {
      "id": "official_gluecrawlerhook_20260128_160330",
      "component_name": "GlueCrawlerHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueCrawlerHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom functools import cached_property\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.sts import StsHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "glue_client",
          "has_crawler",
          "get_crawler",
          "update_crawler",
          "update_tags",
          "create_crawler",
          "start_crawler",
          "wait_for_crawler_completion"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interacts with AWS Glue Crawler.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"glue\") <Glue.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n    - `AWS Glue crawlers and classifiers         <https://docs.aws.amazon.com/glue/latest/dg/components-overview.html#crawling-intro>`__",
      "success_score": 165,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.567844",
      "relevance_keywords": [
        "gluecrawlerhook",
        "glue",
        "amazon",
        "aws",
        "s3",
        "ec2",
        "cloud",
        "hook",
        "crawler"
      ]
    },
    {
      "id": "official_gluedatabrewhook_20260128_160330",
      "component_name": "GlueDataBrewHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GlueDataBrewHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "job_completion",
          "get_job_state"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS DataBrew.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.582685",
      "relevance_keywords": [
        "gluedatabrewhook",
        "data",
        "glue",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "brew",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_kinesishook_20260128_160330",
      "component_name": "KinesisHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KinesisHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.hooks.firehose import FirehoseHook as _FirehoseHook",
        "class_attributes": [],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Kinesis.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"kinesis\") <Kinesis.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.595042",
      "relevance_keywords": [
        "kinesis",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "kinesishook",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_kinesisanalyticsv2hook_20260128_160330",
      "component_name": "KinesisAnalyticsV2Hook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KinesisAnalyticsV2Hook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [
          "APPLICATION_START_INTERMEDIATE_STATES",
          "APPLICATION_START_FAILURE_STATES",
          "APPLICATION_START_SUCCESS_STATES",
          "APPLICATION_STOP_INTERMEDIATE_STATES",
          "APPLICATION_STOP_FAILURE_STATES",
          "APPLICATION_STOP_SUCCESS_STATES"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Kinesis Analytics V2.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"kinesisanalyticsv2\") <KinesisAnalyticsV2.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.626317",
      "relevance_keywords": [
        "kinesis",
        "analytics",
        "kinesisanalyticsv2hook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_lambdahook_20260128_160330",
      "component_name": "LambdaHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "LambdaHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport base64\nfrom typing import Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.aws.utils import trim_none_values\nfrom airflow.providers.amazon.aws.utils.suppress import return_on_error",
        "class_attributes": [],
        "methods": [
          "__init__",
          "invoke_lambda",
          "create_lambda",
          "encode_log_result"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS Lambda.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"lambda\") <Lambda.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.643623",
      "relevance_keywords": [
        "lambdahook",
        "lambda",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_awslogshook_20260128_160330",
      "component_name": "AwsLogsHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AwsLogsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nfrom collections.abc import AsyncGenerator, Generator\nfrom typing import Any, TypedDict\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.utils.helpers import prune_dict",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_log_events"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon CloudWatch Logs.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"logs\") <CloudWatchLogs.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.659545",
      "relevance_keywords": [
        "awslogshook",
        "hook",
        "amazon",
        "s3",
        "ec2",
        "cloud",
        "aws",
        "logs"
      ]
    },
    {
      "id": "official_mwaahook_20260128_160330",
      "component_name": "MwaaHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "MwaaHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom typing import Literal\nimport requests\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "invoke_rest_api",
          "_invoke_rest_api_using_local_session_token",
          "_get_session_conn"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS Managed Workflows for Apache Airflow.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"mwaa\") <MWAA.Client>`\n\nIf your IAM policy doesn't have `airflow:InvokeRestApi` permission, the hook will use a fallback method\nthat uses the AWS credential to generate a local web login token for the Airflow Web UI and then directly\nmake requests to the Airflow API. This fallback method can be set as the default (and only) method used by\nsetting `generate_local_token` to T",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.677779",
      "relevance_keywords": [
        "mwaahook",
        "mwaa",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_neptunehook_20260128_160330",
      "component_name": "NeptuneHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "NeptuneHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [
          "AVAILABLE_STATES",
          "STOPPED_STATES",
          "ERROR_STATES"
        ],
        "methods": [
          "__init__",
          "wait_for_cluster_availability",
          "wait_for_cluster_stopped",
          "get_cluster_status",
          "get_db_instance_status",
          "wait_for_cluster_instance_availability"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Neptune.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`~airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.696891",
      "relevance_keywords": [
        "neptunehook",
        "neptune",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_opensearchserverlesshook_20260128_160330",
      "component_name": "OpenSearchServerlessHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "OpenSearchServerlessHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [
          "client_type"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with the Amazon OpenSearch Serverless API.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"opensearchserverless\") <OpenSearchServiceServerless.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.711806",
      "relevance_keywords": [
        "open",
        "serverless",
        "search",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "opensearchserverlesshook",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_quicksighthook_20260128_160330",
      "component_name": "QuickSightHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "QuickSightHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom botocore.exceptions import ClientError\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "NON_TERMINAL_STATES",
          "FAILED_STATES"
        ],
        "methods": [
          "__init__",
          "create_ingestion",
          "get_status",
          "get_error_info",
          "wait_for_state"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon QuickSight.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"quicksight\") <QuickSight.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.729345",
      "relevance_keywords": [
        "quick",
        "amazon",
        "quicksighthook",
        "sight",
        "aws",
        "cloud",
        "s3",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_rdshook_20260128_160330",
      "component_name": "RdsHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RdsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsGenericHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsGenericHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsGenericHook\nfrom airflow.providers.amazon.aws.utils.waiter_with_logging import wait\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowNotFoundException",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_db_snapshot_state",
          "wait_for_db_snapshot_state",
          "get_db_cluster_snapshot_state",
          "wait_for_db_cluster_snapshot_state",
          "get_export_task_state",
          "wait_for_export_task_state",
          "get_event_subscription_state",
          "wait_for_event_subscription_state",
          "get_db_instance_state",
          "wait_for_db_instance_state",
          "get_db_cluster_state",
          "wait_for_db_cluster_state",
          "_wait_for_state"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Relational Database Service (RDS).\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"rds\") <RDS.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n    - `Amazon RDS and Aurora Documentation         <https://docs.aws.amazon.com/rds/index.html>`__",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.748025",
      "relevance_keywords": [
        "rdshook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "rds",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_redshifthook_20260128_160330",
      "component_name": "RedshiftHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [
          "cluster_identifier"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "create_cluster",
          "cluster_status",
          "delete_cluster",
          "describe_cluster_snapshots",
          "restore_from_cluster_snapshot",
          "create_cluster_snapshot",
          "get_cluster_snapshot_status"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Redshift.\n\nThis is a thin wrapper around\n:external+boto3:py:class:`boto3.client(\"redshift\") <Redshift.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 163,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.767024",
      "relevance_keywords": [
        "amazon",
        "aws",
        "s3",
        "cloud",
        "redshifthook",
        "redshift",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_redshiftdatahook_20260128_160330",
      "component_name": "RedshiftDataHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftDataHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsGenericHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsGenericHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom pprint import pformat\nfrom typing import TYPE_CHECKING, Any\nfrom uuid import UUID\nfrom pendulum import duration\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsGenericHook\nfrom airflow.providers.amazon.aws.utils import trim_none_values",
        "class_attributes": [],
        "methods": [
          "__init__",
          "execute_query",
          "wait_for_results",
          "check_query_is_finished",
          "parse_statement_response",
          "get_table_primary_key"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Redshift Data API.\n\nProvide thin wrapper around\n:external+boto3:py:class:`boto3.client(\"redshift-data\") <RedshiftDataAPIService.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n    - `Amazon Redshift Data API         <https://docs.aws.amazon.com/redshift-data/latest/APIReference/Welcome.html>`__",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.786290",
      "relevance_keywords": [
        "data",
        "redshiftdatahook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "redshift",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_redshiftsqlhook_20260128_160330",
      "component_name": "RedshiftSQLHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedshiftSQLHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "DbApiHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        }
      ],
      "success_factors": [
        "Inherits from DbApiHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nimport redshift_connector\nimport tenacity\nfrom redshift_connector import Connection as RedshiftConnection, InterfaceError, OperationalError",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name",
          "supports_autocommit"
        ],
        "methods": [
          "__init__",
          "get_ui_field_behaviour",
          "conn",
          "_get_conn_params",
          "get_iam_token",
          "get_uri",
          "get_sqlalchemy_engine",
          "get_table_primary_key",
          "get_conn",
          "get_openlineage_database_info",
          "_get_openlineage_redshift_authority_part",
          "_get_identifier_from_hostname",
          "get_openlineage_database_dialect",
          "get_openlineage_default_schema"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Execute statements against Amazon Redshift.\n\nThis hook requires the redshift_conn_id connection.\n\nNote: For AWS IAM authentication, use iam in the extra connection parameters\nand set it to true. Leave the password field empty. This will use the\n\"aws_default\" connection to get the temporary token unless you override\nwith aws_conn_id when initializing the hook.\nThe cluster-identifier is extracted from the beginning of\nthe host field, so is optional. It can however be overridden in the extra field.",
      "success_score": 175,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.808442",
      "relevance_keywords": [
        "redshiftsqlhook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "redshift",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_s3hook_20260128_160330",
      "component_name": "S3Hook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "S3Hook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "transfer_config_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "extra_args",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AwsBaseHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport fnmatch\nimport gzip as gz\nimport inspect\nimport logging\nimport os\nimport re\nimport shutil\nimport time",
        "class_attributes": [],
        "methods": [
          "__init__",
          "resource",
          "extra_args",
          "parse_s3_url",
          "get_s3_bucket_key",
          "check_for_bucket",
          "get_bucket",
          "create_bucket",
          "check_for_prefix",
          "list_prefixes",
          "_list_key_object_filter",
          "list_keys",
          "get_file_metadata",
          "iter_file_metadata",
          "head_object",
          "check_for_key",
          "get_key",
          "read_key",
          "select_key",
          "check_for_wildcard_key",
          "get_wildcard_key",
          "load_file",
          "load_string",
          "load_bytes",
          "load_file_obj",
          "_upload_file_obj",
          "copy_object",
          "delete_bucket",
          "delete_objects",
          "download_file",
          "generate_presigned_url",
          "get_bucket_tagging",
          "put_bucket_tagging",
          "delete_bucket_tagging",
          "_sync_to_local_dir_delete_stale_local_files",
          "_sync_to_local_dir_if_changed",
          "sync_to_local_dir"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Simple Storage Service (S3).\n\nProvide thick wrapper around :external+boto3:py:class:`boto3.client(\"s3\") <S3.Client>`\nand :external+boto3:py:class:`boto3.resource(\"s3\") <S3.ServiceResource>`.\n\n:param transfer_config_args: Configuration object for managed S3 transfers.\n:param extra_args: Extra arguments that may be passed to the download/upload operations.\n\n.. seealso::\n    https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#s3-transfers\n\n ",
      "success_score": 175,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.843560",
      "relevance_keywords": [
        "s3hook",
        "hook",
        "amazon",
        "s3",
        "cloud",
        "aws",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakerhook_20260128_160330",
      "component_name": "SageMakerHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport re\nimport tarfile\nimport tempfile\nimport time\nfrom collections import Counter, namedtuple\nfrom collections.abc import AsyncGenerator, Callable, Generator\nfrom datetime import datetime\nfrom functools import partial",
        "class_attributes": [
          "non_terminal_states",
          "endpoint_non_terminal_states",
          "pipeline_non_terminal_states",
          "processing_job_non_terminal_states",
          "failed_states",
          "processing_job_failed_states",
          "training_failed_states"
        ],
        "methods": [
          "__init__",
          "tar_and_s3_upload",
          "configure_s3_resources",
          "check_s3_url",
          "check_training_config",
          "check_tuning_config",
          "multi_stream_iter",
          "create_training_job",
          "create_tuning_job",
          "create_transform_job",
          "create_processing_job",
          "create_model",
          "create_endpoint_config",
          "create_endpoint",
          "update_endpoint",
          "describe_training_job",
          "describe_training_job_with_log",
          "describe_tuning_job",
          "describe_model",
          "describe_transform_job",
          "describe_processing_job",
          "describe_endpoint_config",
          "describe_endpoint",
          "check_status",
          "check_training_status_with_log",
          "list_training_jobs",
          "list_transform_jobs",
          "list_processing_jobs",
          "_preprocess_list_request_args",
          "_list_request",
          "_name_matches_pattern",
          "count_processing_jobs_by_name",
          "delete_model",
          "describe_pipeline_exec",
          "start_pipeline",
          "stop_pipeline",
          "create_model_package_group",
          "_describe_auto_ml_job",
          "create_auto_ml_job",
          "count_billable_seconds"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon SageMaker.\n\nProvide thick wrapper around\n:external+boto3:py:class:`boto3.client(\"sagemaker\") <SageMaker.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.898813",
      "relevance_keywords": [
        "sagemakerhook",
        "sage",
        "maker",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_sagemakernotebookhook_20260128_160330",
      "component_name": "SageMakerNotebookHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SageMakerNotebookHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "execution_name",
          "type": "str",
          "required": true
        },
        {
          "name": "input_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "output_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "compute",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "termination_condition",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "waiter_delay",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "waiter_max_attempts",
          "type": "int",
          "required": false,
          "default": 1440
        }
      ],
      "success_factors": [
        "Inherits from BaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom sagemaker_studio import ClientConfig\nfrom sagemaker_studio.sagemaker_studio_api import SageMakerStudioAPI\nfrom airflow.providers.amazon.aws.utils.sagemaker_unified_studio import is_local_runner\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_get_sagemaker_studio_config",
          "_format_start_execution_input_config",
          "_format_start_execution_output_config",
          "start_notebook_execution",
          "wait_for_execution_completion",
          "_set_xcom_files",
          "_set_xcom_s3_path",
          "_handle_state"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Sagemaker Unified Studio Workflows.\n\nThis hook provides a wrapper around the Sagemaker Workflows Notebook Execution API.\n\nExamples:\n .. code-block:: python\n\n    from airflow.providers.amazon.aws.hooks.sagemaker_unified_studio import SageMakerNotebookHook\n\n    notebook_hook = SageMakerNotebookHook(\n        input_config={\"input_path\": \"path/to/notebook.ipynb\", \"input_params\": {\"param1\": \"value1\"}},\n        output_config={\"output_uri\": \"folder/output/location/prefix\", \"output_formats\"",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.921357",
      "relevance_keywords": [
        "notebook",
        "sage",
        "maker",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "sagemakernotebookhook",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_secretsmanagerhook_20260128_160330",
      "component_name": "SecretsManagerHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SecretsManagerHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport base64\nimport json\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_secret",
          "get_secret_as_dict"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon SecretsManager Service.\n\nProvide thin wrapper around\n:external+boto3:py:class:`boto3.client(\"secretsmanager\") <SecretsManager.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.944013",
      "relevance_keywords": [
        "manager",
        "secrets",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "secretsmanagerhook",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_seshook_20260128_160330",
      "component_name": "SesHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SesHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable\nfrom typing import Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.utils.email import build_mime_message",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_build_headers",
          "send_email"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Simple Email Service.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"ses\") <SES.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.959748",
      "relevance_keywords": [
        "seshook",
        "ses",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_snshook_20260128_160330",
      "component_name": "SnsHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SnsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.utils.helpers import prune_dict",
        "class_attributes": [],
        "methods": [
          "__init__",
          "publish_to_target"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Simple Notification Service.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"sns\") <SNS.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.975077",
      "relevance_keywords": [
        "sns",
        "snshook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_sqshook_20260128_160330",
      "component_name": "SqsHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SqsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.utils.helpers import prune_dict",
        "class_attributes": [],
        "methods": [
          "__init__",
          "create_queue",
          "_build_msg_params",
          "send_message"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Simple Queue Service.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"sqs\") <SQS.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:21.988314",
      "relevance_keywords": [
        "amazon",
        "aws",
        "s3",
        "cloud",
        "sqshook",
        "sqs",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_ssmhook_20260128_160330",
      "component_name": "SsmHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SsmHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.amazon.version_compat import NOTSET, ArgNotSet, is_arg_set",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_parameter_value",
          "get_command_invocation",
          "list_command_invocations",
          "is_aws_level_failure"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Systems Manager (SSM).\n\nProvide thin wrapper around\n:external+boto3:py:class:`boto3.client(\"ssm\") <SSM.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:22.002477",
      "relevance_keywords": [
        "ssmhook",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "ssm",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_stepfunctionhook_20260128_160330",
      "component_name": "StepFunctionHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StepFunctionHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowFailException",
        "class_attributes": [],
        "methods": [
          "__init__",
          "start_execution",
          "describe_execution"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with an AWS Step Functions State Machine.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"stepfunctions\") <SFN.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:22.026666",
      "relevance_keywords": [
        "step",
        "stepfunctionhook",
        "function",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_stshook_20260128_160330",
      "component_name": "StsHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_account_number"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with AWS Security Token Service (STS).\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"sts\") <STS.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:22.043187",
      "relevance_keywords": [
        "stshook",
        "sts",
        "amazon",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_verifiedpermissionshook_20260128_160330",
      "component_name": "VerifiedPermissionsHook",
      "category": "cloud",
      "subcategory": "aws",
      "provider": "amazon",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "VerifiedPermissionsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "amazon",
        "base_classes": [
          "AwsGenericHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from AwsGenericHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsGenericHook",
        "class_attributes": [],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Amazon Verified Permissions.\n\nProvide thin wrapper around :external+boto3:py:class:`boto3.client(\"verifiedpermissions\")\n<VerifiedPermissions.Client>`.\n\nAdditional arguments (such as ``aws_conn_id``) may be specified and\nare passed down to the underlying AwsBaseHook.\n\n.. seealso::\n    - :class:`airflow.providers.amazon.aws.hooks.base_aws.AwsBaseHook`\n    - `Amazon Appflow API Reference <https://docs.aws.amazon.com/verifiedpermissions/latest/apireference/Welcome.html>`__",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:22.069619",
      "relevance_keywords": [
        "permissions",
        "verified",
        "amazon",
        "verifiedpermissionshook",
        "aws",
        "s3",
        "cloud",
        "hook",
        "ec2"
      ]
    },
    {
      "id": "official_googleadslistaccountsoperator_20260128_160330",
      "component_name": "GoogleAdsListAccountsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleAdsListAccountsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "bucket",
          "object_name",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "object_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "google_ads_conn_id",
          "type": "str",
          "required": false,
          "default": "google_ads_default"
        },
        {
          "name": "gzip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_version",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport csv\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.ads.hooks.ads import GoogleAdsHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Saves list of customers on GCS in form of a csv file.\n\nThe resulting list of customers is based on your OAuth credentials. The request returns a list\nof all accounts that you are able to act upon directly given your current credentials. This will\nnot necessarily include all accounts within the account hierarchy; rather, it will only include\naccounts where your authenticated user has been added with admin or other rights in the account.\n\n.. seealso::\n    https://developers.google.com/google-ads/a",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.209750",
      "relevance_keywords": [
        "gcs",
        "googleadslistaccountsoperator",
        "operator",
        "ads",
        "google",
        "gcp",
        "bigquery",
        "list",
        "cloud",
        "accounts"
      ]
    },
    {
      "id": "official_alloydbbaseoperator_20260128_160330",
      "component_name": "AlloyDBBaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "location",
          "gcp_conn_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "hook"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Base class for all AlloyDB operators.\n\n:param project_id: Required. The ID of the Google Cloud project where the service is used.\n:param location: Required. The ID of the Google Cloud region where the service is used.\n:param gcp_conn_id: Optional. The connection ID to use to connect to Google Cloud.\n:param retry: Optional. A retry object used to retry requests. If `None` is specified, requests will not\n    be retried.\n:param timeout: Optional. The amount of time, in seconds, to wait for the requ",
      "success_score": 184,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.243162",
      "relevance_keywords": [
        "alloy",
        "gcs",
        "alloydbbaseoperator",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "base",
        "cloud"
      ]
    },
    {
      "id": "official_alloydbwritebaseoperator_20260128_160330",
      "component_name": "AlloyDBWriteBaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBWriteBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "validate_request",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBBaseOperator",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "get_operation_result"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Base class for writing AlloyDB operators.\n\nThese operators perform create, update or delete operations. with the objects (not inside of database).\n\n:param request_id: Optional. An optional request ID to identify requests. Specify a unique request ID\n    so that if you must retry your request, the server ignores the request if it has already been\n    completed. The server guarantees that for at least 60 minutes since the first request.\n    For example, consider a situation where you make an initi",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.243478",
      "relevance_keywords": [
        "alloy",
        "gcs",
        "operator",
        "google",
        "gcp",
        "alloydbwritebaseoperator",
        "bigquery",
        "base",
        "write",
        "cloud"
      ]
    },
    {
      "id": "official_alloydbcreateclusteroperator_20260128_160330",
      "component_name": "AlloyDBCreateClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBCreateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "is_secondary",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_get_cluster",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create an Alloy DB cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBCreateClusterOperator`\n\n:param cluster_id: Required. ID of the cluster to create.\n:param cluster_configuration: Required. Cluster to create. For more details please see API documentation:\n        https://cloud.google.com/python/docs/reference/alloydb/latest/google.cloud.alloydb_v1.types.Cluster\n:param is_secondary: Required. Specifies if the Cl",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.243831",
      "relevance_keywords": [
        "alloy",
        "gcs",
        "operator",
        "google",
        "cluster",
        "create",
        "gcp",
        "bigquery",
        "alloydbcreateclusteroperator",
        "cloud"
      ]
    },
    {
      "id": "official_alloydbupdateclusteroperator_20260128_160330",
      "component_name": "AlloyDBUpdateClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBUpdateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "allow_missing",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Update an Alloy DB cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBUpdateClusterOperator`\n\n:param cluster_id: Required. ID of the cluster to update.\n:param cluster_configuration: Required. Cluster to update. For more details please see API documentation:\n        https://cloud.google.com/python/docs/reference/alloydb/latest/google.cloud.alloydb_v1.types.Cluster\n:param update_mask: Optional. Field mask is used t",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.244218",
      "relevance_keywords": [
        "alloy",
        "gcs",
        "operator",
        "google",
        "update",
        "cluster",
        "gcp",
        "bigquery",
        "alloydbupdateclusteroperator",
        "cloud"
      ]
    },
    {
      "id": "official_alloydbdeleteclusteroperator_20260128_160330",
      "component_name": "AlloyDBDeleteClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBDeleteClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "etag",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "force",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete an Alloy DB cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBDeleteClusterOperator`\n\n:param cluster_id: Required. ID of the cluster to delete.\n:param request_id: Optional. An optional request ID to identify requests. Specify a unique request ID\n    so that if you must retry your request, the server ignores the request if it has already been\n    completed. The server guarantees that for at least 60 minute",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.244543",
      "relevance_keywords": [
        "alloy",
        "gcs",
        "alloydbdeleteclusteroperator",
        "operator",
        "google",
        "cluster",
        "gcp",
        "bigquery",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_alloydbcreateinstanceoperator_20260128_160330",
      "component_name": "AlloyDBCreateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBCreateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "instance_id",
          "type": "str",
          "required": true
        },
        {
          "name": "instance_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "is_secondary",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_get_instance",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create an Instance in an Alloy DB cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBCreateInstanceOperator`\n\n:param cluster_id: Required. ID of the cluster for creating an instance in.\n:param instance_id: Required. ID of the instance to create.\n:param instance_configuration: Required. Instance to create. For more details please see API documentation:\n        https://cloud.google.com/python/docs/reference/alloydb",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.244879",
      "relevance_keywords": [
        "alloy",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "instance",
        "bigquery",
        "cloud",
        "alloydbcreateinstanceoperator"
      ]
    },
    {
      "id": "official_alloydbupdateinstanceoperator_20260128_160330",
      "component_name": "AlloyDBUpdateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBUpdateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "instance_id",
          "type": "str",
          "required": true
        },
        {
          "name": "instance_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "allow_missing",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Update an Alloy DB instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBUpdateInstanceOperator`\n\n:param cluster_id: Required. ID of the cluster.\n:param instance_id: Required. ID of the instance to update.\n:param instance_configuration: Required. Instance to update. For more details please see API documentation:\n        https://cloud.google.com/python/docs/reference/alloydb/latest/google.cloud.alloydb_v1.types.Inst",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.245215",
      "relevance_keywords": [
        "alloy",
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "instance",
        "bigquery",
        "cloud",
        "alloydbupdateinstanceoperator"
      ]
    },
    {
      "id": "official_alloydbdeleteinstanceoperator_20260128_160330",
      "component_name": "AlloyDBDeleteInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBDeleteInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "etag",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete an Alloy DB instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBDeleteInstanceOperator`\n\n:param instance_id: Required. ID of the instance to delete.\n:param cluster_id: Required. ID of the cluster.\n:param request_id: Optional. An optional request ID to identify requests. Specify a unique request ID\n    so that if you must retry your request, the server ignores the request if it has already been\n    complete",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.245500",
      "relevance_keywords": [
        "alloy",
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "alloydbdeleteinstanceoperator",
        "bigquery",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_bigquerycheckoperator_20260128_160330",
      "component_name": "BigQueryCheckOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryCheckOperator",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_BigQueryDbHookMixin",
          "SQLCheckOperator",
          "_BigQueryOperatorsEncryptionConfigurationMixin"
        ],
        "template_fields": [
          "sql",
          "gcp_conn_id",
          "impersonation_chain",
          "labels",
          "query_params"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "sql",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "use_legacy_sql",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "encryption_configuration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "float",
          "required": false,
          "default": 4.0
        },
        {
          "name": "query_params",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from _BigQueryDbHookMixin, SQLCheckOperator, _BigQueryOperatorsEncryptionConfigurationMixin",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "ui_color",
          "conn_id_field"
        ],
        "methods": [
          "__init__",
          "_submit_job",
          "execute",
          "_handle_job_error",
          "_validate_records",
          "execute_complete"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Performs checks against BigQuery.\n\nThis operator expects a SQL query that returns a single row. Each value on\nthat row is evaluated using a Python ``bool`` cast. If any of the values\nis falsy, the check errors out.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryCheckOperator`\n\nNote that Python bool casting evals the following as *False*:\n\n* ``False``\n* ``0``\n* Empty string (``\"\"``)\n* Empty list (``[]``)\n* Empty dicti",
      "success_score": 205,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:23.282085",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "big",
        "check",
        "bigquery",
        "cloud",
        "bigquerycheckoperator",
        "hook",
        "query"
      ]
    },
    {
      "id": "official_bigqueryvaluecheckoperator_20260128_160330",
      "component_name": "BigQueryValueCheckOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryValueCheckOperator",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_BigQueryDbHookMixin",
          "SQLValueCheckOperator",
          "_BigQueryOperatorsEncryptionConfigurationMixin"
        ],
        "template_fields": [
          "sql",
          "gcp_conn_id",
          "pass_value",
          "impersonation_chain",
          "labels"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "sql",
          "type": "str",
          "required": false
        },
        {
          "name": "pass_value",
          "type": "Any",
          "required": false
        },
        {
          "name": "tolerance",
          "type": "Any",
          "required": false,
          "default": null
        },
        {
          "name": "encryption_configuration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "use_legacy_sql",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "float",
          "required": false,
          "default": 4.0
        }
      ],
      "success_factors": [
        "Inherits from _BigQueryDbHookMixin, SQLValueCheckOperator, _BigQueryOperatorsEncryptionConfigurationMixin",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "ui_color",
          "conn_id_field"
        ],
        "methods": [
          "__init__",
          "_submit_job",
          "execute",
          "_handle_job_error",
          "execute_complete"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Perform a simple value check using sql code.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryValueCheckOperator`\n\n:param sql: SQL to execute.\n:param use_legacy_sql: Whether to use legacy SQL (true)\n    or standard SQL (false).\n:param encryption_configuration: (Optional) Custom encryption configuration (e.g., Cloud KMS keys).\n\n    .. code-block:: python\n\n        encryption_configuration = {\n            \"kmsKeyName\": \"p",
      "success_score": 205,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:23.282492",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "value",
        "gcp",
        "big",
        "check",
        "bigquery",
        "bigqueryvaluecheckoperator",
        "cloud",
        "hook",
        "query"
      ]
    },
    {
      "id": "official_bigqueryintervalcheckoperator_20260128_160330",
      "component_name": "BigQueryIntervalCheckOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryIntervalCheckOperator",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_BigQueryDbHookMixin",
          "SQLIntervalCheckOperator",
          "_BigQueryOperatorsEncryptionConfigurationMixin"
        ],
        "template_fields": [
          "table",
          "gcp_conn_id",
          "sql1",
          "sql2",
          "impersonation_chain",
          "labels"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "table",
          "type": "str",
          "required": false
        },
        {
          "name": "metrics_thresholds",
          "type": "dict",
          "required": false
        },
        {
          "name": "date_filter_column",
          "type": "str",
          "required": false,
          "default": "ds"
        },
        {
          "name": "days_back",
          "type": "SupportsAbs[...]",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "use_legacy_sql",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "encryption_configuration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "float",
          "required": false,
          "default": 4.0
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        }
      ],
      "success_factors": [
        "Inherits from _BigQueryDbHookMixin, SQLIntervalCheckOperator, _BigQueryOperatorsEncryptionConfigurationMixin",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "conn_id_field"
        ],
        "methods": [
          "__init__",
          "_submit_job",
          "execute",
          "execute_complete"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Check that the values of metrics given as SQL expressions are within a tolerance of the older ones.\n\nThis method constructs a query like so ::\n\n    SELECT {metrics_threshold_dict_key} FROM {table}\n    WHERE {date_filter_column}=<date>\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryIntervalCheckOperator`\n\n:param table: the table name\n:param days_back: number of days between ds and the ds we want to check\n    against. ",
      "success_score": 205,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:23.282867",
      "relevance_keywords": [
        "bigqueryintervalcheckoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "big",
        "check",
        "bigquery",
        "interval",
        "cloud",
        "hook",
        "query"
      ]
    },
    {
      "id": "official_bigquerycolumncheckoperator_20260128_160330",
      "component_name": "BigQueryColumnCheckOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryColumnCheckOperator",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_BigQueryDbHookMixin",
          "SQLColumnCheckOperator",
          "_BigQueryOperatorsEncryptionConfigurationMixin"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "table",
          "type": "str",
          "required": false
        },
        {
          "name": "column_mapping",
          "type": "dict",
          "required": false
        },
        {
          "name": "partition_clause",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "database",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "accept_none",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "encryption_configuration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "use_legacy_sql",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from _BigQueryDbHookMixin, SQLColumnCheckOperator, _BigQueryOperatorsEncryptionConfigurationMixin",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "conn_id_field"
        ],
        "methods": [
          "__init__",
          "_submit_job",
          "execute"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Subclasses the SQLColumnCheckOperator in order to provide a job id for OpenLineage to parse.\n\nSee base class docstring for usage.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryColumnCheckOperator`\n\n:param table: the table name\n:param column_mapping: a dictionary relating columns to their checks\n:param partition_clause: a string SQL statement added to a WHERE clause\n    to partition data\n:param gcp_conn_id: (Optional",
      "success_score": 190,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:23.283209",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquerycolumncheckoperator",
        "big",
        "check",
        "bigquery",
        "column",
        "cloud",
        "hook",
        "query"
      ]
    },
    {
      "id": "official_bigquerytablecheckoperator_20260128_160330",
      "component_name": "BigQueryTableCheckOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryTableCheckOperator",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_BigQueryDbHookMixin",
          "SQLTableCheckOperator",
          "_BigQueryOperatorsEncryptionConfigurationMixin"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "table",
          "type": "str",
          "required": false
        },
        {
          "name": "checks",
          "type": "dict",
          "required": false
        },
        {
          "name": "partition_clause",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "use_legacy_sql",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "encryption_configuration",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from _BigQueryDbHookMixin, SQLTableCheckOperator, _BigQueryOperatorsEncryptionConfigurationMixin",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "conn_id_field"
        ],
        "methods": [
          "__init__",
          "_submit_job",
          "execute"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Subclasses the SQLTableCheckOperator in order to provide a job id for OpenLineage to parse.\n\nSee base class for usage.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryTableCheckOperator`\n\n:param table: the table name\n:param checks: a dictionary of check names and boolean SQL statements\n:param partition_clause: a string SQL statement added to a WHERE clause\n    to partition data\n:param gcp_conn_id: (Optional) The conne",
      "success_score": 190,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:23.283535",
      "relevance_keywords": [
        "gcs",
        "bigquerytablecheckoperator",
        "google",
        "operator",
        "gcp",
        "big",
        "check",
        "bigquery",
        "table",
        "cloud",
        "hook",
        "query"
      ]
    },
    {
      "id": "official_bigquerygetdataoperator_20260128_160330",
      "component_name": "BigQueryGetDataOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryGetDataOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator",
          "_BigQueryOperatorsEncryptionConfigurationMixin"
        ],
        "template_fields": [
          "dataset_id",
          "table_id",
          "table_project_id",
          "job_id",
          "job_project_id",
          "project_id",
          "max_results",
          "selected_fields",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataset_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "table_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "table_project_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_project_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "max_results",
          "type": "int",
          "required": false,
          "default": 100
        },
        {
          "name": "selected_fields",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "encryption_configuration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "float",
          "required": false,
          "default": 4.0
        },
        {
          "name": "as_dict",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator, _BigQueryOperatorsEncryptionConfigurationMixin",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "_submit_job",
          "generate_query",
          "_assign_project_id",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Fetch data and return it, either from a BigQuery table, or results of a query job.\n\nData could be narrowed down by specific columns or retrieved as a whole.\nIt is returned in either of the following two formats, based on \"as_dict\" value:\n1. False (Default) - A Python list of lists, with the number of nested lists equal to the number of rows\nfetched. Each nested list represents a row, where the elements within it correspond to the column values\nfor that particular row.\n\n**Example Result**: ``[['T",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.283897",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquerygetdataoperator",
        "big",
        "bigquery",
        "data",
        "get",
        "cloud",
        "query"
      ]
    },
    {
      "id": "official_bigquerycreatetableoperator_20260128_160330",
      "component_name": "BigQueryCreateTableOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryCreateTableOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset_id",
          "table_id",
          "table_resource",
          "project_id",
          "gcs_schema_object",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "table_id",
          "type": "str",
          "required": false
        },
        {
          "name": "table_resource",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcs_schema_object",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "google_cloud_storage_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "if_exists",
          "type": "str",
          "required": false,
          "default": "log"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a new table in the specified BigQuery dataset, optionally with schema.\n\nThe schema to be used for the BigQuery table may be specified in one of\ntwo ways. You may either directly pass the schema fields in, or you may\npoint the operator to a Google Cloud Storage object name. The object in\nGoogle Cloud Storage must be a JSON file with the schema fields in it.\nYou can also create a table without schema.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the gu",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.284228",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "bigquerycreatetableoperator",
        "create",
        "big",
        "gcp",
        "bigquery",
        "table",
        "cloud",
        "query"
      ]
    },
    {
      "id": "official_bigquerydeletedatasetoperator_20260128_160330",
      "component_name": "BigQueryDeleteDatasetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryDeleteDatasetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "delete_contents",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete an existing dataset from your Project in BigQuery.\n\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/datasets/delete\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryDeleteDatasetOperator`\n\n:param project_id: The project id of the dataset.\n:param dataset_id: The dataset to be deleted.\n:param delete_contents: (Optional) Whether to force the deletion even if the dataset is not empty.\n    Will delete all ta",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.284542",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "big",
        "bigquery",
        "delete",
        "bigquerydeletedatasetoperator",
        "cloud",
        "dataset",
        "query"
      ]
    },
    {
      "id": "official_bigquerycreateemptydatasetoperator_20260128_160330",
      "component_name": "BigQueryCreateEmptyDatasetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryCreateEmptyDatasetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset_id",
          "project_id",
          "dataset_reference",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataset_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "dataset_reference",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "if_exists",
          "type": "str",
          "required": false,
          "default": "log"
        },
        {
          "name": "exists_ok",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a new dataset for your Project in BigQuery.\n\nhttps://cloud.google.com/bigquery/docs/reference/rest/v2/datasets#resource\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryCreateEmptyDatasetOperator`\n\n:param project_id: The name of the project where we want to create the dataset.\n:param dataset_id: The id of dataset. Don't need to provide, if datasetId in dataset_reference.\n:param location: The geographic location ",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.284873",
      "relevance_keywords": [
        "gcs",
        "empty",
        "operator",
        "bigquerycreateemptydatasetoperator",
        "google",
        "create",
        "big",
        "gcp",
        "bigquery",
        "cloud",
        "dataset",
        "query"
      ]
    },
    {
      "id": "official_bigquerygetdatasetoperator_20260128_160330",
      "component_name": "BigQueryGetDatasetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryGetDatasetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Get the dataset specified by ID.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryGetDatasetOperator`\n\n:param dataset_id: The id of dataset. Don't need to provide,\n    if datasetId in dataset_reference.\n:param project_id: The name of the project where we want to create the dataset.\n    Don't need to provide, if projectId in dataset_reference.\n:param gcp_conn_id: (Optional) The connection ID used to connect to Google Cl",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.285177",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquerygetdatasetoperator",
        "big",
        "bigquery",
        "get",
        "cloud",
        "dataset",
        "query"
      ]
    },
    {
      "id": "official_bigquerygetdatasettablesoperator_20260128_160330",
      "component_name": "BigQueryGetDatasetTablesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryGetDatasetTablesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "max_results",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Retrieve the list of tables in the specified dataset.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryGetDatasetTablesOperator`\n\n:param dataset_id: the dataset ID of the requested dataset.\n:param project_id: (Optional) the project of the requested dataset. If None,\n    self.project_id will be used.\n:param max_results: (Optional) the maximum number of tables to return.\n:param gcp_conn_id: (Optional) The connection ID u",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.285491",
      "relevance_keywords": [
        "bigquerygetdatasettablesoperator",
        "gcs",
        "operator",
        "google",
        "tables",
        "gcp",
        "big",
        "bigquery",
        "get",
        "cloud",
        "dataset",
        "query"
      ]
    },
    {
      "id": "official_bigqueryupdatetableoperator_20260128_160330",
      "component_name": "BigQueryUpdateTableOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryUpdateTableOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset_id",
          "table_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "table_resource",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "fields",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "dataset_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "table_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update a table for your Project in BigQuery.\n\nUse ``fields`` to specify which fields of table to update. If a field\nis listed in ``fields`` and is ``None`` in table, it will be deleted.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryUpdateTableOperator`\n\n:param dataset_id: The id of dataset. Don't need to provide,\n    if datasetId in table_reference.\n:param table_id: The id of table. Don't need to provide,\n    if tab",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.286122",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "big",
        "bigquery",
        "bigqueryupdatetableoperator",
        "table",
        "cloud",
        "query"
      ]
    },
    {
      "id": "official_bigqueryupdatedatasetoperator_20260128_160330",
      "component_name": "BigQueryUpdateDatasetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryUpdateDatasetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataset_resource",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "fields",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "dataset_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update a dataset for your Project in BigQuery.\n\nUse ``fields`` to specify which fields of dataset to update. If a field\nis listed in ``fields`` and is ``None`` in dataset, it will be deleted.\nIf no ``fields`` are provided then all fields of provided ``dataset_resource``\nwill be used.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryUpdateDatasetOperator`\n\n:param dataset_id: The id of dataset. Don't need to provide,\n   ",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.286933",
      "relevance_keywords": [
        "bigqueryupdatedatasetoperator",
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "big",
        "bigquery",
        "cloud",
        "dataset",
        "query"
      ]
    },
    {
      "id": "official_bigquerydeletetableoperator_20260128_160330",
      "component_name": "BigQueryDeleteTableOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryDeleteTableOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "deletion_dataset_table",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "deletion_dataset_table",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "ignore_if_missing",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a BigQuery table.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryDeleteTableOperator`\n\n:param deletion_dataset_table: A dotted\n    ``(<project>.|<project>:)<dataset>.<table>`` that indicates which table\n    will be deleted. (templated)\n:param gcp_conn_id: (Optional) The connection ID used to connect to Google Cloud.\n:param ignore_if_missing: if True, then return success even if the\n    requested table does not",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.287416",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "big",
        "bigquery",
        "delete",
        "table",
        "cloud",
        "bigquerydeletetableoperator",
        "query"
      ]
    },
    {
      "id": "official_bigqueryupserttableoperator_20260128_160330",
      "component_name": "BigQueryUpsertTableOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryUpsertTableOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset_id",
          "table_resource",
          "gcp_conn_id",
          "impersonation_chain",
          "project_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "table_resource",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Upsert to a BigQuery table.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryUpsertTableOperator`\n\n:param dataset_id: A dotted\n    ``(<project>.|<project>:)<dataset>`` that indicates which dataset\n    will be updated. (templated)\n:param table_resource: a table resource. see\n    https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n:param project_id: The name of the project where we want to update the data",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.287873",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "upsert",
        "bigqueryupserttableoperator",
        "big",
        "gcp",
        "bigquery",
        "table",
        "cloud",
        "query"
      ]
    },
    {
      "id": "official_bigqueryupdatetableschemaoperator_20260128_160330",
      "component_name": "BigQueryUpdateTableSchemaOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryUpdateTableSchemaOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "schema_fields_updates",
          "dataset_id",
          "table_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "schema_fields_updates",
          "type": "list[...]",
          "required": false
        },
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "table_id",
          "type": "str",
          "required": false
        },
        {
          "name": "include_policy_tags",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update BigQuery Table Schema.\n\nUpdates fields on a table schema based on contents of the supplied schema_fields_updates\nparameter. The supplied schema does not need to be complete, if the field\nalready exists in the schema you only need to supply keys & values for the\nitems you want to patch, just ensure the \"name\" key is set.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryUpdateTableSchemaOperator`\n\n:param schema_fi",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.288243",
      "relevance_keywords": [
        "schema",
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "big",
        "bigquery",
        "table",
        "cloud",
        "bigqueryupdatetableschemaoperator",
        "query"
      ]
    },
    {
      "id": "official_bigqueryinsertjoboperator_20260128_160330",
      "component_name": "BigQueryInsertJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryInsertJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator",
          "_BigQueryInsertJobOperatorOpenLineageMixin"
        ],
        "template_fields": [
          "configuration",
          "job_id",
          "gcp_conn_id",
          "impersonation_chain",
          "project_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "configuration",
          "type": "dict[...]",
          "required": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "force_rerun",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "reattach_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cancel_on_kill",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "result_retry",
          "type": "Retry",
          "required": false,
          "default": "<DEFAULT_RETRY>"
        },
        {
          "name": "result_timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "float",
          "required": false,
          "default": 4.0
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator, _BigQueryInsertJobOperatorOpenLineageMixin",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport json\nimport re\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, SupportsAbs\nfrom google.api_core.exceptions import Conflict",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "sql",
          "prepare_template",
          "_add_job_labels",
          "_submit_job",
          "_handle_job_error",
          "execute",
          "execute_complete",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Execute a BigQuery job.\n\nWaits for the job to complete and returns job id.\nThis operator work in the following way:\n\n- it calculates a unique hash of the job using job's configuration or uuid if ``force_rerun`` is True\n- creates ``job_id`` in form of\n    ``[provided_job_id | airflow_{dag_id}_{task_id}_{exec_date}]_{uniqueness_suffix}``\n- submits a BigQuery job using the ``job_id``\n- if job with given id already exists then it tries to reattach to the job if its not done and its\n    state is in `",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.288815",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "insert",
        "gcp",
        "big",
        "bigquery",
        "cloud",
        "bigqueryinsertjoboperator",
        "job",
        "query"
      ]
    },
    {
      "id": "official_bigquerycreatedatatransferoperator_20260128_160330",
      "component_name": "BigQueryCreateDataTransferOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryCreateDataTransferOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "transfer_config",
          "project_id",
          "authorization_code",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "transfer_config",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "authorization_code",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "Any",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.bigquery_datatransfer_v1 import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a new data transfer configuration.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryCreateDataTransferOperator`\n\n:param transfer_config: Data transfer configuration to create.\n:param project_id: The BigQuery project id where the transfer configuration should be\n        created. If set to None or missing, the default project_id from the Google Cloud connection\n        is used.\n:param location: BigQuery Transfer ",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.322805",
      "relevance_keywords": [
        "gcs",
        "bigquerycreatedatatransferoperator",
        "operator",
        "google",
        "gcp",
        "create",
        "big",
        "bigquery",
        "data",
        "transfer",
        "cloud",
        "query"
      ]
    },
    {
      "id": "official_bigquerydeletedatatransferconfigoperator_20260128_160330",
      "component_name": "BigQueryDeleteDataTransferConfigOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryDeleteDataTransferConfigOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "transfer_config_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "transfer_config_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "Any",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.bigquery_datatransfer_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes transfer configuration.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryDeleteDataTransferConfigOperator`\n\n:param transfer_config_id: Id of transfer config to be used.\n:param project_id: The BigQuery project id where the transfer configuration should be\n    created. If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param location: BigQuery Transfer Service location fo",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.322962",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "big",
        "bigquery",
        "bigquerydeletedatatransferconfigoperator",
        "delete",
        "data",
        "transfer",
        "config",
        "cloud",
        "query"
      ]
    },
    {
      "id": "official_bigquerydatatransferservicestarttransferrunsoperator_20260128_160330",
      "component_name": "BigQueryDataTransferServiceStartTransferRunsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryDataTransferServiceStartTransferRunsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "transfer_config_id",
          "project_id",
          "requested_time_range",
          "requested_run_time",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "transfer_config_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "requested_time_range",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "requested_run_time",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "Any",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.bigquery_datatransfer_v1 import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "hook",
          "execute",
          "_wait_for_transfer_to_be_done",
          "_job_is_done",
          "execute_completed",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Start manual transfer runs to be executed now with schedule_time equal to current time.\n\nThe transfer runs can be created for a time range where the run_time is between\nstart_time (inclusive) and end_time (exclusive), or for a specific run_time.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigQueryDataTransferServiceStartTransferRunsOperator`\n\n:param transfer_config_id: Id of transfer config to be used.\n:param requested_ti",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.323102",
      "relevance_keywords": [
        "runs",
        "bigquerydatatransferservicestarttransferrunsoperator",
        "gcs",
        "operator",
        "google",
        "service",
        "start",
        "big",
        "gcp",
        "bigquery",
        "data",
        "transfer",
        "cloud",
        "query"
      ]
    },
    {
      "id": "official_bigtablecreateinstanceoperator_20260128_160330",
      "component_name": "BigtableCreateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigtableCreateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator",
          "BigtableValidationMixin"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "main_cluster_id",
          "main_cluster_zone",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "main_cluster_id",
          "type": "str",
          "required": false
        },
        {
          "name": "main_cluster_zone",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "replica_clusters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "instance_display_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "instance_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "instance_labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_nodes",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_storage_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator, BigtableValidationMixin",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.api_core.exceptions\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.bigtable import BigtableHook\nfrom airflow.providers.google.cloud.links.bigtable import (",
        "class_attributes": [
          "REQUIRED_ATTRIBUTES",
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new Cloud Bigtable instance.\n\nIf the Cloud Bigtable instance with the given ID exists, the operator does not\ncompare its configuration and immediately succeeds. No changes are made to the\nexisting instance.\n\nFor more details about instance creation have a look at the reference:\nhttps://googleapis.github.io/google-cloud-python/latest/bigtable/instance.html#google.cloud.bigtable.instance.Instance.create\n\n.. seealso::\n    For more information on how to use this operator, take a look at th",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.351463",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "instance",
        "bigquery",
        "bigtable",
        "bigtablecreateinstanceoperator",
        "cloud"
      ]
    },
    {
      "id": "official_bigtableupdateinstanceoperator_20260128_160330",
      "component_name": "BigtableUpdateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigtableUpdateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator",
          "BigtableValidationMixin"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "instance_display_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "instance_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "instance_labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator, BigtableValidationMixin",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.api_core.exceptions\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.bigtable import BigtableHook\nfrom airflow.providers.google.cloud.links.bigtable import (",
        "class_attributes": [
          "REQUIRED_ATTRIBUTES",
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates an existing Cloud Bigtable instance.\n\nFor more details about instance creation have a look at the reference:\nhttps://googleapis.dev/python/bigtable/latest/instance.html#google.cloud.bigtable.instance.Instance.update\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigtableUpdateInstanceOperator`\n\n:param instance_id: The ID of the Cloud Bigtable instance to update.\n:param project_id: Optional, the ID of the Google Cloud",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.351650",
      "relevance_keywords": [
        "gcs",
        "bigtableupdateinstanceoperator",
        "operator",
        "google",
        "update",
        "instance",
        "gcp",
        "bigquery",
        "bigtable",
        "cloud"
      ]
    },
    {
      "id": "official_bigtabledeleteinstanceoperator_20260128_160330",
      "component_name": "BigtableDeleteInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigtableDeleteInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator",
          "BigtableValidationMixin"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator, BigtableValidationMixin",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.api_core.exceptions\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.bigtable import BigtableHook\nfrom airflow.providers.google.cloud.links.bigtable import (",
        "class_attributes": [
          "REQUIRED_ATTRIBUTES",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes the Cloud Bigtable instance, including its clusters and all related tables.\n\nFor more details about deleting instance have a look at the reference:\nhttps://googleapis.github.io/google-cloud-python/latest/bigtable/instance.html#google.cloud.bigtable.instance.Instance.delete\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigtableDeleteInstanceOperator`\n\n:param instance_id: The ID of the Cloud Bigtable instance to delet",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.351800",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigtabledeleteinstanceoperator",
        "instance",
        "bigquery",
        "delete",
        "bigtable",
        "cloud"
      ]
    },
    {
      "id": "official_bigtablecreatetableoperator_20260128_160330",
      "component_name": "BigtableCreateTableOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigtableCreateTableOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator",
          "BigtableValidationMixin"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "table_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "table_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "initial_split_keys",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "column_families",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator, BigtableValidationMixin",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.api_core.exceptions\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.bigtable import BigtableHook\nfrom airflow.providers.google.cloud.links.bigtable import (",
        "class_attributes": [
          "REQUIRED_ATTRIBUTES",
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_compare_column_families",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates the table in the Cloud Bigtable instance.\n\nFor more details about creating table have a look at the reference:\nhttps://googleapis.github.io/google-cloud-python/latest/bigtable/table.html#google.cloud.bigtable.table.Table.create\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigtableCreateTableOperator`\n\n:param instance_id: The ID of the Cloud Bigtable instance that will\n    hold the new table.\n:param table_id: The ID",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.351962",
      "relevance_keywords": [
        "bigtablecreatetableoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "bigtable",
        "table",
        "cloud"
      ]
    },
    {
      "id": "official_bigtabledeletetableoperator_20260128_160330",
      "component_name": "BigtableDeleteTableOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigtableDeleteTableOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator",
          "BigtableValidationMixin"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "table_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "table_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "app_profile_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator, BigtableValidationMixin",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.api_core.exceptions\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.bigtable import BigtableHook\nfrom airflow.providers.google.cloud.links.bigtable import (",
        "class_attributes": [
          "REQUIRED_ATTRIBUTES",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes the Cloud Bigtable table.\n\nFor more details about deleting table have a look at the reference:\nhttps://googleapis.github.io/google-cloud-python/latest/bigtable/table.html#google.cloud.bigtable.table.Table.delete\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigtableDeleteTableOperator`\n\n:param instance_id: The ID of the Cloud Bigtable instance.\n:param table_id: The ID of the table to be deleted.\n:param project_id: O",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.352101",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigtabledeletetableoperator",
        "bigquery",
        "delete",
        "bigtable",
        "table",
        "cloud"
      ]
    },
    {
      "id": "official_bigtableupdateclusteroperator_20260128_160330",
      "component_name": "BigtableUpdateClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigtableUpdateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator",
          "BigtableValidationMixin"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "cluster_id",
          "nodes",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "cluster_id",
          "type": "str",
          "required": false
        },
        {
          "name": "nodes",
          "type": "int",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator, BigtableValidationMixin",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.api_core.exceptions\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.bigtable import BigtableHook\nfrom airflow.providers.google.cloud.links.bigtable import (",
        "class_attributes": [
          "REQUIRED_ATTRIBUTES",
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates a Cloud Bigtable cluster.\n\nFor more details about updating a Cloud Bigtable cluster,\nhave a look at the reference:\nhttps://googleapis.github.io/google-cloud-python/latest/bigtable/cluster.html#google.cloud.bigtable.cluster.Cluster.update\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigtableUpdateClusterOperator`\n\n:param instance_id: The ID of the Cloud Bigtable instance.\n:param cluster_id: The ID of the Cloud Bigta",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.352247",
      "relevance_keywords": [
        "bigtableupdateclusteroperator",
        "gcs",
        "operator",
        "google",
        "update",
        "cluster",
        "gcp",
        "bigquery",
        "bigtable",
        "cloud"
      ]
    },
    {
      "id": "official_googlecloudbaseoperator_20260128_160330",
      "component_name": "GoogleCloudBaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCloudBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from BaseOperator"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import Any\nfrom google.api_core.gapic_v1.method import DEFAULT\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [],
        "methods": [
          "__deepcopy__",
          "extra_links_params"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Abstract base class for operators using Google API client libraries.",
      "success_score": 150,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.381564",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "googlecloudbaseoperator",
        "bigquery",
        "base",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbatchsubmitjoboperator_20260128_160330",
      "component_name": "CloudBatchSubmitJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBatchSubmitJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain",
          "job_name"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "job",
          "type": "Union",
          "required": true
        },
        {
          "name": "polling_period_seconds",
          "type": "float",
          "required": false,
          "default": 10
        },
        {
          "name": "timeout_seconds",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.cloud.batch_v1 import Job, Task\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_batch import CloudBatchHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.cloud.triggers.cloud_batch import CloudBatchJobFinishedTrigger",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Submit a job and wait for its completion.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param job_name: Required. The name of the job to create.\n:param job: Required. The job descriptor containing the configuration of the job to submit.\n:param polling_period_seconds: Optional: Control the rate of the poll for the result of deferrable run.\n    By default, the tr",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.393043",
      "relevance_keywords": [
        "batch",
        "gcs",
        "operator",
        "cloudbatchsubmitjoboperator",
        "google",
        "gcp",
        "submit",
        "bigquery",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_cloudbatchdeletejoboperator_20260128_160330",
      "component_name": "CloudBatchDeleteJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBatchDeleteJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain",
          "job_name"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.cloud.batch_v1 import Job, Task\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_batch import CloudBatchHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.cloud.triggers.cloud_batch import CloudBatchJobFinishedTrigger",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "_wait_for_operation"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a job and wait for the operation to be completed.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param job_name: Required. The name of the job to be deleted.\n:param timeout: The timeout for this request.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param impersonation_chain: Optional service account to impersonate using short-t",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.393165",
      "relevance_keywords": [
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "cloudbatchdeletejoboperator",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_cloudbatchlistjobsoperator_20260128_160330",
      "component_name": "CloudBatchListJobsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBatchListJobsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "limit",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.cloud.batch_v1 import Job, Task\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_batch import CloudBatchHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.cloud.triggers.cloud_batch import CloudBatchJobFinishedTrigger",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "List Cloud Batch jobs.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param filter: The filter based on which to list the jobs. If left empty, all the jobs are listed.\n:param limit: The number of jobs to list. If left empty,\n    all the jobs matching the filter will be returned.\n:param impers",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.393250",
      "relevance_keywords": [
        "batch",
        "cloudbatchlistjobsoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "jobs",
        "bigquery",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbatchlisttasksoperator_20260128_160330",
      "component_name": "CloudBatchListTasksOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBatchListTasksOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "job_name",
          "gcp_conn_id",
          "impersonation_chain",
          "group_name"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "group_name",
          "type": "str",
          "required": false,
          "default": "group0"
        },
        {
          "name": "filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "limit",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.cloud.batch_v1 import Job, Task\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_batch import CloudBatchHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.cloud.triggers.cloud_batch import CloudBatchJobFinishedTrigger",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "List Cloud Batch tasks for a given job.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param job_name: Required. The name of the job for which to list tasks.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param filter: The filter based on which to list the jobs. If left empty, all the jobs are listed.\n:param group_name: The name of the g",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.393335",
      "relevance_keywords": [
        "batch",
        "gcs",
        "tasks",
        "cloudbatchlisttasksoperator",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuildcancelbuildoperator_20260128_160330",
      "component_name": "CloudBuildCancelBuildOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildCancelBuildOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "id_",
          "gcp_conn_id",
          "location"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "id_",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Cancels a build in progress.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildCancelBuildOperator`\n\n:param id_: The ID of the build.\n:param project_id: Optional, Google Cloud Project project_id where the function belongs.\n    If set to None or missing, the default project_id from the GCP connection is used.\n:param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests\n    will not b",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.414004",
      "relevance_keywords": [
        "gcs",
        "build",
        "operator",
        "cloudbuildcancelbuildoperator",
        "google",
        "gcp",
        "cancel",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuildcreatebuildoperator_20260128_160330",
      "component_name": "CloudBuildCreateBuildOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildCreateBuildOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "build",
          "gcp_conn_id",
          "impersonation_chain",
          "location"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "build",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "wait",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "poll_interval",
          "type": "float",
          "required": false,
          "default": 4.0
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "prepare_template",
          "extra_links_params",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts a build with the specified configuration.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildCreateBuildOperator`\n\n:param build: The build resource to create. If a dict is provided, it must be of\n    the same form as the protobuf message `google.cloud.devtools.cloudbuild_v1.types.Build`.\n:param project_id: Optional, Google Cloud Project project_id where the function belongs.\n    If set to None or missing, the de",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.414187",
      "relevance_keywords": [
        "gcs",
        "cloudbuildcreatebuildoperator",
        "build",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuildcreatebuildtriggeroperator_20260128_160330",
      "component_name": "CloudBuildCreateBuildTriggerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildCreateBuildTriggerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "trigger",
          "gcp_conn_id",
          "location"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "trigger",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a new BuildTrigger.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildCreateBuildTriggerOperator`\n\n:param trigger: The BuildTrigger to create. If a dict is provided, it must be of the same form\n    as the protobuf message `google.cloud.devtools.cloudbuild_v1.types.BuildTrigger`\n:param project_id: Optional, Google Cloud Project project_id where the function belongs.\n    If set to None or missing, the default pr",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.414334",
      "relevance_keywords": [
        "gcs",
        "build",
        "operator",
        "google",
        "trigger",
        "create",
        "gcp",
        "bigquery",
        "cloudbuildcreatebuildtriggeroperator",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuilddeletebuildtriggeroperator_20260128_160330",
      "component_name": "CloudBuildDeleteBuildTriggerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildDeleteBuildTriggerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "trigger_id",
          "gcp_conn_id",
          "location"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "trigger_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a BuildTrigger by its project ID and trigger ID.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildDeleteBuildTriggerOperator`\n\n:param trigger_id: The ID of the BuildTrigger to delete.\n:param project_id: Optional, Google Cloud Project project_id where the function belongs.\n    If set to None or missing, the default project_id from the GCP connection is used.\n:param retry: Optional, a retry object used  to retr",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.414471",
      "relevance_keywords": [
        "gcs",
        "build",
        "operator",
        "google",
        "trigger",
        "cloudbuilddeletebuildtriggeroperator",
        "gcp",
        "bigquery",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuildgetbuildoperator_20260128_160330",
      "component_name": "CloudBuildGetBuildOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildGetBuildOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "id_",
          "gcp_conn_id",
          "location"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "id_",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Returns information about a previously requested build.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildGetBuildOperator`\n\n:param id_: The ID of the build.\n:param project_id: Optional, Google Cloud Project project_id where the function belongs.\n    If set to None or missing, the default project_id from the GCP connection is used.\n:param retry: Optional, a retry object used  to retry requests. If `None` is specified,",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.414606",
      "relevance_keywords": [
        "gcs",
        "build",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "cloudbuildgetbuildoperator",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuildgetbuildtriggeroperator_20260128_160330",
      "component_name": "CloudBuildGetBuildTriggerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildGetBuildTriggerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "trigger_id",
          "gcp_conn_id",
          "location"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "trigger_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Returns information about a BuildTrigger.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildGetBuildTriggerOperator`\n\n:param trigger_id: The ID of the BuildTrigger to get.\n:param project_id: Optional, Google Cloud Project project_id where the function belongs.\n    If set to None or missing, the default project_id from the GCP connection is used.\n:param retry: Optional, a retry object used  to retry requests. If `None`",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.414748",
      "relevance_keywords": [
        "gcs",
        "build",
        "operator",
        "google",
        "trigger",
        "gcp",
        "bigquery",
        "cloudbuildgetbuildtriggeroperator",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuildlistbuildtriggersoperator_20260128_160330",
      "component_name": "CloudBuildListBuildTriggersOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildListBuildTriggersOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "gcp_conn_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Lists existing BuildTriggers.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildListBuildTriggersOperator`\n\n:param location: The location of the project.\n:param project_id: Optional, Google Cloud Project project_id where the function belongs.\n    If set to None or missing, the default project_id from the GCP connection is used.\n:param page_size: Optional, number of results to return in the list.\n:param page_token: Opt",
      "success_score": 194,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.414889",
      "relevance_keywords": [
        "gcs",
        "build",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "triggers",
        "cloudbuildlistbuildtriggersoperator",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuildlistbuildsoperator_20260128_160330",
      "component_name": "CloudBuildListBuildsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildListBuildsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "gcp_conn_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Lists previously requested builds.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildListBuildsOperator`\n\n:param location: The location of the project.\n:param project_id: Optional, Google Cloud Project project_id where the function belongs.\n    If set to None or missing, the default project_id from the GCP connection is used.\n:param page_size: Optional, number of results to return in the list.\n:param filter_: Optional",
      "success_score": 194,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.415029",
      "relevance_keywords": [
        "gcs",
        "build",
        "cloudbuildlistbuildsoperator",
        "operator",
        "google",
        "builds",
        "gcp",
        "bigquery",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuildretrybuildoperator_20260128_160330",
      "component_name": "CloudBuildRetryBuildOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildRetryBuildOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "id_",
          "gcp_conn_id",
          "location"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "id_",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "wait",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a new build using the original build request, which may or may not result in an identical build.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildRetryBuildOperator`\n\n:param id_: Build ID of the original build.\n:param project_id: Optional, Google Cloud Project project_id where the function belongs.\n    If set to None or missing, the default project_id from the GCP connection is used.\n:param wait: Optional, wa",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.415171",
      "relevance_keywords": [
        "gcs",
        "build",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "cloudbuildretrybuildoperator",
        "retry",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuildrunbuildtriggeroperator_20260128_160330",
      "component_name": "CloudBuildRunBuildTriggerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildRunBuildTriggerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "trigger_id",
          "source",
          "gcp_conn_id",
          "location"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "trigger_id",
          "type": "str",
          "required": false
        },
        {
          "name": "source",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "wait",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Runs a BuildTrigger at a particular source revision.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildRunBuildTriggerOperator`\n\n:param trigger_id: The ID of the trigger.\n:param source: Source to build against this trigger. If a dict is provided, it must be of the same form\n    as the protobuf message `google.cloud.devtools.cloudbuild_v1.types.RepoSource`\n:param project_id: Optional, Google Cloud Project project_id wh",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.415307",
      "relevance_keywords": [
        "gcs",
        "build",
        "operator",
        "google",
        "trigger",
        "gcp",
        "cloudbuildrunbuildtriggeroperator",
        "bigquery",
        "run",
        "cloud"
      ]
    },
    {
      "id": "official_cloudbuildupdatebuildtriggeroperator_20260128_160330",
      "component_name": "CloudBuildUpdateBuildTriggerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildUpdateBuildTriggerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "trigger_id",
          "trigger",
          "gcp_conn_id",
          "location"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "trigger_id",
          "type": "str",
          "required": false
        },
        {
          "name": "trigger",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "global"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport re\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import unquote, urlsplit\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1.types import Build, BuildTrigger, RepoSource",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Updates a BuildTrigger by its project ID and trigger ID.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudBuildUpdateBuildTriggerOperator`\n\n:param trigger_id: The ID of the trigger.\n:param trigger: The BuildTrigger to create. If a dict is provided, it must be of the same form\n    as the protobuf message `google.cloud.devtools.cloudbuild_v1.types.BuildTrigger`\n:param project_id: Optional, Google Cloud Project project_id wh",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.415443",
      "relevance_keywords": [
        "gcs",
        "build",
        "operator",
        "google",
        "update",
        "trigger",
        "gcp",
        "bigquery",
        "cloud",
        "cloudbuildupdatebuildtriggeroperator"
      ]
    },
    {
      "id": "official_cloudcomposercreateenvironmentoperator_20260128_160330",
      "component_name": "CloudComposerCreateEnvironmentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerCreateEnvironmentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "environment_id",
          "environment",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "environment_id",
          "type": "str",
          "required": false
        },
        {
          "name": "environment",
          "type": "Union",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "pooling_period_seconds",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport shlex\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.orchestration.airflow.service_v1 import ImageVersion\nfrom google.cloud.orchestration.airflow.service_v1.types import Environment, ExecuteAirflowCommandResponse\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_composer import CloudComposerHook",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a new environment.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param environment_id: Required. The ID of the Google Cloud environment that the service belongs to.\n:param environment:  The environment to create.\n:param gcp_conn_id:\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of ac",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.437734",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "cloudcomposercreateenvironmentoperator",
        "environment",
        "cloud",
        "composer"
      ]
    },
    {
      "id": "official_cloudcomposerdeleteenvironmentoperator_20260128_160330",
      "component_name": "CloudComposerDeleteEnvironmentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerDeleteEnvironmentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "environment_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "environment_id",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "pooling_period_seconds",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport shlex\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.orchestration.airflow.service_v1 import ImageVersion\nfrom google.cloud.orchestration.airflow.service_v1.types import Environment, ExecuteAirflowCommandResponse\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_composer import CloudComposerHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete an environment.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param environment_id: Required. The ID of the Google Cloud environment that the service belongs to.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata: Strings which should be sent along with the request as metadat",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.437909",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "cloudcomposerdeleteenvironmentoperator",
        "environment",
        "cloud",
        "composer"
      ]
    },
    {
      "id": "official_cloudcomposergetenvironmentoperator_20260128_160330",
      "component_name": "CloudComposerGetEnvironmentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerGetEnvironmentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "environment_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "environment_id",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport shlex\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.orchestration.airflow.service_v1 import ImageVersion\nfrom google.cloud.orchestration.airflow.service_v1.types import Environment, ExecuteAirflowCommandResponse\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_composer import CloudComposerHook",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Get an existing environment.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param environment_id: Required. The ID of the Google Cloud environment that the service belongs to.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata: Strings which should be sent along with the request as m",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.438043",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "cloudcomposergetenvironmentoperator",
        "bigquery",
        "environment",
        "get",
        "cloud",
        "composer"
      ]
    },
    {
      "id": "official_cloudcomposerlistenvironmentsoperator_20260128_160330",
      "component_name": "CloudComposerListEnvironmentsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerListEnvironmentsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport shlex\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.orchestration.airflow.service_v1 import ImageVersion\nfrom google.cloud.orchestration.airflow.service_v1.types import Environment, ExecuteAirflowCommandResponse\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_composer import CloudComposerHook",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "List environments.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param page_size: The maximum number of environments to return.\n:param page_token: The next_page_token value returned from a previous List\n    request, if any.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata: Strings",
      "success_score": 194,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.438170",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "environments",
        "bigquery",
        "list",
        "cloud",
        "composer",
        "cloudcomposerlistenvironmentsoperator"
      ]
    },
    {
      "id": "official_cloudcomposerupdateenvironmentoperator_20260128_160330",
      "component_name": "CloudComposerUpdateEnvironmentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerUpdateEnvironmentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "environment_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "environment_id",
          "type": "str",
          "required": false
        },
        {
          "name": "environment",
          "type": "Union",
          "required": false
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "pooling_period_seconds",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport shlex\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.orchestration.airflow.service_v1 import ImageVersion\nfrom google.cloud.orchestration.airflow.service_v1.types import Environment, ExecuteAirflowCommandResponse\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_composer import CloudComposerHook",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update an environment.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param environment_id: Required. The ID of the Google Cloud environment that the service belongs to.\n:param environment:  A patch environment. Fields specified by the ``updateMask`` will be copied from the\n    patch environment into the environment under update.\n:param update_mask:  Required. A",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.438311",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "bigquery",
        "environment",
        "cloud",
        "cloudcomposerupdateenvironmentoperator",
        "composer"
      ]
    },
    {
      "id": "official_cloudcomposerlistimageversionsoperator_20260128_160330",
      "component_name": "CloudComposerListImageVersionsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerListImageVersionsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "include_past_releases",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport shlex\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.orchestration.airflow.service_v1 import ImageVersion\nfrom google.cloud.orchestration.airflow.service_v1.types import Environment, ExecuteAirflowCommandResponse\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_composer import CloudComposerHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "List ImageVersions for provided location.\n\n:param request:  The request object. List ImageVersions in a project and location.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata: Strings which should be sent along with the request as metadata.\n:param gcp_conn_id:\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_",
      "success_score": 194,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.438434",
      "relevance_keywords": [
        "versions",
        "gcs",
        "operator",
        "google",
        "gcp",
        "image",
        "bigquery",
        "list",
        "cloud",
        "cloudcomposerlistimageversionsoperator",
        "composer"
      ]
    },
    {
      "id": "official_cloudcomposerrunairflowclicommandoperator_20260128_160330",
      "component_name": "CloudComposerRunAirflowCLICommandOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerRunAirflowCLICommandOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "environment_id",
          "command",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "environment_id",
          "type": "str",
          "required": false
        },
        {
          "name": "command",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport shlex\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.orchestration.airflow.service_v1 import ImageVersion\nfrom google.cloud.orchestration.airflow.service_v1.types import Environment, ExecuteAirflowCommandResponse\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_composer import CloudComposerHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "_parse_cmd_to_args",
          "_merge_cmd_output_result"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Run Airflow command for provided Composer environment.\n\n:param project_id: The ID of the Google Cloud project that the service belongs to.\n:param region: The ID of the Google Cloud region that the service belongs to.\n:param environment_id: The ID of the Google Cloud environment that the service belongs to.\n:param command: Airflow command.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata: Strings which should be sen",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.438560",
      "relevance_keywords": [
        "gcs",
        "airflow",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "run",
        "cloud",
        "cloudcomposerrunairflowclicommandoperator",
        "composer",
        "command"
      ]
    },
    {
      "id": "official_cloudcomposertriggerdagrunoperator_20260128_160330",
      "component_name": "CloudComposerTriggerDAGRunOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerTriggerDAGRunOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "environment_id",
          "composer_dag_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "environment_id",
          "type": "str",
          "required": false
        },
        {
          "name": "composer_dag_id",
          "type": "str",
          "required": false
        },
        {
          "name": "composer_dag_conf",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport shlex\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.orchestration.airflow.service_v1 import ImageVersion\nfrom google.cloud.orchestration.airflow.service_v1.types import Environment, ExecuteAirflowCommandResponse\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_composer import CloudComposerHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Trigger DAG run for provided Composer environment.\n\n:param project_id: The ID of the Google Cloud project that the service belongs to.\n:param region: The ID of the Google Cloud region that the service belongs to.\n:param environment_id: The ID of the Google Cloud environment that the service belongs to.\n:param composer_dag_id: The ID of DAG which will be triggered.\n:param composer_dag_conf: Configuration parameters for the DAG run.\n:param timeout: The timeout for this request.\n:param gcp_conn_id:",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.438679",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "cloudcomposertriggerdagrunoperator",
        "trigger",
        "gcp",
        "bigquery",
        "run",
        "cloud",
        "composer"
      ]
    },
    {
      "id": "official_cloudloggingcreatesinkoperator_20260128_160330",
      "component_name": "CloudLoggingCreateSinkOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudLoggingCreateSinkOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "sink_config",
          "gcp_conn_id",
          "impersonation_chain",
          "unique_writer_identity"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "sink_config",
          "type": "Union",
          "required": true
        },
        {
          "name": "unique_writer_identity",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.logging_v2.types import LogSink\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.cloud_logging import CloudLoggingHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a Cloud Logging export sink in a GCP project.\n\nThis operator creates a sink that exports log entries from Cloud Logging\nto destinations like Cloud Storage, BigQuery, or Pub/Sub.\n\n:param project_id: Required. ID of the Google Cloud project where the sink will be created.\n:param sink_config: Required. The full sink configuration as a dictionary or a LogSink object.\n    See: https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks\n:param unique_writer_identity: If True, cre",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.468911",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "cloudloggingcreatesinkoperator",
        "logging",
        "bigquery",
        "sink",
        "cloud"
      ]
    },
    {
      "id": "official_cloudloggingdeletesinkoperator_20260128_160330",
      "component_name": "CloudLoggingDeleteSinkOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudLoggingDeleteSinkOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "sink_name",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "sink_name",
          "type": "str",
          "required": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.logging_v2.types import LogSink\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.cloud_logging import CloudLoggingHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a Cloud Logging export sink from a GCP project.\n\n:param sink_name: Required. Name of the sink to delete.\n:param project_id: Required. The ID of the Google Cloud project.\n:param gcp_conn_id: Optional. The connection ID to use for connecting to Google Cloud.\n    Defaults to \"google_cloud_default\".\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the l",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.469074",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "logging",
        "cloudloggingdeletesinkoperator",
        "delete",
        "bigquery",
        "sink",
        "cloud"
      ]
    },
    {
      "id": "official_cloudloggingupdatesinkoperator_20260128_160330",
      "component_name": "CloudLoggingUpdateSinkOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudLoggingUpdateSinkOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "sink_name",
          "project_id",
          "update_mask",
          "sink_config",
          "unique_writer_identity",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "sink_name",
          "type": "str",
          "required": true
        },
        {
          "name": "sink_config",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": true
        },
        {
          "name": "unique_writer_identity",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.logging_v2.types import LogSink\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.cloud_logging import CloudLoggingHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates an existing Cloud Logging export sink.\n\n:param project_id: Required. The ID of the Google Cloud project that contains the sink.\n:param sink_name: Required. The name of the sink to update.\n:param sink_config: Required. The updated sink configuration. Can be a dictionary or a\n    `google.cloud.logging_v2.types.LogSink` object. Refer to:\n    https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks\n:param update_mask: Required. A FieldMask or dictionary specifying which fiel",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.469199",
      "relevance_keywords": [
        "gcs",
        "cloudloggingupdatesinkoperator",
        "operator",
        "google",
        "update",
        "gcp",
        "logging",
        "bigquery",
        "sink",
        "cloud"
      ]
    },
    {
      "id": "official_cloudlogginglistsinksoperator_20260128_160330",
      "component_name": "CloudLoggingListSinksOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudLoggingListSinksOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "gcp_conn_id",
          "impersonation_chain",
          "page_size"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.logging_v2.types import LogSink\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.cloud_logging import CloudLoggingHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists Cloud Logging export sinks in a Google Cloud project.\n\n:param project_id: Required. The ID of the Google Cloud project to list sinks from.\n:param page_size: Optional. The maximum number of sinks to return per page. Must be greater than 0.\n    If None, the server will use a default value.\n:param gcp_conn_id: Optional. The connection ID used to connect to Google Cloud.\n    Defaults to \"google_cloud_default\".\n:param impersonation_chain: Optional. Service account or chained list of accounts to",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.469307",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "sinks",
        "logging",
        "bigquery",
        "cloudlogginglistsinksoperator",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystorecreateinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreCreateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreCreateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance_id",
          "instance",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a Redis instance based on the specified tier and memory size.\n\nBy default, the instance is accessible from the project's `default network\n<https://cloud.google.com/compute/docs/networks-and-firewalls#networks>`__.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreCreateInstanceOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:param instance_id: Require",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.492600",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "instance",
        "bigquery",
        "memorystore",
        "cloud",
        "cloudmemorystorecreateinstanceoperator"
      ]
    },
    {
      "id": "official_cloudmemorystoredeleteinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreDeleteInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreDeleteInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a specific Redis instance. Instance stops serving and data is deleted.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreDeleteInstanceOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:param instance: The logical name of the Redis instance in the customer project.\n:param project_id: Project ID of the project that contains the instance. If set\n    to No",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.493008",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "delete",
        "cloudmemorystoredeleteinstanceoperator",
        "memorystore",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystoreexportinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreExportInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreExportInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance",
          "output_config",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "output_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Export Redis instance data into a Redis RDB format file in Cloud Storage.\n\nRedis will continue serving during this operation.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreExportInstanceOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:param instance: The logical name of the Redis instance in the customer project.\n:param output_config: Required. Specify da",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.493362",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "export",
        "instance",
        "bigquery",
        "memorystore",
        "cloudmemorystoreexportinstanceoperator",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystorefailoverinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreFailoverInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreFailoverInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance",
          "data_protection_mode",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "data_protection_mode",
          "type": "Any",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Initiate a failover of the primary node for a specific STANDARD tier Cloud Memorystore for Redis instance.\n\nUses the current replica node.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreFailoverInstanceOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:param instance: The logical name of the Redis instance in the customer project.\n:param data_protection_mode",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.493829",
      "relevance_keywords": [
        "failover",
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "cloudmemorystorefailoverinstanceoperator",
        "bigquery",
        "memorystore",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystoregetinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreGetInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreGetInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets the details of a specific Redis instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreGetInstanceOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:param instance: The logical name of the Redis instance in the customer project.\n:param project_id: Project ID of the project that contains the instance. If set\n    to None or missing, the default project_",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.494234",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "memorystore",
        "get",
        "cloudmemorystoregetinstanceoperator",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystoreimportoperator_20260128_160330",
      "component_name": "CloudMemorystoreImportOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreImportOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance",
          "input_config",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "input_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Import a Redis RDB snapshot file from Cloud Storage into a Redis instance.\n\nRedis may stop serving during this operation. Instance state will be IMPORTING for entire operation. When\ncomplete, the instance will contain only data from the imported file.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreImportOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:para",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.494606",
      "relevance_keywords": [
        "gcs",
        "import",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "memorystore",
        "cloudmemorystoreimportoperator",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystorelistinstancesoperator_20260128_160330",
      "component_name": "CloudMemorystoreListInstancesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreListInstancesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "page_size",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "page_size",
          "type": "int",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists all Redis instances owned by a project in either the specified location (region) or all locations.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreListInstancesOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n    If it is specified as ``-`` (wildcard), then all regions available to the project are\n    queried, and the results are aggregated.\n:param pag",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.494988",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "cloudmemorystorelistinstancesoperator",
        "bigquery",
        "memorystore",
        "list",
        "cloud",
        "instances"
      ]
    },
    {
      "id": "official_cloudmemorystoreupdateinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreUpdateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreUpdateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "update_mask",
          "instance",
          "location",
          "instance_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "update_mask",
          "type": "Union",
          "required": false
        },
        {
          "name": "instance",
          "type": "Union",
          "required": false
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "instance_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates the metadata and configuration of a specific Redis instance.\n\n:param update_mask: Required. Mask of fields to update. At least one path must be supplied in this field.\n    The elements of the repeated paths field may only include these fields from ``Instance``:\n\n    -  ``displayName``\n    -  ``labels``\n    -  ``memorySizeGb``\n    -  ``redisConfig``\n\n    If a dict is provided, it must be of the same form as the protobuf message\n    :class:`~google.cloud.redis_v1.types.FieldMask`\n\n.. seeal",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.495447",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "instance",
        "bigquery",
        "cloudmemorystoreupdateinstanceoperator",
        "memorystore",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystorescaleinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreScaleInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreScaleInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "memory_size_gb",
          "location",
          "instance_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "memory_size_gb",
          "type": "int",
          "required": false
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "instance_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates the metadata and configuration of a specific Redis instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreScaleInstanceOperator`\n\n:param memory_size_gb: Redis memory size in GiB.\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:param instance_id: The logical name of the Redis instance in the customer project.\n:param project_id: Project ID of the project tha",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.495875",
      "relevance_keywords": [
        "scale",
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "memorystore",
        "cloud",
        "cloudmemorystorescaleinstanceoperator"
      ]
    },
    {
      "id": "official_cloudmemorystorecreateinstanceandimportoperator_20260128_160330",
      "component_name": "CloudMemorystoreCreateInstanceAndImportOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreCreateInstanceAndImportOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance_id",
          "instance",
          "input_config",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "Union",
          "required": false
        },
        {
          "name": "input_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a Redis instance and import a Redis RDB snapshot file from Cloud Storage into this instance.\n\nBy default, the instance is accessible from the project's `default network\n<https://cloud.google.com/compute/docs/networks-and-firewalls#networks>`__.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreCreateInstanceAndImportOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example eu",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.496267",
      "relevance_keywords": [
        "gcs",
        "import",
        "operator",
        "google",
        "gcp",
        "create",
        "instance",
        "cloudmemorystorecreateinstanceandimportoperator",
        "bigquery",
        "and",
        "memorystore",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystoreexportanddeleteinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreExportAndDeleteInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreExportAndDeleteInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance",
          "output_config",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "output_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Export Redis instance data into a Redis RDB format file in Cloud Storage.\n\nIn next step, deletes this instance.\n\nRedis will continue serving during this operation.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreExportAndDeleteInstanceOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:param instance: The logical name of the Redis instance in the customer proj",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.496609",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "export",
        "instance",
        "bigquery",
        "and",
        "delete",
        "memorystore",
        "cloud",
        "cloudmemorystoreexportanddeleteinstanceoperator"
      ]
    },
    {
      "id": "official_cloudmemorystorememcachedapplyparametersoperator_20260128_160330",
      "component_name": "CloudMemorystoreMemcachedApplyParametersOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreMemcachedApplyParametersOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "node_ids",
          "apply_all",
          "location",
          "instance_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "node_ids",
          "type": "Sequence[...]",
          "required": false
        },
        {
          "name": "apply_all",
          "type": "bool",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Will update current set of Parameters to the set of specified nodes of the Memcached Instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreMemcachedApplyParametersOperator`\n\n:param node_ids: Nodes to which we should apply the instance-level parameter group.\n:param apply_all: Whether to apply instance-level parameter group to all nodes. If set to true,\n    will explicitly restrict users from specifying any ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.496949",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "apply",
        "memorystore",
        "memcached",
        "parameters",
        "cloud",
        "cloudmemorystorememcachedapplyparametersoperator"
      ]
    },
    {
      "id": "official_cloudmemorystorememcachedcreateinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreMemcachedCreateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreMemcachedCreateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance_id",
          "instance",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "instance_id",
          "type": "str",
          "required": true
        },
        {
          "name": "instance",
          "type": "Union",
          "required": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a Memcached instance based on the specified tier and memory size.\n\nBy default, the instance is accessible from the project's `default network\n<https://cloud.google.com/compute/docs/networks-and-firewalls#networks>`__.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreMemcachedCreateInstanceOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:param instanc",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.497279",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "instance",
        "bigquery",
        "memorystore",
        "memcached",
        "cloudmemorystorememcachedcreateinstanceoperator",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystorememcacheddeleteinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreMemcachedDeleteInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreMemcachedDeleteInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "instance",
          "type": "str",
          "required": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a specific Memcached instance. Instance stops serving and data is deleted.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreMemcachedDeleteInstanceOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:param instance: The logical name of the Memcached instance in the customer project.\n:param project_id: Project ID of the project that contains the instance.",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.497613",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "delete",
        "memorystore",
        "memcached",
        "cloud",
        "cloudmemorystorememcacheddeleteinstanceoperator"
      ]
    },
    {
      "id": "official_cloudmemorystorememcachedgetinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreMemcachedGetInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreMemcachedGetInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "instance",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets the details of a specific Memcached instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreMemcachedGetInstanceOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n:param instance: The logical name of the Memcached instance in the customer project.\n:param project_id: Project ID of the project that contains the instance. If set\n    to None or missing, the",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.497944",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "cloudmemorystorememcachedgetinstanceoperator",
        "memorystore",
        "memcached",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystorememcachedlistinstancesoperator_20260128_160330",
      "component_name": "CloudMemorystoreMemcachedListInstancesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreMemcachedListInstancesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "List all Memcached instances owned by a project in either the specified location/region or all locations.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreMemcachedListInstancesOperator`\n\n:param location: The location of the Cloud Memorystore instance (for example europe-west1)\n    If it is specified as ``-`` (wildcard), then all regions available to the project are\n    queried, and the results are aggregated.\n",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.498269",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "memorystore",
        "memcached",
        "list",
        "cloud",
        "cloudmemorystorememcachedlistinstancesoperator",
        "instances"
      ]
    },
    {
      "id": "official_cloudmemorystorememcachedupdateinstanceoperator_20260128_160330",
      "component_name": "CloudMemorystoreMemcachedUpdateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreMemcachedUpdateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "update_mask",
          "instance",
          "location",
          "instance_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "update_mask",
          "type": "Union",
          "required": false
        },
        {
          "name": "instance",
          "type": "Union",
          "required": false
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "instance_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates the metadata and configuration of a specific Memcached instance.\n\n:param update_mask: Required. Mask of fields to update. At least one path must be supplied in this field.\n    The elements of the repeated paths field may only include these fields from ``Instance``:\n\n    -  ``displayName``\n\n    If a dict is provided, it must be of the same form as the protobuf message\n    :class:`~google.protobuf.field_mask_pb2.FieldMask`\n\n.. seealso::\n    For more information on how to use this operator,",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.498602",
      "relevance_keywords": [
        "cloudmemorystorememcachedupdateinstanceoperator",
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "instance",
        "bigquery",
        "memorystore",
        "memcached",
        "cloud"
      ]
    },
    {
      "id": "official_cloudmemorystorememcachedupdateparametersoperator_20260128_160330",
      "component_name": "CloudMemorystoreMemcachedUpdateParametersOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreMemcachedUpdateParametersOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "update_mask",
          "parameters",
          "location",
          "instance_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "update_mask",
          "type": "Union",
          "required": false
        },
        {
          "name": "parameters",
          "type": "Union",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates the defined Memcached Parameters for an existing Instance.\n\nThis method only stages the parameters, it must be followed by apply_parameters\nto apply the parameters to nodes of the Memcached Instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudMemorystoreMemcachedApplyParametersOperator`\n\n:param update_mask: Required. Mask of fields to update.\n    If a dict is provided, it must be of the same form as the prot",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.498931",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "bigquery",
        "memorystore",
        "memcached",
        "parameters",
        "cloud",
        "cloudmemorystorememcachedupdateparametersoperator"
      ]
    },
    {
      "id": "official_cloudruncreatejoboperator_20260128_160330",
      "component_name": "CloudRunCreateJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunCreateJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain",
          "job_name"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "job",
          "type": "Union",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.run_v2 import Job, Service\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_run import CloudRunHook, CloudRunServiceHook\nfrom airflow.providers.google.cloud.links.cloud_run import CloudRunJobLoggingLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a job without executing it. Pushes the created job to xcom.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param job_name: Required. The name of the job to create.\n:param job: Required. The job descriptor containing the configuration of the job to submit.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param impersonation_chain: O",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.530804",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "cloudruncreatejoboperator",
        "bigquery",
        "run",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_cloudrunupdatejoboperator_20260128_160330",
      "component_name": "CloudRunUpdateJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunUpdateJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain",
          "job_name"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "job",
          "type": "Union",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.run_v2 import Job, Service\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_run import CloudRunHook, CloudRunServiceHook\nfrom airflow.providers.google.cloud.links.cloud_run import CloudRunJobLoggingLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Updates a job and wait for the operation to be completed. Pushes the updated job to xcom.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param job_name: Required. The name of the job to update.\n:param job: Required. The job descriptor containing the new configuration of the job to update.\n    The name field will be replaced by job_name\n:param gcp_conn_id: The co",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.530950",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "bigquery",
        "run",
        "cloud",
        "cloudrunupdatejoboperator",
        "job"
      ]
    },
    {
      "id": "official_cloudrundeletejoboperator_20260128_160330",
      "component_name": "CloudRunDeleteJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunDeleteJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain",
          "job_name"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.run_v2 import Job, Service\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_run import CloudRunHook, CloudRunServiceHook\nfrom airflow.providers.google.cloud.links.cloud_run import CloudRunJobLoggingLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a job and wait for the operation to be completed. Pushes the deleted job to xcom.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param job_name: Required. The name of the job to delete.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credential",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.531060",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "run",
        "delete",
        "cloud",
        "job",
        "cloudrundeletejoboperator"
      ]
    },
    {
      "id": "official_cloudrunlistjobsoperator_20260128_160330",
      "component_name": "CloudRunListJobsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunListJobsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "show_deleted",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "limit",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.run_v2 import Job, Service\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_run import CloudRunHook, CloudRunServiceHook\nfrom airflow.providers.google.cloud.links.cloud_run import CloudRunJobLoggingLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Lists jobs.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param show_deleted: If true, returns deleted (but unexpired)\n    resources along with active ones.\n:param limit: The number of jobs to list. If left empty,\n    all the jobs will be returned.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param impersonation_chain: Optional servic",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.531168",
      "relevance_keywords": [
        "cloudrunlistjobsoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "jobs",
        "bigquery",
        "run",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_cloudrunexecutejoboperator_20260128_160330",
      "component_name": "CloudRunExecuteJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunExecuteJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain",
          "job_name",
          "overrides",
          "polling_period_seconds",
          "timeout_seconds"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "overrides",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "polling_period_seconds",
          "type": "float",
          "required": false,
          "default": 10
        },
        {
          "name": "timeout_seconds",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.run_v2 import Job, Service\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_run import CloudRunHook, CloudRunServiceHook\nfrom airflow.providers.google.cloud.links.cloud_run import CloudRunJobLoggingLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "_fail_if_execution_failed",
          "_wait_for_operation"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Executes a job and waits for the operation to be completed. Pushes the executed job to xcom.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param job_name: Required. The name of the job to execute.\n:param overrides: Optional map of override values.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param polling_period_seconds: Optional. Con",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.531278",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "run",
        "execute",
        "job",
        "cloud",
        "cloudrunexecutejoboperator"
      ]
    },
    {
      "id": "official_cloudruncreateserviceoperator_20260128_160330",
      "component_name": "CloudRunCreateServiceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunCreateServiceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain",
          "service_name"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "service_name",
          "type": "str",
          "required": true
        },
        {
          "name": "service",
          "type": "Union",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.run_v2 import Job, Service\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_run import CloudRunHook, CloudRunServiceHook\nfrom airflow.providers.google.cloud.links.cloud_run import CloudRunJobLoggingLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a Service without executing it. Pushes the created service to xcom.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param service_name: Required. The name of the service to create.\n:param service: The service descriptor containing the configuration of the service to submit.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param impe",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.531379",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "service",
        "create",
        "gcp",
        "bigquery",
        "run",
        "cloud",
        "cloudruncreateserviceoperator"
      ]
    },
    {
      "id": "official_cloudrundeleteserviceoperator_20260128_160330",
      "component_name": "CloudRunDeleteServiceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunDeleteServiceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain",
          "service_name"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "service_name",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.cloud.exceptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.cloud.run_v2 import Job, Service\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_run import CloudRunHook, CloudRunServiceHook\nfrom airflow.providers.google.cloud.links.cloud_run import CloudRunJobLoggingLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a Service without executing it. Pushes the deleted service to xcom.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param service_name: Required. The name of the service to create.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or ",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.531572",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "service",
        "gcp",
        "bigquery",
        "run",
        "delete",
        "cloud",
        "cloudrundeleteserviceoperator"
      ]
    },
    {
      "id": "official_cloudsqlbaseoperator_20260128_160330",
      "component_name": "CloudSQLBaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta4"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_validate_inputs",
          "_check_if_instance_exists",
          "_check_if_db_exists",
          "extra_links_params",
          "execute",
          "_get_settings_version"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Abstract base operator for Google Cloud SQL operators.\n\n:param instance: Cloud SQL instance ID. This does not include the project ID.\n:param project_id: Optional, Google Cloud Project ID.  f set to None or missing,\n        the default project_id from the Google Cloud connection is used.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param api_version: API version used (e.g. v1beta4).\n:param impersonation_chain: Optional service account to impersonate using short-term\n   ",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.563588",
      "relevance_keywords": [
        "cloudsqlbaseoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "base",
        "cloud"
      ]
    },
    {
      "id": "official_cloudsqlcreateinstanceoperator_20260128_160330",
      "component_name": "CloudSQLCreateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLCreateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "CloudSQLBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance",
          "body",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta4"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from CloudSQLBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "_validate_body_fields",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a new Cloud SQL instance.\n\nIf an instance with the same name exists, no action will be taken and\nthe operator will succeed.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudSQLCreateInstanceOperator`\n\n:param body: Body required by the Cloud SQL insert API, as described in\n    https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert\n    #request-body\n:param instance: Cloud SQL instance ID. This doe",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.563881",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "instance",
        "cloudsqlcreateinstanceoperator",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_cloudsqlinstancepatchoperator_20260128_160330",
      "component_name": "CloudSQLInstancePatchOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLInstancePatchOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "CloudSQLBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance",
          "body",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta4"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from CloudSQLBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update settings of a Cloud SQL instance.\n\nCaution: This is a partial update, so only included values for the settings will be\nupdated.\n\nIn the request body, supply the relevant portions of an instance resource, according\nto the rules of patch semantics.\nhttps://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudSQLInstancePatchOperator`\n\n:param body: Body r",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.564150",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "cloudsqlinstancepatchoperator",
        "cloud",
        "patch"
      ]
    },
    {
      "id": "official_cloudsqldeleteinstanceoperator_20260128_160330",
      "component_name": "CloudSQLDeleteInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLDeleteInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "CloudSQLBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from CloudSQLBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a Cloud SQL instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudSQLDeleteInstanceOperator`\n\n:param instance: Cloud SQL instance ID. This does not include the project ID.\n:param project_id: Optional, Google Cloud Project ID. If set to None or missing,\n        the default project_id from the Google Cloud connection is used.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param api_versi",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.564388",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "cloudsqldeleteinstanceoperator",
        "bigquery",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_cloudsqlcloneinstanceoperator_20260128_160330",
      "component_name": "CloudSQLCloneInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLCloneInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "CloudSQLBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance",
          "destination_instance_name",
          "gcp_conn_id",
          "api_version"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "destination_instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "clone_context",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta4"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from CloudSQLBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Clone an instance to a target instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudSQLCloneInstanceOperator`\n\n:param instance: Database instance ID to be cloned. This does not include the\n        project ID.\n:param destination_instance_name: Database instance ID to be created. This does not include the\n    project ID.\n:param clone_context: additional clone_context parameters as described in\n    https://cloud.google.",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.564631",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "cloudsqlcloneinstanceoperator",
        "clone",
        "cloud"
      ]
    },
    {
      "id": "official_cloudsqlcreateinstancedatabaseoperator_20260128_160330",
      "component_name": "CloudSQLCreateInstanceDatabaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLCreateInstanceDatabaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "CloudSQLBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance",
          "body",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta4"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from CloudSQLBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "_validate_body_fields",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a new database inside a Cloud SQL instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudSQLCreateInstanceDatabaseOperator`\n\n:param instance: Database instance ID. This does not include the project ID.\n:param body: The request body, as described in\n    https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body\n:param project_id: Optional, Google Cloud Project ID. If set to None or m",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.564874",
      "relevance_keywords": [
        "gcs",
        "operator",
        "cloudsqlcreateinstancedatabaseoperator",
        "google",
        "create",
        "instance",
        "gcp",
        "bigquery",
        "cloud",
        "database"
      ]
    },
    {
      "id": "official_cloudsqlpatchinstancedatabaseoperator_20260128_160330",
      "component_name": "CloudSQLPatchInstanceDatabaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLPatchInstanceDatabaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "CloudSQLBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance",
          "body",
          "database",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "database",
          "type": "str",
          "required": false
        },
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta4"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from CloudSQLBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "_validate_body_fields",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update resource containing information about a database using patch semantics.\n\nSee: https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudSQLPatchInstanceDatabaseOperator`\n\n:param instance: Database instance ID. This does not include the project ID.\n:param database: Name of the database to be updated in the instance.\n:param body: The request body, as ",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.565110",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "cloudsqlpatchinstancedatabaseoperator",
        "cloud",
        "database",
        "patch"
      ]
    },
    {
      "id": "official_cloudsqldeleteinstancedatabaseoperator_20260128_160330",
      "component_name": "CloudSQLDeleteInstanceDatabaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLDeleteInstanceDatabaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "CloudSQLBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance",
          "database",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "database",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta4"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from CloudSQLBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a database from a Cloud SQL instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudSQLDeleteInstanceDatabaseOperator`\n\n:param instance: Database instance ID. This does not include the project ID.\n:param database: Name of the database to be deleted in the instance.\n:param project_id: Optional, Google Cloud Project ID. If set to None or missing,\n        the default project_id from the Google Cloud connection is u",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.565339",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "delete",
        "cloud",
        "cloudsqldeleteinstancedatabaseoperator",
        "database"
      ]
    },
    {
      "id": "official_cloudsqlexportinstanceoperator_20260128_160330",
      "component_name": "CloudSQLExportInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLExportInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "CloudSQLBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance",
          "body",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta4"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from CloudSQLBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "_validate_body_fields",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Export data from a Cloud SQL instance to a Cloud Storage bucket.\n\nThe exported format can be a SQL dump or CSV file.\n\nNote: This operator is idempotent. If executed multiple times with the same\nexport file URI, the export file in GCS will simply be overridden.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudSQLExportInstanceOperator`\n\n:param instance: Cloud SQL instance ID. This does not include the project ID.\n:param bo",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.565578",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "export",
        "instance",
        "bigquery",
        "cloudsqlexportinstanceoperator",
        "cloud"
      ]
    },
    {
      "id": "official_cloudsqlimportinstanceoperator_20260128_160330",
      "component_name": "CloudSQLImportInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLImportInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "CloudSQLBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance",
          "body",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance",
          "type": "str",
          "required": false
        },
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta4"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from CloudSQLBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "_validate_body_fields",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Import data into a Cloud SQL instance from Cloud Storage.\n\nCSV IMPORT\n``````````\n\nThis operator is NOT idempotent for a CSV import. If the same file is imported\nmultiple times, the imported data will be duplicated in the database.\nMoreover, if there are any unique constraints the duplicate import may result in an\nerror.\n\nSQL IMPORT\n``````````\n\nThis operator is idempotent for a SQL import if it was also exported by Cloud SQL.\nThe exported SQL contains 'DROP TABLE IF EXISTS' statements for all tab",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.565942",
      "relevance_keywords": [
        "gcs",
        "import",
        "operator",
        "google",
        "gcp",
        "instance",
        "cloudsqlimportinstanceoperator",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_cloudsqlexecutequeryoperator_20260128_160330",
      "component_name": "CloudSQLExecuteQueryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLExecuteQueryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "sql",
          "gcp_cloudsql_conn_id",
          "gcp_conn_id",
          "ssl_server_cert",
          "ssl_client_cert",
          "ssl_client_key",
          "ssl_secret_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "sql",
          "type": "Union",
          "required": false
        },
        {
          "name": "autocommit",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "parameters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "gcp_cloudsql_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_sql_default"
        },
        {
          "name": "sql_proxy_binary_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ssl_server_cert",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ssl_client_cert",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ssl_client_key",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ssl_secret_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, conf\nfrom airflow.providers.google.cloud.hooks.cloud_sql import CloudSQLDatabaseHook, CloudSQLHook",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "cloud_sql_proxy_context",
          "execute",
          "hook",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Perform DML or DDL query on an existing Cloud Sql instance.\n\nIt optionally uses cloud-sql-proxy to establish secure connection with the\ndatabase.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudSQLExecuteQueryOperator`\n\n:param sql: SQL query or list of queries to run (should be DML or DDL query -\n    this operator does not return any data from the database,\n    so it is useless to pass it DQL queries. Note that it is res",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.566353",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "cloudsqlexecutequeryoperator",
        "execute",
        "cloud",
        "query"
      ]
    },
    {
      "id": "official_clouddatatransferservicecreatejoboperator_20260128_160330",
      "component_name": "CloudDataTransferServiceCreateJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceCreateJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "body",
          "gcp_conn_id",
          "aws_conn_id",
          "google_impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a transfer job that runs periodically.\n\n.. warning::\n\n    This operator is NOT idempotent in the following cases:\n\n    * `name` is not passed in body param\n    * transfer job `name` has been soft deleted. In this case,\n      each new task will receive a unique suffix\n\n    If you run it many times, many transfer jobs will be created in the Google Cloud.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferS",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.606979",
      "relevance_keywords": [
        "gcs",
        "clouddatatransferservicecreatejoboperator",
        "operator",
        "google",
        "service",
        "create",
        "gcp",
        "bigquery",
        "data",
        "transfer",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddatatransferserviceupdatejoboperator_20260128_160330",
      "component_name": "CloudDataTransferServiceUpdateJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceUpdateJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "job_name",
          "body",
          "gcp_conn_id",
          "aws_conn_id",
          "google_impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Updates a transfer job that runs periodically.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferServiceUpdateJobOperator`\n\n:param job_name: (Required) Name of the job to be updated\n:param body: (Required) The request body, as described in\n    https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n    With three additional improvements:\n\n    * dates can be given in the ",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.607254",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "service",
        "update",
        "gcp",
        "bigquery",
        "data",
        "transfer",
        "cloud",
        "clouddatatransferserviceupdatejoboperator",
        "job"
      ]
    },
    {
      "id": "official_clouddatatransferservicedeletejoboperator_20260128_160330",
      "component_name": "CloudDataTransferServiceDeleteJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceDeleteJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "job_name",
          "project_id",
          "gcp_conn_id",
          "api_version",
          "google_impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a transfer job.\n\nThis is a soft delete. After a transfer job is deleted, the job and all the transfer\nexecutions are subject to garbage collection. Transfer jobs become eligible for garbage\ncollection 30 days after soft delete.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferServiceDeleteJobOperator`\n\n:param job_name: (Required) Name of the TRANSFER operation\n:param project_id: (Optional) the ID of the",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.607482",
      "relevance_keywords": [
        "clouddatatransferservicedeletejoboperator",
        "gcs",
        "operator",
        "google",
        "service",
        "gcp",
        "bigquery",
        "data",
        "delete",
        "transfer",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddatatransferservicerunjoboperator_20260128_160330",
      "component_name": "CloudDataTransferServiceRunJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceRunJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "job_name",
          "project_id",
          "gcp_conn_id",
          "api_version",
          "google_impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Runs a transfer job.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferServiceRunJobOperator`\n\n:param job_name: (Required) Name of the job to be run\n:param project_id: (Optional) the ID of the project that owns the Transfer\n    Job. If set to None or missing, the default project_id from the Google Cloud\n    connection is used.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param api_ver",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.607709",
      "relevance_keywords": [
        "clouddatatransferservicerunjoboperator",
        "gcs",
        "operator",
        "google",
        "service",
        "gcp",
        "bigquery",
        "data",
        "run",
        "transfer",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddatatransferservicegetoperationoperator_20260128_160330",
      "component_name": "CloudDataTransferServiceGetOperationOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceGetOperationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "operation_name",
          "gcp_conn_id",
          "google_impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "operation_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Gets the latest state of a long-running operation in Google Storage Transfer Service.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferServiceGetOperationOperator`\n\n:param operation_name: (Required) Name of the transfer operation.\n:param gcp_conn_id: The connection ID used to connect to Google\n    Cloud Platform.\n:param api_version: API version used (e.g. v1).\n:param google_impersonation_chain: Optional Google",
      "success_score": 194,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.607956",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "service",
        "clouddatatransferservicegetoperationoperator",
        "operation",
        "gcp",
        "bigquery",
        "data",
        "transfer",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatatransferservicelistoperationsoperator_20260128_160330",
      "component_name": "CloudDataTransferServiceListOperationsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceListOperationsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "request_filter",
          "gcp_conn_id",
          "google_impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "request_filter",
          "type": "dict",
          "required": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "filter",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Lists long-running operations in Google Storage Transfer Service that match the specified filter.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferServiceListOperationsOperator`\n\n:param request_filter: (Required) A request filter, as described in\n        https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter\n:param gcp_conn_id: The connection ID used to ",
      "success_score": 194,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.608225",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "service",
        "gcp",
        "bigquery",
        "data",
        "transfer",
        "list",
        "cloud",
        "clouddatatransferservicelistoperationsoperator",
        "operations"
      ]
    },
    {
      "id": "official_clouddatatransferservicepauseoperationoperator_20260128_160330",
      "component_name": "CloudDataTransferServicePauseOperationOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServicePauseOperationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "operation_name",
          "gcp_conn_id",
          "api_version",
          "google_impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "operation_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Pauses a transfer operation in Google Storage Transfer Service.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferServicePauseOperationOperator`\n\n:param operation_name: (Required) Name of the transfer operation.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param api_version:  API version used (e.g. v1).\n:param google_impersonation_chain: Optional Google service account to impersonate ",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.608454",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "service",
        "operation",
        "gcp",
        "clouddatatransferservicepauseoperationoperator",
        "bigquery",
        "data",
        "transfer",
        "pause",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatatransferserviceresumeoperationoperator_20260128_160330",
      "component_name": "CloudDataTransferServiceResumeOperationOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceResumeOperationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "operation_name",
          "gcp_conn_id",
          "api_version",
          "google_impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "operation_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Resumes a transfer operation in Google Storage Transfer Service.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferServiceResumeOperationOperator`\n\n:param operation_name: (Required) Name of the transfer operation.\n:param gcp_conn_id: The connection ID used to connect to Google Cloud.\n:param api_version: API version used (e.g. v1).\n:param google_impersonation_chain: Optional Google service account to impersonate",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.608660",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "service",
        "operation",
        "gcp",
        "bigquery",
        "data",
        "resume",
        "transfer",
        "cloud",
        "clouddatatransferserviceresumeoperationoperator"
      ]
    },
    {
      "id": "official_clouddatatransferservicecanceloperationoperator_20260128_160330",
      "component_name": "CloudDataTransferServiceCancelOperationOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceCancelOperationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "operation_name",
          "gcp_conn_id",
          "api_version",
          "google_impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "operation_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Cancels a transfer operation in Google Storage Transfer Service.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferServiceCancelOperationOperator`\n\n:param operation_name: (Required) Name of the transfer operation.\n:param api_version: API version used (e.g. v1).\n:param gcp_conn_id: The connection ID used to connect to Google\n    Cloud Platform.\n:param google_impersonation_chain: Optional Google service account t",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.609035",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "service",
        "operation",
        "gcp",
        "cancel",
        "bigquery",
        "data",
        "transfer",
        "cloud",
        "clouddatatransferservicecanceloperationoperator"
      ]
    },
    {
      "id": "official_clouddatatransferservices3togcsoperator_20260128_160330",
      "component_name": "CloudDataTransferServiceS3ToGCSOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceS3ToGCSOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "gcp_conn_id",
          "s3_bucket",
          "gcs_bucket",
          "s3_path",
          "gcs_path",
          "description",
          "object_conditions",
          "google_impersonation_chain",
          "aws_role_arn"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "s3_bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "gcs_bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "s3_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcs_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "aws_conn_id",
          "type": "Union",
          "required": false,
          "default": "aws_default"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "description",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "schedule",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "object_conditions",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "transfer_options",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "delete_job_after_completion",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute",
          "execute_complete",
          "_create_body",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Sync an S3 bucket with a Google Cloud Storage bucket using the Google Cloud Storage Transfer Service.\n\n.. warning::\n\n    This operator is NOT idempotent. If you run it many times, many transfer\n    jobs will be created in the Google Cloud.\n\n**Example**:\n\n.. code-block:: python\n\n   s3_to_gcs_transfer_op = S3ToGoogleCloudStorageTransferOperator(\n       task_id=\"s3_to_gcs_transfer_example\",\n       s3_bucket=\"my-s3-bucket\",\n       project_id=\"my-gcp-project\",\n       gcs_bucket=\"my-gcs-bucket\",\n     ",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.609606",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "clouddatatransferservices3togcsoperator",
        "service",
        "gcp",
        "bigquery",
        "data",
        "to",
        "transfer",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatatransferservicegcstogcsoperator_20260128_160330",
      "component_name": "CloudDataTransferServiceGCSToGCSOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceGCSToGCSOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "gcp_conn_id",
          "source_bucket",
          "destination_bucket",
          "source_path",
          "destination_path",
          "description",
          "object_conditions",
          "google_impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source_bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "destination_bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "source_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "destination_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "description",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "schedule",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "object_conditions",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "transfer_options",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "google_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "delete_job_after_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom datetime import date, time, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute",
          "execute_complete",
          "_create_body"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Copies objects from a bucket to another using the Google Cloud Storage Transfer Service.\n\n.. warning::\n\n    This operator is NOT idempotent. If you run it many times, many transfer\n    jobs will be created in the Google Cloud.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferServiceGCSToGCSOperator`\n\n**Example**:\n\n.. code-block:: python\n\n   gcs_to_gcs_transfer_op = GoogleCloudStorageToGoogleCloudStorageTransfe",
      "success_score": 200,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.609914",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "service",
        "gcp",
        "bigquery",
        "clouddatatransferservicegcstogcsoperator",
        "data",
        "to",
        "transfer",
        "cloud"
      ]
    },
    {
      "id": "official_computeenginebaseoperator_20260128_160330",
      "component_name": "ComputeEngineBaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "zone",
          "type": "str",
          "required": false
        },
        {
          "name": "resource_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_validate_inputs",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Abstract base operator for Google Compute Engine operators to inherit from.",
      "success_score": 150,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.644027",
      "relevance_keywords": [
        "gcs",
        "operator",
        "compute",
        "google",
        "gcp",
        "bigquery",
        "engine",
        "base",
        "computeenginebaseoperator",
        "cloud"
      ]
    },
    {
      "id": "official_computeengineinsertinstanceoperator_20260128_160330",
      "component_name": "ComputeEngineInsertInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineInsertInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "body",
          "project_id",
          "zone",
          "request_id",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain",
          "resource_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "zone",
          "type": "str",
          "required": false
        },
        {
          "name": "resource_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "check_body_fields",
          "_validate_inputs",
          "_validate_all_body_fields",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates an Instance in Google Compute Engine based on specified parameters.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineInsertInstanceOperator`\n\n:param body: Instance representation as an object. Should at least include 'name', 'machine_type',\n    'disks' and 'network_interfaces' fields but doesn't include 'zone' field, as it will be specified\n    in 'zone' parameter.\n    Full or partial URL and can be repres",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.644784",
      "relevance_keywords": [
        "gcs",
        "operator",
        "compute",
        "google",
        "insert",
        "instance",
        "gcp",
        "bigquery",
        "engine",
        "cloud",
        "computeengineinsertinstanceoperator"
      ]
    },
    {
      "id": "official_computeengineinsertinstancefromtemplateoperator_20260128_160330",
      "component_name": "ComputeEngineInsertInstanceFromTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineInsertInstanceFromTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "body",
          "source_instance_template",
          "project_id",
          "zone",
          "request_id",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain",
          "resource_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source_instance_template",
          "type": "str",
          "required": false
        },
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "zone",
          "type": "str",
          "required": false
        },
        {
          "name": "resource_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_all_body_fields",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates an Instance in Google Compute Engine based on specified parameters from existing Template.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineInsertInstanceFromTemplateOperator`\n\n:param body: Instance representation as object. For this Operator only 'name' parameter is required for\n    creating new Instance since all other parameters will be passed through the Template.\n:param source_instance_template: Exist",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.645487",
      "relevance_keywords": [
        "gcs",
        "operator",
        "compute",
        "google",
        "insert",
        "instance",
        "template",
        "gcp",
        "bigquery",
        "engine",
        "from",
        "cloud",
        "computeengineinsertinstancefromtemplateoperator"
      ]
    },
    {
      "id": "official_computeenginedeleteinstanceoperator_20260128_160330",
      "component_name": "ComputeEngineDeleteInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineDeleteInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "zone",
          "resource_id",
          "request_id",
          "project_id",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "resource_id",
          "type": "str",
          "required": false
        },
        {
          "name": "zone",
          "type": "str",
          "required": false
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes an Instance in Google Compute Engine.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineDeleteInstanceOperator`\n\n:param project_id: Google Cloud project ID where the Compute Engine Instance exists.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param zone: Google Cloud zone where the instance exists.\n:param resource_id: Name of the Instance.\n:param request_i",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.646363",
      "relevance_keywords": [
        "gcs",
        "computeenginedeleteinstanceoperator",
        "compute",
        "operator",
        "google",
        "instance",
        "gcp",
        "bigquery",
        "delete",
        "engine",
        "cloud"
      ]
    },
    {
      "id": "official_computeenginestartinstanceoperator_20260128_160330",
      "component_name": "ComputeEngineStartInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineStartInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "zone",
          "resource_id",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Starts an instance in Google Compute Engine.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineStartInstanceOperator`\n\n:param zone: Google Cloud zone where the instance exists.\n:param resource_id: Name of the Compute Engine instance resource.\n:param project_id: Optional, Google Cloud Project ID where the Compute\n    Engine Instance exists. If set to None or missing, the default project_id from the Google Cloud\n    ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.647241",
      "relevance_keywords": [
        "gcs",
        "operator",
        "compute",
        "google",
        "start",
        "instance",
        "gcp",
        "bigquery",
        "engine",
        "cloud",
        "computeenginestartinstanceoperator"
      ]
    },
    {
      "id": "official_computeenginestopinstanceoperator_20260128_160330",
      "component_name": "ComputeEngineStopInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineStopInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "zone",
          "resource_id",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Stops an instance in Google Compute Engine.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineStopInstanceOperator`\n\n:param zone: Google Cloud zone where the instance exists.\n:param resource_id: Name of the Compute Engine instance resource.\n:param project_id: Optional, Google Cloud Project ID where the Compute\n    Engine Instance exists. If set to None or missing, the default project_id from the Google Cloud\n    co",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.647818",
      "relevance_keywords": [
        "gcs",
        "operator",
        "compute",
        "google",
        "stop",
        "instance",
        "gcp",
        "bigquery",
        "engine",
        "cloud",
        "computeenginestopinstanceoperator"
      ]
    },
    {
      "id": "official_computeenginesetmachinetypeoperator_20260128_160330",
      "component_name": "ComputeEngineSetMachineTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineSetMachineTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "zone",
          "resource_id",
          "body",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "zone",
          "type": "str",
          "required": false
        },
        {
          "name": "resource_id",
          "type": "str",
          "required": false
        },
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_all_body_fields",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Changes the machine type for a stopped instance to the machine type specified in the request.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineSetMachineTypeOperator`\n\n:param zone: Google Cloud zone where the instance exists.\n:param resource_id: Name of the Compute Engine instance resource.\n:param body: Body required by the Compute Engine setMachineType API, as described in\n    https://cloud.google.com/compute/doc",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.648979",
      "relevance_keywords": [
        "machine",
        "gcs",
        "operator",
        "compute",
        "computeenginesetmachinetypeoperator",
        "google",
        "set",
        "gcp",
        "bigquery",
        "engine",
        "type",
        "cloud"
      ]
    },
    {
      "id": "official_computeengineinsertinstancetemplateoperator_20260128_160330",
      "component_name": "ComputeEngineInsertInstanceTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineInsertInstanceTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "body",
          "project_id",
          "request_id",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain",
          "resource_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "resource_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "check_body_fields",
          "_validate_all_body_fields",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates an Instance Template using specified fields.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineInsertInstanceTemplateOperator`\n\n:param body: Instance template representation as object.\n:param project_id: Google Cloud project ID where the Compute Engine Instance exists.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param request_id: Unique request_id that yo",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.649873",
      "relevance_keywords": [
        "gcs",
        "operator",
        "compute",
        "computeengineinsertinstancetemplateoperator",
        "google",
        "insert",
        "instance",
        "template",
        "gcp",
        "bigquery",
        "engine",
        "cloud"
      ]
    },
    {
      "id": "official_computeenginedeleteinstancetemplateoperator_20260128_160330",
      "component_name": "ComputeEngineDeleteInstanceTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineDeleteInstanceTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "resource_id",
          "request_id",
          "project_id",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "resource_id",
          "type": "str",
          "required": false
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes an Instance Template in Google Compute Engine.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineDeleteInstanceTemplateOperator`\n\n:param resource_id: Name of the Instance Template.\n:param project_id: Google Cloud project ID where the Compute Engine Instance exists.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param request_id: Unique request_id that you mi",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.650609",
      "relevance_keywords": [
        "gcs",
        "operator",
        "compute",
        "google",
        "template",
        "instance",
        "gcp",
        "bigquery",
        "delete",
        "engine",
        "computeenginedeleteinstancetemplateoperator",
        "cloud"
      ]
    },
    {
      "id": "official_computeenginecopyinstancetemplateoperator_20260128_160330",
      "component_name": "ComputeEngineCopyInstanceTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineCopyInstanceTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "resource_id",
          "request_id",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "resource_id",
          "type": "str",
          "required": false
        },
        {
          "name": "body_patch",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_all_body_fields",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Copies the instance template, applying specified changes.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineCopyInstanceTemplateOperator`\n\n:param resource_id: Name of the Instance Template\n:param body_patch: Patch to the body of instanceTemplates object following rfc7386\n    PATCH semantics. The body_patch content follows\n    https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates\n    Name field i",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.651315",
      "relevance_keywords": [
        "computeenginecopyinstancetemplateoperator",
        "gcs",
        "operator",
        "compute",
        "google",
        "template",
        "instance",
        "gcp",
        "bigquery",
        "engine",
        "copy",
        "cloud"
      ]
    },
    {
      "id": "official_computeengineinstancegroupupdatemanagertemplateoperator_20260128_160330",
      "component_name": "ComputeEngineInstanceGroupUpdateManagerTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineInstanceGroupUpdateManagerTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "resource_id",
          "zone",
          "request_id",
          "source_template",
          "destination_template",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "resource_id",
          "type": "str",
          "required": false
        },
        {
          "name": "zone",
          "type": "str",
          "required": false
        },
        {
          "name": "source_template",
          "type": "str",
          "required": false
        },
        {
          "name": "destination_template",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "update_policy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "Any",
          "required": false,
          "default": "beta"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "_possibly_replace_template",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Patches the Instance Group Manager, replacing source template URL with the destination one.\n\nAPI V1 does not have update/patch operations for Instance Group Manager,\nso you must use beta or newer API version. Beta is the default.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineInstanceGroupUpdateManagerTemplateOperator`\n\n:param resource_id: Name of the Instance Group Manager\n:param zone: Google Cloud zone where t",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.652013",
      "relevance_keywords": [
        "manager",
        "gcs",
        "operator",
        "compute",
        "google",
        "update",
        "instance",
        "template",
        "gcp",
        "computeengineinstancegroupupdatemanagertemplateoperator",
        "bigquery",
        "engine",
        "group",
        "cloud"
      ]
    },
    {
      "id": "official_computeengineinsertinstancegroupmanageroperator_20260128_160330",
      "component_name": "ComputeEngineInsertInstanceGroupManagerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineInsertInstanceGroupManagerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "body",
          "zone",
          "request_id",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain",
          "resource_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "zone",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "resource_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "Any",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "check_body_fields",
          "_validate_all_body_fields",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates an Instance Group Managers using the body specified.\n\nAfter the group is created, instances in the group are created using the specified Instance Template.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineInsertInstanceGroupManagerOperator`\n\n:param body: Instance Group Managers representation as object.\n:param project_id: Google Cloud project ID where the Compute Engine Instance Group Managers exists.\n    ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.652675",
      "relevance_keywords": [
        "manager",
        "gcs",
        "operator",
        "compute",
        "google",
        "insert",
        "instance",
        "gcp",
        "bigquery",
        "engine",
        "group",
        "cloud",
        "computeengineinsertinstancegroupmanageroperator"
      ]
    },
    {
      "id": "official_computeenginedeleteinstancegroupmanageroperator_20260128_160330",
      "component_name": "ComputeEngineDeleteInstanceGroupManagerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineDeleteInstanceGroupManagerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ComputeEngineBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "resource_id",
          "zone",
          "request_id",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "resource_id",
          "type": "str",
          "required": false
        },
        {
          "name": "zone",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "Any",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from ComputeEngineBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core import exceptions\nfrom google.cloud.compute_v1.types import Instance, InstanceGroupManager, InstanceTemplate\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google._vendor.json_merge_patch import merge",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Permanently and irrevocably deletes an Instance Group Managers.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:ComputeEngineDeleteInstanceGroupManagerOperator`\n\n:param resource_id: Name of the Instance Group Managers.\n:param project_id: Google Cloud project ID where the Compute Engine Instance Group Managers exists.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param request",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.653325",
      "relevance_keywords": [
        "manager",
        "gcs",
        "operator",
        "compute",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "delete",
        "computeenginedeleteinstancegroupmanageroperator",
        "engine",
        "group",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatacatalogcreateentryoperator_20260128_160330",
      "component_name": "CloudDataCatalogCreateEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogCreateEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "entry_group",
          "entry_id",
          "entry",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_group",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_id",
          "type": "str",
          "required": false
        },
        {
          "name": "entry",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates an entry.\n\nCurrently only entries of 'FILESET' type can be created.\n\nThe newly created entry ID are saved under the ``entry_id`` key in XCOM.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogCreateEntryOperator`\n\n:param location: Required. The location of the entry to create.\n:param entry_group: Required. Entry group ID under which the entry is created.\n:param entry_id: Required. The id of the entry to ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.703670",
      "relevance_keywords": [
        "gcs",
        "clouddatacatalogcreateentryoperator",
        "operator",
        "google",
        "create",
        "entry",
        "gcp",
        "bigquery",
        "data",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogcreateentrygroupoperator_20260128_160330",
      "component_name": "CloudDataCatalogCreateEntryGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogCreateEntryGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "entry_group_id",
          "entry_group",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_group_id",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_group",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates an EntryGroup.\n\nThe newly created entry group ID are saved under the ``entry_group_id`` key in XCOM.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogCreateEntryGroupOperator`\n\n:param location: Required. The location of the entry group to create.\n:param entry_group_id: Required. The id of the entry group to create. The id must begin with a letter\n    or underscore, contain only English letters, numbers ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.704611",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "entry",
        "create",
        "gcp",
        "bigquery",
        "data",
        "group",
        "cloud",
        "clouddatacatalogcreateentrygroupoperator",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogcreatetagoperator_20260128_160330",
      "component_name": "CloudDataCatalogCreateTagOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogCreateTagOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "entry_group",
          "entry",
          "tag",
          "template_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_group",
          "type": "str",
          "required": false
        },
        {
          "name": "entry",
          "type": "str",
          "required": false
        },
        {
          "name": "tag",
          "type": "Union",
          "required": false
        },
        {
          "name": "template_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a tag on an entry.\n\nThe newly created tag ID are saved under the ``tag_id`` key in XCOM.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogCreateTagOperator`\n\n:param location: Required. The location of the tag to create.\n:param entry_group: Required. Entry group ID under which the tag is created.\n:param entry: Required. Entry group ID under which the tag is created.\n:param tag: Required. The tag to creat",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.705419",
      "relevance_keywords": [
        "tag",
        "gcs",
        "catalog",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "data",
        "cloud",
        "clouddatacatalogcreatetagoperator"
      ]
    },
    {
      "id": "official_clouddatacatalogcreatetagtemplateoperator_20260128_160330",
      "component_name": "CloudDataCatalogCreateTagTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogCreateTagTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "tag_template_id",
          "tag_template",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "tag_template_id",
          "type": "str",
          "required": false
        },
        {
          "name": "tag_template",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a tag template.\n\nThe newly created tag template are saved under the ``tag_template_id`` key in XCOM.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogCreateTagTemplateOperator`\n\n:param location: Required. The location of the tag template to create.\n:param tag_template_id: Required. The id of the tag template to create.\n:param tag_template: Required. The tag template to create.\n\n    If a dict is provided",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.706226",
      "relevance_keywords": [
        "clouddatacatalogcreatetagtemplateoperator",
        "tag",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "template",
        "bigquery",
        "data",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogcreatetagtemplatefieldoperator_20260128_160330",
      "component_name": "CloudDataCatalogCreateTagTemplateFieldOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogCreateTagTemplateFieldOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "tag_template",
          "tag_template_field_id",
          "tag_template_field",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "tag_template",
          "type": "str",
          "required": false
        },
        {
          "name": "tag_template_field_id",
          "type": "str",
          "required": false
        },
        {
          "name": "tag_template_field",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a field in a tag template.\n\nThe newly created tag template field are saved under the ``tag_template_field_id`` key in XCOM.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogCreateTagTemplateFieldOperator`\n\n:param location: Required. The location of the tag template field to create.\n:param tag_template: Required. The id of the tag template to create.\n:param tag_template_field_id: Required. The ID of the ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.707026",
      "relevance_keywords": [
        "tag",
        "gcs",
        "operator",
        "google",
        "clouddatacatalogcreatetagtemplatefieldoperator",
        "create",
        "template",
        "gcp",
        "bigquery",
        "data",
        "cloud",
        "field",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogdeleteentryoperator_20260128_160330",
      "component_name": "CloudDataCatalogDeleteEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogDeleteEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "entry_group",
          "entry",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_group",
          "type": "str",
          "required": false
        },
        {
          "name": "entry",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes an existing entry.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogDeleteEntryOperator`\n\n:param location: Required. The location of the entry to delete.\n:param entry_group: Required. Entry group ID for entries that is deleted.\n:param entry: Entry ID that is deleted.\n:param project_id: The ID of the Google Cloud project that owns the entry group.\n    If set to ``None`` or missing, the default project_id",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.707796",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "entry",
        "gcp",
        "clouddatacatalogdeleteentryoperator",
        "bigquery",
        "data",
        "delete",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogdeleteentrygroupoperator_20260128_160330",
      "component_name": "CloudDataCatalogDeleteEntryGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogDeleteEntryGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "entry_group",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_group",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes an EntryGroup.\n\nOnly entry groups that do not contain entries can be deleted.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogDeleteEntryGroupOperator`\n\n:param location: Required. The location of the entry group to delete.\n:param entry_group: Entry group ID that is deleted.\n:param project_id: The ID of the Google Cloud project that owns the entry group.\n    If set to ``None`` or missing, the default pr",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.708689",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "entry",
        "gcp",
        "bigquery",
        "data",
        "delete",
        "group",
        "cloud",
        "catalog",
        "clouddatacatalogdeleteentrygroupoperator"
      ]
    },
    {
      "id": "official_clouddatacatalogdeletetagoperator_20260128_160330",
      "component_name": "CloudDataCatalogDeleteTagOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogDeleteTagOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "entry_group",
          "entry",
          "tag",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_group",
          "type": "str",
          "required": false
        },
        {
          "name": "entry",
          "type": "str",
          "required": false
        },
        {
          "name": "tag",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a tag.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogDeleteTagOperator`\n\n:param location: Required. The location of the tag to delete.\n:param entry_group: Entry group ID for tag that is deleted.\n:param entry: Entry  ID for tag that is deleted.\n:param tag: Identifier for TAG that is deleted.\n:param project_id: The ID of the Google Cloud project that owns the entry group.\n    If set to ``None`` or miss",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.709491",
      "relevance_keywords": [
        "tag",
        "clouddatacatalogdeletetagoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "data",
        "delete",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogdeletetagtemplateoperator_20260128_160330",
      "component_name": "CloudDataCatalogDeleteTagTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogDeleteTagTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "tag_template",
          "force",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "tag_template",
          "type": "str",
          "required": false
        },
        {
          "name": "force",
          "type": "bool",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a tag template and all tags using the template.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogDeleteTagTemplateOperator`\n\n:param location: Required. The location of the tag template to delete.\n:param tag_template: ID for tag template that is deleted.\n:param project_id: The ID of the Google Cloud project that owns the entry group.\n    If set to ``None`` or missing, the default project_id from the Goog",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.710318",
      "relevance_keywords": [
        "tag",
        "gcs",
        "operator",
        "google",
        "gcp",
        "template",
        "bigquery",
        "data",
        "delete",
        "cloud",
        "clouddatacatalogdeletetagtemplateoperator",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogdeletetagtemplatefieldoperator_20260128_160330",
      "component_name": "CloudDataCatalogDeleteTagTemplateFieldOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogDeleteTagTemplateFieldOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "tag_template",
          "field",
          "force",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "tag_template",
          "type": "str",
          "required": false
        },
        {
          "name": "field",
          "type": "str",
          "required": false
        },
        {
          "name": "force",
          "type": "bool",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a field in a tag template and all uses of that field.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogDeleteTagTemplateFieldOperator`\n\n:param location: Required. The location of the tag template to delete.\n:param tag_template: Tag Template ID for tag template field that is deleted.\n:param field: Name of field that is deleted.\n:param force: Required. This confirms the deletion of this field from any tag",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.711170",
      "relevance_keywords": [
        "tag",
        "gcs",
        "operator",
        "google",
        "gcp",
        "template",
        "bigquery",
        "data",
        "delete",
        "clouddatacatalogdeletetagtemplatefieldoperator",
        "cloud",
        "field",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacataloggetentryoperator_20260128_160330",
      "component_name": "CloudDataCatalogGetEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogGetEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "entry_group",
          "entry",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_group",
          "type": "str",
          "required": false
        },
        {
          "name": "entry",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets an entry.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogGetEntryOperator`\n\n:param location: Required. The location of the entry to get.\n:param entry_group: Required. The entry group of the entry to get.\n:param entry: The ID of the entry to get.\n:param project_id: The ID of the Google Cloud project that owns the entry group.\n    If set to ``None`` or missing, the default project_id from the Google Cloud ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.711709",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "entry",
        "gcp",
        "bigquery",
        "data",
        "get",
        "cloud",
        "clouddatacataloggetentryoperator",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacataloggetentrygroupoperator_20260128_160330",
      "component_name": "CloudDataCatalogGetEntryGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogGetEntryGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "entry_group",
          "read_mask",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_group",
          "type": "str",
          "required": false
        },
        {
          "name": "read_mask",
          "type": "FieldMask",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets an entry group.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogGetEntryGroupOperator`\n\n:param location: Required. The location of the entry group to get.\n:param entry_group: The ID of the entry group to get.\n:param read_mask: The fields to return. If not set or empty, all fields are returned.\n\n    If a dict is provided, it must be of the same form as the protobuf message\n    :class:`~google.protobuf.fiel",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.712208",
      "relevance_keywords": [
        "gcs",
        "catalog",
        "operator",
        "google",
        "entry",
        "gcp",
        "bigquery",
        "data",
        "get",
        "group",
        "cloud",
        "clouddatacataloggetentrygroupoperator"
      ]
    },
    {
      "id": "official_clouddatacataloggettagtemplateoperator_20260128_160330",
      "component_name": "CloudDataCatalogGetTagTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogGetTagTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "tag_template",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "tag_template",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets a tag template.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogGetTagTemplateOperator`\n\n:param location: Required. The location of the tag template to get.\n:param tag_template: Required. The ID of the tag template to get.\n:param project_id: The ID of the Google Cloud project that owns the entry group.\n    If set to ``None`` or missing, the default project_id from the Google Cloud connection is used.\n:par",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.712969",
      "relevance_keywords": [
        "tag",
        "clouddatacataloggettagtemplateoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "template",
        "bigquery",
        "data",
        "get",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacataloglisttagsoperator_20260128_160330",
      "component_name": "CloudDataCatalogListTagsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogListTagsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "entry_group",
          "entry",
          "page_size",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "entry_group",
          "type": "str",
          "required": false
        },
        {
          "name": "entry",
          "type": "str",
          "required": false
        },
        {
          "name": "page_size",
          "type": "int",
          "required": false,
          "default": 100
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists the tags on an Entry.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogListTagsOperator`\n\n:param location: Required. The location of the tags to get.\n:param entry_group: Required. The entry group of the tags to get.\n:param entry: Required. The entry of the tags to get.\n:param page_size: The maximum number of resources contained in the underlying API response. If page\n    streaming is performed per- resour",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.713479",
      "relevance_keywords": [
        "tags",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "data",
        "clouddatacataloglisttagsoperator",
        "list",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacataloglookupentryoperator_20260128_160330",
      "component_name": "CloudDataCatalogLookupEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogLookupEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "linked_resource",
          "sql_resource",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "linked_resource",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sql_resource",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get an entry by target resource name.\n\nThis method allows clients to use the resource name from the source Google Cloud service\nto get the Data Catalog Entry.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogLookupEntryOperator`\n\n:param linked_resource: The full name of the Google Cloud resource the Data Catalog entry\n    represents. See: https://cloud.google.com/apis/design/resource\\_names#full\\_resource\\_name",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.713969",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "entry",
        "gcp",
        "clouddatacataloglookupentryoperator",
        "bigquery",
        "data",
        "cloud",
        "lookup",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogrenametagtemplatefieldoperator_20260128_160330",
      "component_name": "CloudDataCatalogRenameTagTemplateFieldOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogRenameTagTemplateFieldOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "tag_template",
          "field",
          "new_tag_template_field_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "tag_template",
          "type": "str",
          "required": false
        },
        {
          "name": "field",
          "type": "str",
          "required": false
        },
        {
          "name": "new_tag_template_field_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Renames a field in a tag template.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogRenameTagTemplateFieldOperator`\n\n:param location: Required. The location of the tag template field to rename.\n:param tag_template: The tag template ID for field that is renamed.\n:param field: Required. The old ID of this tag template field. For example,\n    ``my_old_field``.\n:param new_tag_template_field_id: Required. The new ID",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.714455",
      "relevance_keywords": [
        "tag",
        "gcs",
        "operator",
        "google",
        "gcp",
        "template",
        "bigquery",
        "data",
        "clouddatacatalogrenametagtemplatefieldoperator",
        "cloud",
        "field",
        "rename",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogsearchcatalogoperator_20260128_160330",
      "component_name": "CloudDataCatalogSearchCatalogOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogSearchCatalogOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "scope",
          "query",
          "page_size",
          "order_by",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "scope",
          "type": "Union",
          "required": false
        },
        {
          "name": "query",
          "type": "str",
          "required": false
        },
        {
          "name": "page_size",
          "type": "int",
          "required": false,
          "default": 100
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Searches Data Catalog for multiple resources like entries, tags that match a query.\n\nThis does not return the complete resource, only the resource identifier and high level fields.\nClients can subsequently call ``Get`` methods.\n\nNote that searches do not have full recall. There may be results that match your query but are not\nreturned, even in subsequent pages of results. These missing results may vary across repeated calls to\nsearch. Do not rely on this method if you need to guarantee full reca",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.715280",
      "relevance_keywords": [
        "gcs",
        "catalog",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "data",
        "search",
        "cloud",
        "clouddatacatalogsearchcatalogoperator"
      ]
    },
    {
      "id": "official_clouddatacatalogupdateentryoperator_20260128_160330",
      "component_name": "CloudDataCatalogUpdateEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogUpdateEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "entry",
          "update_mask",
          "location",
          "entry_group",
          "entry_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry",
          "type": "Union",
          "required": false
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "entry_group",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "entry_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates an existing entry.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogUpdateEntryOperator`\n\n:param entry: Required. The updated entry. The \"name\" field must be set.\n\n    If a dict is provided, it must be of the same form as the protobuf message\n    :class:`~google.cloud.datacatalog_v1beta1.types.Entry`\n:param update_mask: The fields to update on the entry. If absent or empty, all modifiable fields are\n   ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.716124",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "entry",
        "gcp",
        "clouddatacatalogupdateentryoperator",
        "bigquery",
        "data",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogupdatetagoperator_20260128_160330",
      "component_name": "CloudDataCatalogUpdateTagOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogUpdateTagOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "tag",
          "update_mask",
          "location",
          "entry_group",
          "entry",
          "tag_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "tag",
          "type": "Union",
          "required": false
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "entry_group",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "entry",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tag_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates an existing tag.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogUpdateTagOperator`\n\n:param tag: Required. The updated tag. The \"name\" field must be set.\n\n    If a dict is provided, it must be of the same form as the protobuf message\n    :class:`~google.cloud.datacatalog_v1beta1.types.Tag`\n:param update_mask: The fields to update on the Tag. If absent or empty, all modifiable fields are\n    updated. Cu",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.716973",
      "relevance_keywords": [
        "tag",
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "bigquery",
        "data",
        "cloud",
        "clouddatacatalogupdatetagoperator",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogupdatetagtemplateoperator_20260128_160330",
      "component_name": "CloudDataCatalogUpdateTagTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogUpdateTagTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "tag_template",
          "update_mask",
          "location",
          "tag_template_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "tag_template",
          "type": "Union",
          "required": false
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tag_template_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates a tag template.\n\nThis method cannot be used to update the fields of a template. The tag\ntemplate fields are represented as separate resources and should be updated using their own\ncreate/update/delete methods.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogUpdateTagTemplateOperator`\n\n:param tag_template: Required. The template to update. The \"name\" field must be set.\n\n    If a dict is provided, it mus",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.717895",
      "relevance_keywords": [
        "tag",
        "gcs",
        "operator",
        "google",
        "update",
        "clouddatacatalogupdatetagtemplateoperator",
        "template",
        "gcp",
        "bigquery",
        "data",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_clouddatacatalogupdatetagtemplatefieldoperator_20260128_160330",
      "component_name": "CloudDataCatalogUpdateTagTemplateFieldOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogUpdateTagTemplateFieldOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "tag_template_field",
          "update_mask",
          "tag_template_field_name",
          "location",
          "tag_template",
          "tag_template_field_id",
          "project_id",
          "retry",
          "timeout",
          "metadata",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "tag_template_field",
          "type": "Union",
          "required": false
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false
        },
        {
          "name": "tag_template_field_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tag_template",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tag_template_field_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.datacatalog import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates a field in a tag template. This method cannot be used to update the field type.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataCatalogUpdateTagTemplateFieldOperator`\n\n:param tag_template_field: Required. The template to update.\n\n    If a dict is provided, it must be of the same form as the protobuf message\n    :class:`~google.cloud.datacatalog_v1beta1.types.TagTemplateField`\n:param update_mask: The field mas",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.719590",
      "relevance_keywords": [
        "tag",
        "gcs",
        "operator",
        "google",
        "update",
        "template",
        "gcp",
        "bigquery",
        "data",
        "clouddatacatalogupdatetagtemplatefieldoperator",
        "cloud",
        "field",
        "catalog"
      ]
    },
    {
      "id": "official_dataflowtemplatedjobstartoperator_20260128_160330",
      "component_name": "DataflowTemplatedJobStartOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowTemplatedJobStartOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "template",
          "job_name",
          "options",
          "parameters",
          "project_id",
          "location",
          "gcp_conn_id",
          "impersonation_chain",
          "environment",
          "dataflow_default_options"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "template",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "job_name",
          "type": "str",
          "required": false,
          "default": "{{task.task_id}}"
        },
        {
          "name": "options",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "dataflow_default_options",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "parameters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "poll_sleep",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "environment",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cancel_timeout",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "wait_until_finished",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "append_job_name",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport uuid\nfrom collections.abc import Sequence\nfrom enum import Enum\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_deferrable_params",
          "hook",
          "execute",
          "execute_complete",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Start a Dataflow job with a classic template; the parameters of the operation will be passed to the job.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowTemplatedJobStartOperator`\n\n:param template: The reference to the Dataflow template.\n:param job_name: The 'jobName' to use when executing the Dataflow template\n    (templated).\n:param options: Map of job runtime environment options.\n    It will update environment argu",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.784963",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "start",
        "dataflowtemplatedjobstartoperator",
        "bigquery",
        "dataflow",
        "cloud",
        "templated",
        "job"
      ]
    },
    {
      "id": "official_dataflowstartflextemplateoperator_20260128_160330",
      "component_name": "DataflowStartFlexTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowStartFlexTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "body",
          "location",
          "project_id",
          "gcp_conn_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict",
          "required": true
        },
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "drain_pipeline",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "cancel_timeout",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "wait_until_finished",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "append_job_name",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "expected_terminal_state",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "poll_sleep",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport uuid\nfrom collections.abc import Sequence\nfrom enum import Enum\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_deferrable_params",
          "hook",
          "execute",
          "_append_uuid_to_job_name",
          "execute_complete",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts a Dataflow Job with a Flex Template.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowStartFlexTemplateOperator`\n\n:param body: The request body. See:\n    https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch#request-body\n:param location: The location of the Dataflow job (for example europe-west1)\n:param project_id: The ID of the GCP project that owns the job.\n:param g",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.785531",
      "relevance_keywords": [
        "gcs",
        "operator",
        "dataflowstartflextemplateoperator",
        "google",
        "start",
        "template",
        "gcp",
        "bigquery",
        "dataflow",
        "flex",
        "cloud"
      ]
    },
    {
      "id": "official_dataflowstartyamljoboperator_20260128_160330",
      "component_name": "DataflowStartYamlJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowStartYamlJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "job_name",
          "yaml_pipeline_file",
          "jinja_variables",
          "options",
          "region",
          "project_id",
          "gcp_conn_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "yaml_pipeline_file",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_DATAFLOW_LOCATION>"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "append_job_name",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "drain_pipeline",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_sleep",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "cancel_timeout",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "expected_terminal_state",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "jinja_variables",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "options",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport uuid\nfrom collections.abc import Sequence\nfrom enum import Enum\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "on_kill",
          "hook"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Launch a Dataflow YAML job and return the result.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowStartYamlJobOperator`\n\n.. warning::\n    This operator requires ``gcloud`` command (Google Cloud SDK) must be installed on the Airflow worker\n    <https://cloud.google.com/sdk/docs/install>`__\n\n:param job_name: Required. The unique name to assign to the Cloud Dataflow job.\n:param yaml_pipeline_file: Required. Path to a fil",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.786012",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "start",
        "bigquery",
        "dataflow",
        "yaml",
        "cloud",
        "job",
        "dataflowstartyamljoboperator"
      ]
    },
    {
      "id": "official_dataflowstopjoboperator_20260128_160330",
      "component_name": "DataflowStopJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowStopJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "job_id",
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name_prefix",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_DATAFLOW_LOCATION>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "poll_sleep",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "stop_timeout",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "drain_pipeline",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport uuid\nfrom collections.abc import Sequence\nfrom enum import Enum\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Stops the job with the specified name prefix or Job ID.\n\nAll jobs with provided name prefix will be stopped.\nStreaming jobs are drained by default.\n\nParameter ``job_name_prefix`` and ``job_id`` are mutually exclusive.\n\n.. seealso::\n    For more details on stopping a pipeline see:\n    https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowStopJobOperator`\n\n:par",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.786434",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "dataflowstopjoboperator",
        "stop",
        "gcp",
        "bigquery",
        "dataflow",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_dataflowcreatepipelineoperator_20260128_160330",
      "component_name": "DataflowCreatePipelineOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowCreatePipelineOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_DATAFLOW_LOCATION>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport uuid\nfrom collections.abc import Sequence\nfrom enum import Enum\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a new Dataflow Data Pipeline instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowCreatePipelineOperator`\n\n:param body: The request body (contains instance of Pipeline). See:\n    https://cloud.google.com/dataflow/docs/reference/data-pipelines/rest/v1/projects.locations.pipelines/create#request-body\n:param project_id: The ID of the GCP project that owns the job.\n:param location: The location to direct the ",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.786841",
      "relevance_keywords": [
        "dataflowcreatepipelineoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "pipeline",
        "dataflow",
        "cloud"
      ]
    },
    {
      "id": "official_dataflowrunpipelineoperator_20260128_160330",
      "component_name": "DataflowRunPipelineOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowRunPipelineOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "pipeline_name",
          "type": "str",
          "required": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_DATAFLOW_LOCATION>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport uuid\nfrom collections.abc import Sequence\nfrom enum import Enum\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Runs a Dataflow Data Pipeline.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowRunPipelineOperator`\n\n:param pipeline_name:  The display name of the pipeline. In example\n    projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID it would be the PIPELINE_ID.\n:param project_id: The ID of the GCP project that owns the job.\n:param location: The location to direct the Data Pipelines instance to (for example us-cent",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.787232",
      "relevance_keywords": [
        "dataflowrunpipelineoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "run",
        "pipeline",
        "dataflow",
        "cloud"
      ]
    },
    {
      "id": "official_dataflowdeletepipelineoperator_20260128_160330",
      "component_name": "DataflowDeletePipelineOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowDeletePipelineOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "pipeline_name",
          "type": "str",
          "required": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_DATAFLOW_LOCATION>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport uuid\nfrom collections.abc import Sequence\nfrom enum import Enum\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf",
        "class_attributes": [],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a Dataflow Data Pipeline.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowDeletePipelineOperator`\n\n:param pipeline_name: The display name of the pipeline. In example\n    projects/PROJECT_ID/locations/LOCATION_ID/pipelines/PIPELINE_ID it would be the PIPELINE_ID.\n:param project_id: The ID of the GCP project that owns the job.\n:param location: The location to direct the Data Pipelines instance to (for example us",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.787648",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "pipeline",
        "dataflow",
        "dataflowdeletepipelineoperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataformcreatecompilationresultoperator_20260128_160330",
      "component_name": "DataformCreateCompilationResultOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformCreateCompilationResultOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "compilation_result",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "compilation_result",
          "type": "Union",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new CompilationResult in a given project and location.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param repository_id: Required. The ID of the Dataform repository that the task belongs to.\n:param compilation_result:  Required. The compilation result to create.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeou",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.832606",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataform",
        "create",
        "compilation",
        "bigquery",
        "result",
        "dataformcreatecompilationresultoperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataformgetcompilationresultoperator_20260128_160330",
      "component_name": "DataformGetCompilationResultOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformGetCompilationResultOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "compilation_result_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "compilation_result_id",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Fetches a single CompilationResult.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param repository_id: Required. The ID of the Dataform repository that the task belongs to.\n:param compilation_result_id:  The Id of the Dataform Compilation Result\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param m",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.833164",
      "relevance_keywords": [
        "dataformgetcompilationresultoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataform",
        "compilation",
        "bigquery",
        "result",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_dataformcreateworkflowinvocationoperator_20260128_160330",
      "component_name": "DataformCreateWorkflowInvocationOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformCreateWorkflowInvocationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workflow_invocation",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workflow_invocation",
          "type": "Union",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "asynchronous",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "wait_time",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new WorkflowInvocation in a given Repository.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param repository_id: Required. The ID of the Dataform repository that the task belongs to.\n:param workflow_invocation:  Required. The workflow invocation resource to create.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The time",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.833709",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataformcreateworkflowinvocationoperator",
        "dataform",
        "create",
        "invocation",
        "bigquery",
        "cloud",
        "workflow"
      ]
    },
    {
      "id": "official_dataformgetworkflowinvocationoperator_20260128_160330",
      "component_name": "DataformGetWorkflowInvocationOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformGetWorkflowInvocationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workflow_invocation_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workflow_invocation_id",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Fetches a single WorkflowInvocation.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param repository_id: Required. The ID of the Dataform repository that the task belongs to.\n:param workflow_invocation_id:  the workflow invocation resource's id.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param me",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.834215",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataformgetworkflowinvocationoperator",
        "dataform",
        "invocation",
        "bigquery",
        "get",
        "cloud",
        "workflow"
      ]
    },
    {
      "id": "official_dataformqueryworkflowinvocationactionsoperator_20260128_160330",
      "component_name": "DataformQueryWorkflowInvocationActionsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformQueryWorkflowInvocationActionsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workflow_invocation_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workflow_invocation_id",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Returns WorkflowInvocationActions in a given WorkflowInvocation.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param repository_id: Required. The ID of the Dataform repository that the task belongs to.\n:param workflow_invocation_id:  the workflow invocation resource's id.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.834866",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataform",
        "dataformqueryworkflowinvocationactionsoperator",
        "invocation",
        "bigquery",
        "cloud",
        "workflow",
        "actions",
        "query"
      ]
    },
    {
      "id": "official_dataformcancelworkflowinvocationoperator_20260128_160330",
      "component_name": "DataformCancelWorkflowInvocationOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformCancelWorkflowInvocationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workflow_invocation_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workflow_invocation_id",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Requests cancellation of a running WorkflowInvocation.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param repository_id: Required. The ID of the Dataform repository that the task belongs to.\n:param workflow_invocation_id:  the workflow invocation resource's id.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.835487",
      "relevance_keywords": [
        "gcs",
        "operator",
        "dataformcancelworkflowinvocationoperator",
        "google",
        "dataform",
        "gcp",
        "cancel",
        "invocation",
        "bigquery",
        "cloud",
        "workflow"
      ]
    },
    {
      "id": "official_dataformcreaterepositoryoperator_20260128_160330",
      "component_name": "DataformCreateRepositoryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformCreateRepositoryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates repository.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param repository_id: Required. The ID of the Dataform repository that the task belongs to.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata: Strings which should be sent along with the request as metadata.\n:param gcp_conn",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.835995",
      "relevance_keywords": [
        "dataformcreaterepositoryoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataform",
        "create",
        "bigquery",
        "cloud",
        "repository"
      ]
    },
    {
      "id": "official_dataformdeleterepositoryoperator_20260128_160330",
      "component_name": "DataformDeleteRepositoryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformDeleteRepositoryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "force",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes repository.\n\n:param project_id: Required. The ID of the Google Cloud project where repository located.\n:param region: Required. The ID of the Google Cloud region where repository located.\n:param repository_id: Required. The ID of the Dataform repository that should be deleted.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata: Strings which should be sent along with the request as metadata.\n:param gcp_conn_i",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.836464",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataform",
        "dataformdeleterepositoryoperator",
        "bigquery",
        "delete",
        "cloud",
        "repository"
      ]
    },
    {
      "id": "official_dataformcreateworkspaceoperator_20260128_160330",
      "component_name": "DataformCreateWorkspaceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformCreateWorkspaceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workspace_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workspace_id",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates workspace.\n\n:param project_id: Required. The ID of the Google Cloud project where workspace should be in.\n:param region: Required. Name of the Google Cloud region that where workspace should be in.\n:param repository_id: Required. The ID of the Dataform repository that the workspace belongs to.\n:param workspace_id: Required. The ID of the new workspace that will be created.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.836893",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataform",
        "create",
        "bigquery",
        "workspace",
        "dataformcreateworkspaceoperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataformdeleteworkspaceoperator_20260128_160330",
      "component_name": "DataformDeleteWorkspaceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformDeleteWorkspaceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workspace_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workspace_id",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes workspace.\n\n:param project_id: Required. The ID of the Google Cloud project where workspace located.\n:param region: Required. The ID of the Google Cloud region where workspace located.\n:param repository_id: Required. The ID of the Dataform repository where workspace located.\n:param workspace_id: Required. The ID of the Dataform workspace that should be deleted.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metada",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.837378",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataform",
        "bigquery",
        "workspace",
        "delete",
        "dataformdeleteworkspaceoperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataformwritefileoperator_20260128_160330",
      "component_name": "DataformWriteFileOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformWriteFileOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workspace_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workspace_id",
          "type": "str",
          "required": true
        },
        {
          "name": "filepath",
          "type": "str",
          "required": true
        },
        {
          "name": "contents",
          "type": "bytes",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Writes new file to specified workspace.\n\n:param project_id: Required. The ID of the Google Cloud project where workspace located.\n:param region: Required. The ID of the Google Cloud region where workspace located.\n:param repository_id: Required. The ID of the Dataform repository where workspace located.\n:param workspace_id: Required. The ID of the Dataform workspace where files should be created.\n:param filepath: Required. Path to file including name of the file relative to workspace root.\n:para",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.837912",
      "relevance_keywords": [
        "gcs",
        "file",
        "operator",
        "dataformwritefileoperator",
        "google",
        "dataform",
        "gcp",
        "bigquery",
        "write",
        "cloud"
      ]
    },
    {
      "id": "official_dataformmakedirectoryoperator_20260128_160330",
      "component_name": "DataformMakeDirectoryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformMakeDirectoryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workspace_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workspace_id",
          "type": "str",
          "required": true
        },
        {
          "name": "directory_path",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Makes new directory in specified workspace.\n\n:param project_id: Required. The ID of the Google Cloud project where workspace located.\n:param region: Required. The ID of the Google Cloud region where workspace located.\n:param repository_id: Required. The ID of the Dataform repository where workspace located.\n:param workspace_id: Required. The ID of the Dataform workspace where directory should be created.\n:param path: Required. The directory's full path including directory name, relative to the w",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.838357",
      "relevance_keywords": [
        "make",
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataform",
        "directory",
        "bigquery",
        "dataformmakedirectoryoperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataformremovefileoperator_20260128_160330",
      "component_name": "DataformRemoveFileOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformRemoveFileOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workspace_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workspace_id",
          "type": "str",
          "required": true
        },
        {
          "name": "filepath",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Removes file in specified workspace.\n\n:param project_id: Required. The ID of the Google Cloud project where workspace located.\n:param region: Required. The ID of the Google Cloud region where workspace located.\n:param repository_id: Required. The ID of the Dataform repository where workspace located.\n:param workspace_id: Required. The ID of the Dataform workspace where directory located.\n:param filepath: Required. The full path including name of the file, relative to the workspace root.\n:param r",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.838889",
      "relevance_keywords": [
        "dataformremovefileoperator",
        "gcs",
        "file",
        "operator",
        "google",
        "gcp",
        "dataform",
        "remove",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatafusionrestartinstanceoperator_20260128_160330",
      "component_name": "CloudDataFusionRestartInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionRestartInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "instance_name",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.retry import exponential_sleep_generator\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.datafusion import SUCCESS_STATES, DataFusionHook, PipelineStates\nfrom airflow.providers.google.cloud.links.datafusion import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Restart a single Data Fusion instance.\n\nAt the end of an operation instance is fully restarted.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataFusionRestartInstanceOperator`\n\n:param instance_name: The name of the instance to restart.\n:param location: The Cloud Data Fusion location in which to handle the request.\n:param project_id: The ID of the Google Cloud project that the instance belongs to.\n:param api_version: T",
      "success_score": 191,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.863606",
      "relevance_keywords": [
        "restart",
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "data",
        "clouddatafusionrestartinstanceoperator",
        "fusion",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatafusiondeleteinstanceoperator_20260128_160330",
      "component_name": "CloudDataFusionDeleteInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionDeleteInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "instance_name",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.retry import exponential_sleep_generator\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.datafusion import SUCCESS_STATES, DataFusionHook, PipelineStates\nfrom airflow.providers.google.cloud.links.datafusion import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a single Date Fusion instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataFusionDeleteInstanceOperator`\n\n:param instance_name: The name of the instance to restart.\n:param location: The Cloud Data Fusion location in which to handle the request.\n:param project_id: The ID of the Google Cloud project that the instance belongs to.\n:param api_version: The version of the api that will be requested for example '",
      "success_score": 191,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.864024",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "data",
        "delete",
        "fusion",
        "clouddatafusiondeleteinstanceoperator",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatafusioncreateinstanceoperator_20260128_160330",
      "component_name": "CloudDataFusionCreateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionCreateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "instance_name",
          "instance",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.retry import exponential_sleep_generator\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.datafusion import SUCCESS_STATES, DataFusionHook, PipelineStates\nfrom airflow.providers.google.cloud.links.datafusion import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a new Data Fusion instance in the specified project and location.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataFusionCreateInstanceOperator`\n\n:param instance_name: The name of the instance to create.\n:param instance: An instance of Instance.\n    https://cloud.google.com/data-fusion/docs/reference/rest/v1beta1/projects.locations.instances#Instance\n:param location: The Cloud Data Fusion location in which to ",
      "success_score": 194,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.864374",
      "relevance_keywords": [
        "clouddatafusioncreateinstanceoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "instance",
        "bigquery",
        "data",
        "fusion",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatafusionupdateinstanceoperator_20260128_160330",
      "component_name": "CloudDataFusionUpdateInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionUpdateInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "instance_name",
          "instance",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "instance",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "update_mask",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.retry import exponential_sleep_generator\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.datafusion import SUCCESS_STATES, DataFusionHook, PipelineStates\nfrom airflow.providers.google.cloud.links.datafusion import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Updates a single Data Fusion instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataFusionUpdateInstanceOperator`\n\n:param instance_name: The name of the instance to create.\n:param instance: An instance of Instance.\n    https://cloud.google.com/data-fusion/docs/reference/rest/v1beta1/projects.locations.instances#Instance\n:param update_mask: Field mask is used to specify the fields that the update will overwrite\n   ",
      "success_score": 194,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.864781",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "instance",
        "bigquery",
        "data",
        "fusion",
        "cloud",
        "clouddatafusionupdateinstanceoperator"
      ]
    },
    {
      "id": "official_clouddatafusiongetinstanceoperator_20260128_160330",
      "component_name": "CloudDataFusionGetInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionGetInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "instance_name",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.retry import exponential_sleep_generator\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.datafusion import SUCCESS_STATES, DataFusionHook, PipelineStates\nfrom airflow.providers.google.cloud.links.datafusion import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Gets details of a single Data Fusion instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataFusionGetInstanceOperator`\n\n:param instance_name: The name of the instance.\n:param location: The Cloud Data Fusion location in which to handle the request.\n:param project_id: The ID of the Google Cloud project that the instance belongs to.\n:param api_version: The version of the api that will be requested for example 'v3'.\n:",
      "success_score": 191,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.865219",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "data",
        "clouddatafusiongetinstanceoperator",
        "fusion",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatafusioncreatepipelineoperator_20260128_160330",
      "component_name": "CloudDataFusionCreatePipelineOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionCreatePipelineOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "instance_name",
          "pipeline_name",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "pipeline_name",
          "type": "str",
          "required": false
        },
        {
          "name": "pipeline",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "namespace",
          "type": "str",
          "required": false,
          "default": "default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.retry import exponential_sleep_generator\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.datafusion import SUCCESS_STATES, DataFusionHook, PipelineStates\nfrom airflow.providers.google.cloud.links.datafusion import (",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a Cloud Data Fusion pipeline.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataFusionCreatePipelineOperator`\n\n:param pipeline_name: Your pipeline name.\n:param pipeline: The pipeline definition. For more information check:\n    https://docs.cdap.io/cdap/current/en/developer-manual/pipelines/developing-pipelines.html#pipeline-configuration-file-format\n:param instance_name: The name of the instance.\n:param locatio",
      "success_score": 194,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.865582",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "data",
        "pipeline",
        "fusion",
        "cloud",
        "clouddatafusioncreatepipelineoperator"
      ]
    },
    {
      "id": "official_clouddatafusiondeletepipelineoperator_20260128_160330",
      "component_name": "CloudDataFusionDeletePipelineOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionDeletePipelineOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "instance_name",
          "version_id",
          "pipeline_name",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "pipeline_name",
          "type": "str",
          "required": false
        },
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "version_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "namespace",
          "type": "str",
          "required": false,
          "default": "default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.retry import exponential_sleep_generator\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.datafusion import SUCCESS_STATES, DataFusionHook, PipelineStates\nfrom airflow.providers.google.cloud.links.datafusion import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a Cloud Data Fusion pipeline.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataFusionDeletePipelineOperator`\n\n:param pipeline_name: Your pipeline name.\n:param version_id: Version of pipeline to delete\n:param instance_name: The name of the instance.\n:param location: The Cloud Data Fusion location in which to handle the request.\n:param namespace: If your pipeline belongs to a Basic edition instance, the namespac",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.865882",
      "relevance_keywords": [
        "clouddatafusiondeletepipelineoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "data",
        "delete",
        "pipeline",
        "fusion",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatafusionlistpipelinesoperator_20260128_160330",
      "component_name": "CloudDataFusionListPipelinesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionListPipelinesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "instance_name",
          "artifact_name",
          "artifact_version",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "artifact_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "artifact_version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "namespace",
          "type": "str",
          "required": false,
          "default": "default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.retry import exponential_sleep_generator\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.datafusion import SUCCESS_STATES, DataFusionHook, PipelineStates\nfrom airflow.providers.google.cloud.links.datafusion import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Lists Cloud Data Fusion pipelines.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataFusionListPipelinesOperator`\n\n\n:param instance_name: The name of the instance.\n:param location: The Cloud Data Fusion location in which to handle the request.\n:param artifact_version: Artifact version to filter instances\n:param artifact_name: Artifact name to filter instances\n:param namespace: If your pipeline belongs to a Basic editio",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.866200",
      "relevance_keywords": [
        "clouddatafusionlistpipelinesoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "pipelines",
        "data",
        "list",
        "fusion",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatafusionstartpipelineoperator_20260128_160330",
      "component_name": "CloudDataFusionStartPipelineOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionStartPipelineOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "instance_name",
          "pipeline_name",
          "runtime_args",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "pipeline_name",
          "type": "str",
          "required": false
        },
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "pipeline_type",
          "type": "DataFusionPipelineType",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "runtime_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "success_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "namespace",
          "type": "str",
          "required": false,
          "default": "default"
        },
        {
          "name": "pipeline_timeout",
          "type": "int",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "asynchronous",
          "type": "Any",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "Any",
          "required": false,
          "default": 3.0
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.retry import exponential_sleep_generator\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.datafusion import SUCCESS_STATES, DataFusionHook, PipelineStates\nfrom airflow.providers.google.cloud.links.datafusion import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Starts a Cloud Data Fusion pipeline. Works for both batch and stream pipelines.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataFusionStartPipelineOperator`\n\n:param pipeline_name: Your pipeline name.\n:param pipeline_type: Optional pipeline type (BATCH by default).\n:param instance_name: The name of the instance.\n:param success_states: If provided the operator will wait for pipeline to be in one of\n    the provided sta",
      "success_score": 197,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.866562",
      "relevance_keywords": [
        "clouddatafusionstartpipelineoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "start",
        "bigquery",
        "data",
        "pipeline",
        "fusion",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatafusionstoppipelineoperator_20260128_160330",
      "component_name": "CloudDataFusionStopPipelineOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionStopPipelineOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "instance_name",
          "pipeline_name",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "pipeline_name",
          "type": "str",
          "required": false
        },
        {
          "name": "instance_name",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "namespace",
          "type": "str",
          "required": false,
          "default": "default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.retry import exponential_sleep_generator\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException, conf\nfrom airflow.providers.google.cloud.hooks.datafusion import SUCCESS_STATES, DataFusionHook, PipelineStates\nfrom airflow.providers.google.cloud.links.datafusion import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Stops a Cloud Data Fusion pipeline. Works for both batch and stream pipelines.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataFusionStopPipelineOperator`\n\n:param pipeline_name: Your pipeline name.\n:param instance_name: The name of the instance.\n:param location: The Cloud Data Fusion location in which to handle the request.\n:param namespace: If your pipeline belongs to a Basic edition instance, the namespace ID\n    i",
      "success_score": 194,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.866956",
      "relevance_keywords": [
        "clouddatafusionstoppipelineoperator",
        "gcs",
        "operator",
        "google",
        "stop",
        "gcp",
        "bigquery",
        "data",
        "pipeline",
        "fusion",
        "cloud"
      ]
    },
    {
      "id": "official_dataplexcreatetaskoperator_20260128_160330",
      "component_name": "DataplexCreateTaskOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCreateTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "dataplex_task_id",
          "body",
          "validate_only",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "body",
          "type": "dict[...]",
          "required": true
        },
        {
          "name": "dataplex_task_id",
          "type": "str",
          "required": true
        },
        {
          "name": "validate_only",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "asynchronous",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a task resource within a lake.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param lake_id: Required. The ID of the Google Cloud lake that the task belongs to.\n:param body:  Required. The Request body contains an instance of Task.\n:param dataplex_task_id: Required. Task identifier.\n:param validate_only: Optional. Only validate the request, but do not perform ",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.939884",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "task",
        "create",
        "gcp",
        "bigquery",
        "cloud",
        "dataplexcreatetaskoperator"
      ]
    },
    {
      "id": "official_dataplexdeletetaskoperator_20260128_160330",
      "component_name": "DataplexDeleteTaskOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexDeleteTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "dataplex_task_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "dataplex_task_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete the task resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param lake_id: Required. The ID of the Google Cloud lake that the task belongs to.\n:param dataplex_task_id: Required. Task identifier.\n:param api_version: The version of the api that will be requested for example 'v3'.\n:param retry: A retry object used  to retry requests. If `None` is specified, ",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.940526",
      "relevance_keywords": [
        "dataplexdeletetaskoperator",
        "dataplex",
        "gcs",
        "operator",
        "google",
        "task",
        "gcp",
        "bigquery",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_dataplexlisttasksoperator_20260128_160330",
      "component_name": "DataplexListTasksOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexListTasksOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "page_size",
          "page_token",
          "filter",
          "order_by",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Lists tasks under the given lake.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param lake_id: Required. The ID of the Google Cloud lake that the task belongs to.\n:param page_size: Optional. Maximum number of tasks to return. The service may return fewer than this\n    value. If unspecified, at most 10 tasks will be returned. The maximum value is 1000; values above 10",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.941094",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "tasks",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "dataplexlisttasksoperator",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_dataplexgettaskoperator_20260128_160330",
      "component_name": "DataplexGetTaskOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexGetTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "dataplex_task_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "dataplex_task_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Get task resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param lake_id: Required. The ID of the Google Cloud lake that the task belongs to.\n:param dataplex_task_id: Required. Task identifier.\n:param api_version: The version of the api that will be requested for example 'v3'.\n:param retry: A retry object used  to retry requests. If `None` is specified, request",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.941804",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "task",
        "gcp",
        "dataplexgettaskoperator",
        "bigquery",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_dataplexcreatelakeoperator_20260128_160330",
      "component_name": "DataplexCreateLakeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCreateLakeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "lake_id",
          "body",
          "validate_only",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "body",
          "type": "dict[...]",
          "required": true
        },
        {
          "name": "validate_only",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "asynchronous",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a lake resource within a lake.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param lake_id: Required. Lake identifier.\n:param body:  Required. The Request body contains an instance of Lake.\n:param validate_only: Optional. Only validate the request, but do not perform mutations. The default is\n    false.\n:param api_version: The version of the api that will be ",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.942531",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "lake",
        "cloud",
        "dataplexcreatelakeoperator"
      ]
    },
    {
      "id": "official_dataplexdeletelakeoperator_20260128_160330",
      "component_name": "DataplexDeleteLakeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexDeleteLakeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "lake_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete the lake resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param lake_id: Required. Lake identifier.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A retry object used  to retry requests. If `None` is specified, requests\n    will not be retried.\n:param timeout: The amount of time, in seconds, to wait for",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.943122",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "lake",
        "cloud",
        "dataplexdeletelakeoperator"
      ]
    },
    {
      "id": "official_dataplexcreateorupdatedataqualityscanoperator_20260128_160330",
      "component_name": "DataplexCreateOrUpdateDataQualityScanOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCreateOrUpdateDataQualityScanOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "data_scan_id",
          "body",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "body",
          "type": "Union",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute",
          "_update_data_scan"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a DataScan resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param body:  Required. The Request body contains an instance of DataScan.\n:param data_scan_id: Required. Data Quality scan identifier.\n:param update_mask: Mask of fields to update.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A retry object ",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.944196",
      "relevance_keywords": [
        "quality",
        "dataplex",
        "or",
        "gcs",
        "operator",
        "google",
        "update",
        "create",
        "gcp",
        "bigquery",
        "data",
        "dataplexcreateorupdatedataqualityscanoperator",
        "cloud",
        "scan"
      ]
    },
    {
      "id": "official_dataplexgetdataqualityscanoperator_20260128_160330",
      "component_name": "DataplexGetDataQualityScanOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexGetDataQualityScanOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "data_scan_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Gets a DataScan resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param data_scan_id: Required. Data Quality scan identifier.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A retry object used  to retry requests. If `None` is specified, requests\n    will not be retried.\n:param timeout: The amount of time, in se",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.945227",
      "relevance_keywords": [
        "quality",
        "dataplexgetdataqualityscanoperator",
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "data",
        "get",
        "cloud",
        "scan"
      ]
    },
    {
      "id": "official_dataplexdeletedataqualityscanoperator_20260128_160330",
      "component_name": "DataplexDeleteDataQualityScanOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexDeleteDataQualityScanOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "data_scan_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a DataScan resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param data_scan_id: Required. Data Quality scan identifier.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A retry object used  to retry requests. If `None` is specified, requests\n    will not be retried.\n:param timeout: The amount of time, in",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.946168",
      "relevance_keywords": [
        "quality",
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "data",
        "dataplexdeletedataqualityscanoperator",
        "cloud",
        "scan"
      ]
    },
    {
      "id": "official_dataplexrundataqualityscanoperator_20260128_160330",
      "component_name": "DataplexRunDataQualityScanOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexRunDataQualityScanOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "data_scan_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "asynchronous",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "fail_on_dq_failure",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "result_timeout",
          "type": "float",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Runs an on-demand execution of a DataScan.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param data_scan_id: Required. Data Quality scan identifier.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A retry object used  to retry requests. If `None` is specified, requests\n    will not be retried.\n:param timeout: The amou",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.947169",
      "relevance_keywords": [
        "quality",
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "run",
        "data",
        "cloud",
        "scan",
        "dataplexrundataqualityscanoperator"
      ]
    },
    {
      "id": "official_dataplexgetdataqualityscanresultoperator_20260128_160330",
      "component_name": "DataplexGetDataQualityScanResultOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexGetDataQualityScanResultOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "data_scan_id",
          "impersonation_chain",
          "job_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "job_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "fail_on_dq_failure",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "wait_for_results",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "result_timeout",
          "type": "float",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Gets a Data Scan Job resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param data_scan_id: Required. Data Quality scan identifier.\n:param job_id: Optional. Data Quality scan job identifier.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A retry object used  to retry requests. If `None` is specified, requests\n  ",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.948131",
      "relevance_keywords": [
        "dataplexgetdataqualityscanresultoperator",
        "dataplex",
        "quality",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "result",
        "data",
        "get",
        "cloud",
        "scan"
      ]
    },
    {
      "id": "official_dataplexcreateorupdatedataprofilescanoperator_20260128_160330",
      "component_name": "DataplexCreateOrUpdateDataProfileScanOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCreateOrUpdateDataProfileScanOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "data_scan_id",
          "body",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "body",
          "type": "Union",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a DataScan Data Profile resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param body:  Required. The Request body contains an instance of DataScan.\n:param data_scan_id: Required. Data Profile scan identifier.\n:param update_mask: Mask of fields to update.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A ",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.949041",
      "relevance_keywords": [
        "dataplex",
        "or",
        "gcs",
        "operator",
        "google",
        "update",
        "create",
        "dataplexcreateorupdatedataprofilescanoperator",
        "gcp",
        "bigquery",
        "data",
        "scan",
        "cloud",
        "profile"
      ]
    },
    {
      "id": "official_dataplexgetdataprofilescanoperator_20260128_160330",
      "component_name": "DataplexGetDataProfileScanOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexGetDataProfileScanOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "data_scan_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Gets a DataScan DataProfile resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param data_scan_id: Required. Data Profile scan identifier.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A retry object used  to retry requests. If `None` is specified, requests\n    will not be retried.\n:param timeout: The amount of",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.949956",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "dataplexgetdataprofilescanoperator",
        "data",
        "scan",
        "get",
        "cloud",
        "profile"
      ]
    },
    {
      "id": "official_dataplexdeletedataprofilescanoperator_20260128_160330",
      "component_name": "DataplexDeleteDataProfileScanOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexDeleteDataProfileScanOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "data_scan_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a DataScan DataProfile resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param data_scan_id: Required. Data Profile scan identifier.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A retry object used  to retry requests. If `None` is specified, requests\n    will not be retried.\n:param timeout: The amount",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.950837",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "dataplexdeletedataprofilescanoperator",
        "delete",
        "data",
        "scan",
        "cloud",
        "profile"
      ]
    },
    {
      "id": "official_dataplexrundataprofilescanoperator_20260128_160330",
      "component_name": "DataplexRunDataProfileScanOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexRunDataProfileScanOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "data_scan_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "asynchronous",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "result_timeout",
          "type": "float",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Runs an on-demand execution of a DataScan Data Profile Scan.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param data_scan_id: Required. Data Profile scan identifier.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A retry object used  to retry requests. If `None` is specified, requests\n        will not be retried.\n:p",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.951756",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "run",
        "data",
        "dataplexrundataprofilescanoperator",
        "scan",
        "cloud",
        "profile"
      ]
    },
    {
      "id": "official_dataplexgetdataprofilescanresultoperator_20260128_160330",
      "component_name": "DataplexGetDataProfileScanResultOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexGetDataProfileScanResultOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "data_scan_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "job_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_results",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "result_timeout",
          "type": "float",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Gets a DataScan Data Profile Job resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the lake belongs to.\n:param region: Required. The ID of the Google Cloud region that the lake belongs to.\n:param data_scan_id: Required. Data Profile scan identifier.\n:param job_id: Optional. Data Profile scan job identifier.\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param retry: A retry object used  to retry requests. If `None` is specified,",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.952666",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "result",
        "data",
        "dataplexgetdataprofilescanresultoperator",
        "scan",
        "get",
        "cloud",
        "profile"
      ]
    },
    {
      "id": "official_dataplexcreatezoneoperator_20260128_160330",
      "component_name": "DataplexCreateZoneOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCreateZoneOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "zone_id",
          "body",
          "lake_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "body",
          "type": "Union",
          "required": true
        },
        {
          "name": "zone_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates a Zone resource within a Lake.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param lake_id: Required. The ID of the Google Cloud lake that the task belongs to.\n:param body:  Required. The Request body contains an instance of Zone.\n:param zone_id: Required. Task identifier.\n:param api_version: The version of the api that will be requested for example 'v3'.\n:pa",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.953356",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "dataplexcreatezoneoperator",
        "operator",
        "google",
        "create",
        "gcp",
        "bigquery",
        "zone",
        "cloud"
      ]
    },
    {
      "id": "official_dataplexdeletezoneoperator_20260128_160330",
      "component_name": "DataplexDeleteZoneOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexDeleteZoneOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "lake_id",
          "zone_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "zone_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes a Zone resource. All assets within a zone must be deleted before the zone can be deleted.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param lake_id: Required. The ID of the Google Cloud lake that the task belongs to.\n:param zone_id: Required. Zone identifier.\n:param api_version: The version of the api that will be requested for example 'v3'.\n:param retry: A",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.953934",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "zone",
        "cloud",
        "dataplexdeletezoneoperator"
      ]
    },
    {
      "id": "official_dataplexcreateassetoperator_20260128_160330",
      "component_name": "DataplexCreateAssetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCreateAssetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "zone_id",
          "asset_id",
          "body",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "body",
          "type": "Union",
          "required": true
        },
        {
          "name": "zone_id",
          "type": "str",
          "required": true
        },
        {
          "name": "asset_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates an Asset resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param lake_id: Required. The ID of the Google Cloud lake that the lake belongs to.\n:param zone_id: Required. Zone identifier.\n:param asset_id: Required. Asset identifier.\n:param body:  Required. The Request body contains an instance of Asset.\n:param api_version: The version of the api that will ",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.954716",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "dataplexcreateassetoperator",
        "cloud",
        "asset"
      ]
    },
    {
      "id": "official_dataplexdeleteassetoperator_20260128_160330",
      "component_name": "DataplexDeleteAssetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexDeleteAssetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "zone_id",
          "asset_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "zone_id",
          "type": "str",
          "required": true
        },
        {
          "name": "asset_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes an asset resource.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param lake_id: Required. The ID of the Google Cloud lake that the asset belongs to.\n:param zone_id: Required. Zone identifier.\n:param asset_id: Required. Asset identifier.\n:param api_version: The version of the api that will be requested for example 'v3'.\n:param retry: A retry object used  to re",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.955540",
      "relevance_keywords": [
        "dataplexdeleteassetoperator",
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "cloud",
        "asset"
      ]
    },
    {
      "id": "official_dataplexcatalogbaseoperator_20260128_160330",
      "component_name": "DataplexCatalogBaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "location",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "hook",
          "extra_links_params"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Base class for all Dataplex Catalog operators.\n\n:param project_id: Required. The ID of the Google Cloud project where the service is used.\n:param location: Required. The ID of the Google Cloud region where the service is used.\n:param gcp_conn_id: Optional. The connection ID to use to connect to Google Cloud.\n:param retry: Optional. A retry object used to retry requests. If `None` is specified, requests will not\n    be retried.\n:param timeout: Optional. The amount of time, in seconds, to wait for",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.956495",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataplexcatalogbaseoperator",
        "bigquery",
        "base",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcatalogcreateentrygroupoperator_20260128_160330",
      "component_name": "DataplexCatalogCreateEntryGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogCreateEntryGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_group_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_group_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "validate_request",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create an EntryGroup resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogCreateEntryGroupOperator`\n\n:param entry_group_id: Required. EntryGroup identifier.\n:param entry_group_configuration: Required. EntryGroup configuration.\n    For more details please see API documentation:\n    https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.entryGroups#EntryGroup\n:param validate_request: Opt",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.957454",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "create",
        "gcp",
        "bigquery",
        "group",
        "cloud",
        "dataplexcatalogcreateentrygroupoperator",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcataloggetentrygroupoperator_20260128_160330",
      "component_name": "DataplexCatalogGetEntryGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogGetEntryGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_group_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Get an EntryGroup resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogGetEntryGroupOperator`\n\n:param entry_group_id: Required. EntryGroup identifier.\n:param project_id: Required. The ID of the Google Cloud project where the service is used.\n:param location: Required. The ID of the Google Cloud region where the service is used.\n:param gcp_conn_id: Optional. The connection ID to use to connect to Google Clo",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.958379",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "gcp",
        "bigquery",
        "get",
        "group",
        "cloud",
        "dataplexcataloggetentrygroupoperator",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcatalogdeleteentrygroupoperator_20260128_160330",
      "component_name": "DataplexCatalogDeleteEntryGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogDeleteEntryGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_group_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete an EntryGroup resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogDeleteEntryGroupOperator`\n\n:param entry_group_id: Required. EntryGroup identifier.\n:param project_id: Required. The ID of the Google Cloud project where the service is used.\n:param location: Required. The ID of the Google Cloud region where the service is used.\n:param gcp_conn_id: Optional. The connection ID to use to connect to Goog",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.958947",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "gcp",
        "dataplexcatalogdeleteentrygroupoperator",
        "bigquery",
        "delete",
        "group",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcataloglistentrygroupsoperator_20260128_160330",
      "component_name": "DataplexCatalogListEntryGroupsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogListEntryGroupsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "List EntryGroup resources.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogListEntryGroupsOperator`\n\n:param filter_by: Optional. Filter to apply on the list results.\n:param order_by: Optional. Fields to order the results by.\n:param page_size: Optional. Maximum number of EntryGroups to return on the page.\n:param page_token: Optional. Token to retrieve the next page of results.\n:param project_id: Required. The ID",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.959528",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "groups",
        "gcp",
        "dataplexcataloglistentrygroupsoperator",
        "bigquery",
        "list",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcatalogupdateentrygroupoperator_20260128_160330",
      "component_name": "DataplexCatalogUpdateEntryGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogUpdateEntryGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_group_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_group_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "validate_request",
          "type": "Union",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update an EntryGroup resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogUpdateEntryGroupOperator`\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param location: Required. The ID of the Google Cloud region that the task belongs to.\n:param update_mask: Optional. Names of fields whose values to overwrite on an entry group.\n    If this parameter is absent or empty",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.960411",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "update",
        "entry",
        "gcp",
        "bigquery",
        "group",
        "cloud",
        "catalog",
        "dataplexcatalogupdateentrygroupoperator"
      ]
    },
    {
      "id": "official_dataplexcatalogcreateentrytypeoperator_20260128_160330",
      "component_name": "DataplexCatalogCreateEntryTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogCreateEntryTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_type_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_type_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "validate_request",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create an EntryType resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogCreateEntryTypeOperator`\n\n:param entry_type_id: Required. EntryType identifier.\n:param entry_type_configuration: Required. EntryType configuration.\n    For more details please see API documentation:\n    https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.entryGroups#EntryGroup\n:param validate_request: Optional.",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.961074",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "create",
        "dataplexcatalogcreateentrytypeoperator",
        "gcp",
        "bigquery",
        "type",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcataloggetentrytypeoperator_20260128_160330",
      "component_name": "DataplexCatalogGetEntryTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogGetEntryTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_type_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Get an EntryType resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogGetEntryTypeOperator`\n\n:param entry_type_id: Required. EntryType identifier.\n:param project_id: Required. The ID of the Google Cloud project where the service is used.\n:param location: Required. The ID of the Google Cloud region where the service is used.\n:param gcp_conn_id: Optional. The connection ID to use to connect to Google Cloud.\n",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.961643",
      "relevance_keywords": [
        "dataplexcataloggetentrytypeoperator",
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "gcp",
        "bigquery",
        "get",
        "type",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcatalogdeleteentrytypeoperator_20260128_160330",
      "component_name": "DataplexCatalogDeleteEntryTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogDeleteEntryTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_type_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete an EntryType resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogDeleteEntryTypeOperator`\n\n:param entry_type_id: Required. EntryType identifier.\n:param project_id: Required. The ID of the Google Cloud project where the service is used.\n:param location: Required. The ID of the Google Cloud region where the service is used.\n:param gcp_conn_id: Optional. The connection ID to use to connect to Google C",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.962213",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "dataplexcatalogdeleteentrytypeoperator",
        "entry",
        "gcp",
        "bigquery",
        "delete",
        "type",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcataloglistentrytypesoperator_20260128_160330",
      "component_name": "DataplexCatalogListEntryTypesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogListEntryTypesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "List EntryType resources.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogListEntryTypesOperator`\n\n:param filter_by: Optional. Filter to apply on the list results.\n:param order_by: Optional. Fields to order the results by.\n:param page_size: Optional. Maximum number of EntryTypes to return on the page.\n:param page_token: Optional. Token to retrieve the next page of results.\n:param project_id: Required. The ID of",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.962761",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "types",
        "gcp",
        "bigquery",
        "list",
        "cloud",
        "dataplexcataloglistentrytypesoperator",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcatalogupdateentrytypeoperator_20260128_160330",
      "component_name": "DataplexCatalogUpdateEntryTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogUpdateEntryTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_type_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_type_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "validate_request",
          "type": "Union",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update an EntryType resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogUpdateEntryTypeOperator`\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param location: Required. The ID of the Google Cloud region that the task belongs to.\n:param update_mask: Optional. Names of fields whose values to overwrite on an entry group.\n    If this parameter is absent or empty, ",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.963279",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "dataplexcatalogupdateentrytypeoperator",
        "update",
        "entry",
        "gcp",
        "bigquery",
        "type",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcatalogcreateaspecttypeoperator_20260128_160330",
      "component_name": "DataplexCatalogCreateAspectTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogCreateAspectTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "aspect_type_id",
          "type": "str",
          "required": true
        },
        {
          "name": "aspect_type_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "validate_request",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create an AspectType resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogCreateAspectTypeOperator`\n\n:param aspect_type_id: Required. AspectType identifier.\n:param aspect_type_configuration: Required. AspectType configuration.\n    For more details please see API documentation:\n    https://cloud.google.com/dataplex/docs/reference/rest/v1/projects.locations.aspectTypes#AspectType\n:param validate_request: Opt",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.963788",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "dataplexcatalogcreateaspecttypeoperator",
        "type",
        "cloud",
        "catalog",
        "aspect"
      ]
    },
    {
      "id": "official_dataplexcataloggetaspecttypeoperator_20260128_160330",
      "component_name": "DataplexCatalogGetAspectTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogGetAspectTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "aspect_type_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Get an AspectType resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogGetAspectTypeOperator`\n\n:param aspect_type_id: Required. AspectType identifier.\n:param project_id: Required. The ID of the Google Cloud project where the service is used.\n:param location: Required. The ID of the Google Cloud region where the service is used.\n:param gcp_conn_id: Optional. The connection ID to use to connect to Google Clo",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.964331",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataplexcataloggetaspecttypeoperator",
        "bigquery",
        "get",
        "type",
        "cloud",
        "catalog",
        "aspect"
      ]
    },
    {
      "id": "official_dataplexcataloglistaspecttypesoperator_20260128_160330",
      "component_name": "DataplexCatalogListAspectTypesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogListAspectTypesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "List AspectType resources.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogListAspectTypesOperator`\n\n:param filter_by: Optional. Filter to apply on the list results.\n:param order_by: Optional. Fields to order the results by.\n:param page_size: Optional. Maximum number of AspectTypes to return on the page.\n:param page_token: Optional. Token to retrieve the next page of results.\n:param project_id: Required. The ID",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.964859",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "types",
        "gcp",
        "bigquery",
        "dataplexcataloglistaspecttypesoperator",
        "list",
        "cloud",
        "catalog",
        "aspect"
      ]
    },
    {
      "id": "official_dataplexcatalogupdateaspecttypeoperator_20260128_160330",
      "component_name": "DataplexCatalogUpdateAspectTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogUpdateAspectTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "aspect_type_id",
          "type": "str",
          "required": true
        },
        {
          "name": "aspect_type_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "validate_request",
          "type": "Union",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update an AspectType resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogUpdateAspectTypeOperator`\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param location: Required. The ID of the Google Cloud region that the task belongs to.\n:param update_mask: Optional. Names of fields whose values to overwrite on an entry group.\n    If this parameter is absent or empty",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.965388",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "bigquery",
        "type",
        "cloud",
        "dataplexcatalogupdateaspecttypeoperator",
        "catalog",
        "aspect"
      ]
    },
    {
      "id": "official_dataplexcatalogdeleteaspecttypeoperator_20260128_160330",
      "component_name": "DataplexCatalogDeleteAspectTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogDeleteAspectTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "aspect_type_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete an AspectType resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogDeleteAspectTypeOperator`\n\n:param aspect_type_id: Required. AspectType identifier.\n:param project_id: Required. The ID of the Google Cloud project where the service is used.\n:param location: Required. The ID of the Google Cloud region where the service is used.\n:param gcp_conn_id: Optional. The connection ID to use to connect to Goog",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.965910",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "dataplexcatalogdeleteaspecttypeoperator",
        "gcp",
        "bigquery",
        "delete",
        "type",
        "cloud",
        "catalog",
        "aspect"
      ]
    },
    {
      "id": "official_dataplexcatalogcreateentryoperator_20260128_160330",
      "component_name": "DataplexCatalogCreateEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogCreateEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_group_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_configuration",
          "type": "Union",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_fields",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create an Entry resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogCreateEntryOperator`\n\n:param entry_id: Required. Entry identifier. It has to be unique within an Entry Group.\n    Entries corresponding to Google Cloud resources use an Entry ID format based on `full resource\n    names <https://cloud.google.com/apis/design/resource_names#full_resource_name>`__.\n    The format is a full resource name of th",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.966820",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "create",
        "gcp",
        "bigquery",
        "dataplexcatalogcreateentryoperator",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcataloggetentryoperator_20260128_160330",
      "component_name": "DataplexCatalogGetEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogGetEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_group_id",
          "type": "str",
          "required": true
        },
        {
          "name": "view",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "aspect_types",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "paths",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Get an Entry resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogGetEntryOperator`\n\n:param entry_id: Required. Entry identifier. It has to be unique within an Entry Group.\n    Entries corresponding to Google Cloud resources use an Entry ID format based on `full resource\n    names <https://cloud.google.com/apis/design/resource_names#full_resource_name>`__.\n    The format is a full resource name of the reso",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.967862",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "gcp",
        "bigquery",
        "dataplexcataloggetentryoperator",
        "get",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcataloglistentriesoperator_20260128_160330",
      "component_name": "DataplexCatalogListEntriesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogListEntriesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_group_id",
          "type": "str",
          "required": true
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter_by",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "List Entry resources.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogListEntriesOperator`\n\n:param entry_group_id: Required. EntryGroup resource name to which created Entry belongs to.\n:param filter_by: Optional. A filter on the entries to return. Filters are case-sensitive.\n    You can filter the request by the following fields:\n\n    -  entry_type\n    -  entry_source.display_name\n\n    The comparison operators ",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.968665",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entries",
        "gcp",
        "bigquery",
        "dataplexcataloglistentriesoperator",
        "list",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcatalogsearchentriesoperator_20260128_160330",
      "component_name": "DataplexCatalogSearchEntriesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogSearchEntriesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "query",
          "type": "str",
          "required": true
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "scope",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Search for Entries matching the given query and scope.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogSearchEntriesOperator`\n\n:param query: Required. The query against which entries in scope should be matched. The query\n    syntax is defined in `Search syntax for Dataplex Catalog\n    <https://cloud.google.com/dataplex/docs/search-syntax>`__.\n:param order_by: Optional. Specifies the ordering of results. Support",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.969242",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "dataplexcatalogsearchentriesoperator",
        "google",
        "entries",
        "gcp",
        "bigquery",
        "search",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcataloglookupentryoperator_20260128_160330",
      "component_name": "DataplexCatalogLookupEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogLookupEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_group_id",
          "type": "str",
          "required": true
        },
        {
          "name": "view",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "aspect_types",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "paths",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Look up a single Entry by name using the permission on the source system.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogLookupEntryOperator`\n\n:param entry_id: Required. Entry identifier. It has to be unique within an Entry Group.\n    Entries corresponding to Google Cloud resources use an Entry ID format based on `full resource\n    names <https://cloud.google.com/apis/design/resource_names#full_resource_name>`",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.969778",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "gcp",
        "bigquery",
        "dataplexcataloglookupentryoperator",
        "cloud",
        "lookup",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcatalogupdateentryoperator_20260128_160330",
      "component_name": "DataplexCatalogUpdateEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogUpdateEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_group_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "allow_missing",
          "type": "Union",
          "required": false,
          "default": false
        },
        {
          "name": "delete_missing_aspects",
          "type": "Union",
          "required": false,
          "default": false
        },
        {
          "name": "aspect_keys",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update an Entry resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogUpdateEntryOperator`\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param location: Required. The ID of the Google Cloud region that the task belongs to.\n:param entry_id: Required. Entry identifier. It has to be unique within an Entry Group.\n    Entries corresponding to Google Cloud resources u",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.970340",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "update",
        "entry",
        "gcp",
        "bigquery",
        "dataplexcatalogupdateentryoperator",
        "cloud",
        "catalog"
      ]
    },
    {
      "id": "official_dataplexcatalogdeleteentryoperator_20260128_160330",
      "component_name": "DataplexCatalogDeleteEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexCatalogDeleteEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "DataplexCatalogBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "entry_id",
          "type": "str",
          "required": true
        },
        {
          "name": "entry_group_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from DataplexCatalogBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.triggers.dataplex import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete an Entry resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataplexCatalogDeleteEntryOperator`\n\n:param entry_id: Required. Entry identifier. It has to be unique within an Entry Group.\n    Entries corresponding to Google Cloud resources use an Entry ID format based on `full resource\n    names <https://cloud.google.com/apis/design/resource_names#full_resource_name>`__.\n    The format is a full resource name of th",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:23.970867",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "operator",
        "google",
        "entry",
        "gcp",
        "bigquery",
        "delete",
        "cloud",
        "dataplexcatalogdeleteentryoperator",
        "catalog"
      ]
    },
    {
      "id": "official_dataprepgetjobsforjobgroupoperator_20260128_160330",
      "component_name": "DataprepGetJobsForJobGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprepGetJobsForJobGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "job_group_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataprep_conn_id",
          "type": "str",
          "required": false,
          "default": "dataprep_default"
        },
        {
          "name": "job_group_id",
          "type": "Union",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.hooks.dataprep import GoogleDataprepHook\nfrom airflow.providers.google.cloud.links.dataprep import DataprepFlowLink, DataprepJobGroupLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get information about the batch jobs within a Cloud Dataprep job.\n\nAPI documentation: https://clouddataprep.com/documentation/api#section/Overview.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataprepGetJobsForJobGroupOperator`\n\n:param job_group_id The ID of the job group that will be requests",
      "success_score": 173,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.015324",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "jobs",
        "dataprepgetjobsforjobgroupoperator",
        "bigquery",
        "for",
        "dataprep",
        "get",
        "cloud",
        "group",
        "job"
      ]
    },
    {
      "id": "official_dataprepgetjobgroupoperator_20260128_160330",
      "component_name": "DataprepGetJobGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprepGetJobGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "job_group_id",
          "embed",
          "project_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataprep_conn_id",
          "type": "str",
          "required": false,
          "default": "dataprep_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "job_group_id",
          "type": "Union",
          "required": false
        },
        {
          "name": "embed",
          "type": "str",
          "required": false
        },
        {
          "name": "include_deleted",
          "type": "bool",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.hooks.dataprep import GoogleDataprepHook\nfrom airflow.providers.google.cloud.links.dataprep import DataprepFlowLink, DataprepJobGroupLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get the specified job group.\n\nA job group is a job that is executed from a specific node in a flow.\n\nAPI documentation: https://clouddataprep.com/documentation/api#section/Overview.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataprepGetJobGroupOperator`\n\n:param job_group_id: The ID of the job group that will be requests\n:param embed: Comma-separated list of objects to pull in as part of the response\n:param include_delete",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.015472",
      "relevance_keywords": [
        "gcs",
        "dataprepgetjobgroupoperator",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "dataprep",
        "get",
        "cloud",
        "group",
        "job"
      ]
    },
    {
      "id": "official_datapreprunjobgroupoperator_20260128_160330",
      "component_name": "DataprepRunJobGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprepRunJobGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "body_request"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "dataprep_conn_id",
          "type": "str",
          "required": false,
          "default": "dataprep_default"
        },
        {
          "name": "body_request",
          "type": "dict",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.hooks.dataprep import GoogleDataprepHook\nfrom airflow.providers.google.cloud.links.dataprep import DataprepFlowLink, DataprepJobGroupLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a ``jobGroup``, which launches the specified job as the authenticated user.\n\nThis performs the same action as clicking on the Run Job button in the application.\n\nTo get recipe_id please follow the Dataprep API documentation:\nhttps://clouddataprep.com/documentation/api#operation/runJobGroup.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataprepRunJobGroupOperator`\n\n:param dataprep_conn_id: The Dataprep connection ID\n",
      "success_score": 173,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.015588",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "run",
        "datapreprunjobgroupoperator",
        "dataprep",
        "cloud",
        "group",
        "job"
      ]
    },
    {
      "id": "official_dataprepcopyflowoperator_20260128_160330",
      "component_name": "DataprepCopyFlowOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprepCopyFlowOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "flow_id",
          "name",
          "project_id",
          "description"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "dataprep_conn_id",
          "type": "str",
          "required": false,
          "default": "dataprep_default"
        },
        {
          "name": "flow_id",
          "type": "Union",
          "required": false
        },
        {
          "name": "name",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "description",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "copy_datasources",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.hooks.dataprep import GoogleDataprepHook\nfrom airflow.providers.google.cloud.links.dataprep import DataprepFlowLink, DataprepJobGroupLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a copy of the provided flow id, as well as all contained recipes.\n\n:param dataprep_conn_id: The Dataprep connection ID\n:param flow_id: ID of the flow to be copied\n:param name: Name for the copy of the flow\n:param description: Description of the copy of the flow\n:param copy_datasources: Bool value to define should the copy of data inputs be made or not.",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.015712",
      "relevance_keywords": [
        "dataprepcopyflowoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "dataprep",
        "copy",
        "cloud",
        "flow"
      ]
    },
    {
      "id": "official_dataprepdeleteflowoperator_20260128_160330",
      "component_name": "DataprepDeleteFlowOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprepDeleteFlowOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "flow_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataprep_conn_id",
          "type": "str",
          "required": false,
          "default": "dataprep_default"
        },
        {
          "name": "flow_id",
          "type": "Union",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.hooks.dataprep import GoogleDataprepHook\nfrom airflow.providers.google.cloud.links.dataprep import DataprepFlowLink, DataprepJobGroupLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete the flow with provided id.\n\n:param dataprep_conn_id: The Dataprep connection ID\n:param flow_id: ID of the flow to be copied",
      "success_score": 163,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.015815",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "dataprep",
        "cloud",
        "flow",
        "dataprepdeleteflowoperator"
      ]
    },
    {
      "id": "official_datapreprunflowoperator_20260128_160330",
      "component_name": "DataprepRunFlowOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprepRunFlowOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "flow_id",
          "project_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "flow_id",
          "type": "Union",
          "required": false
        },
        {
          "name": "body_request",
          "type": "dict",
          "required": false
        },
        {
          "name": "dataprep_conn_id",
          "type": "str",
          "required": false,
          "default": "dataprep_default"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.hooks.dataprep import GoogleDataprepHook\nfrom airflow.providers.google.cloud.links.dataprep import DataprepFlowLink, DataprepJobGroupLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Runs the flow with the provided id copy of the provided flow id.\n\n:param dataprep_conn_id: The Dataprep connection ID\n:param flow_id: ID of the flow to be copied\n:param body_request: Body of the POST request to be sent.",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.015920",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "run",
        "dataprep",
        "cloud",
        "datapreprunflowoperator",
        "flow"
      ]
    },
    {
      "id": "official_dataproccreateclusteroperator_20260128_160330",
      "component_name": "DataprocCreateClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocCreateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "cluster_config",
          "virtual_cluster_config",
          "cluster_name",
          "labels",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "cluster_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "virtual_cluster_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "delete_on_error",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "use_if_exists",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "num_retries_if_resource_is_not_ready",
          "type": "int",
          "required": false,
          "default": 0
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "float",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_create_cluster",
          "_delete_cluster",
          "_get_cluster",
          "_handle_error_state",
          "_wait_for_cluster_in_deleting_state",
          "_wait_for_cluster_in_creating_state",
          "_start_cluster",
          "_retry_cluster_creation",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a new cluster on Google Cloud Dataproc.\n\nThe operator will wait until the creation is successful or an error occurs\nin the creation process.\n\nIf the cluster already exists and ``use_if_exists`` is True, then the operator will:\n- if cluster state is ERROR then delete it if specified and raise error\n- if cluster state is CREATING wait for it and then check for ERROR state\n- if cluster state is DELETING wait for it and then create new cluster\n\nPlease refer to\nhttps://cloud.google.com/datapro",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.086231",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "dataproccreateclusteroperator",
        "operator",
        "google",
        "cluster",
        "create",
        "gcp",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocdeleteclusteroperator_20260128_160330",
      "component_name": "DataprocDeleteClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocDeleteClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "cluster_name",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "cluster_uuid",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "float",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "ignore_if_missing",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "_delete_cluster"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a cluster in a project.\n\n:param region: Required. The Cloud Dataproc region in which to handle the request (templated).\n:param cluster_name: Required. The cluster name (templated).\n:param project_id: Optional. The ID of the Google Cloud project that the cluster belongs to (templated).\n:param cluster_uuid: Optional. Specifying the ``cluster_uuid`` means the RPC should fail\n    if cluster with specified UUID does not exist.\n:param request_id: Optional. A unique id used to identify the reque",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.086979",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "cluster",
        "gcp",
        "bigquery",
        "delete",
        "dataprocdeleteclusteroperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocstartclusteroperator_20260128_160330",
      "component_name": "DataprocStartClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocStartClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_DataprocStartStopClusterBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from _DataprocStartStopClusterBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "operator_extra_links"
        ],
        "methods": [
          "execute",
          "_check_desired_cluster_state",
          "_get_operation"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Start a cluster in a project.",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.087654",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "cluster",
        "start",
        "gcp",
        "bigquery",
        "dataprocstartclusteroperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocstopclusteroperator_20260128_160330",
      "component_name": "DataprocStopClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocStopClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_DataprocStartStopClusterBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from _DataprocStartStopClusterBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [],
        "methods": [
          "execute",
          "_check_desired_cluster_state",
          "_get_operation"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Stop a cluster in a project.",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.088192",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "stop",
        "cluster",
        "gcp",
        "bigquery",
        "cloud",
        "dataprocstopclusteroperator"
      ]
    },
    {
      "id": "official_dataprocjobbaseoperator_20260128_160330",
      "component_name": "DataprocJobBaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocJobBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "job_name",
          "type": "str",
          "required": false,
          "default": "{{task.task_id}}_{{ds_nodash}}"
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": false,
          "default": "cluster-1"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "dataproc_properties",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "dataproc_jars",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_error_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "asynchronous",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "job_type",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "create_job_template",
          "_generate_job_template",
          "execute",
          "execute_complete",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Base class for operators that launch job on DataProc.\n\n:param region: The specified region where the dataproc cluster is created.\n:param job_name: The job name used in the DataProc cluster. This name by default\n    is the task_id appended with the execution data, but can be templated. The\n    name will always be appended with a random number to avoid name clashes.\n:param cluster_name: The name of the DataProc cluster.\n:param project_id: The ID of the Google Cloud project the cluster belongs to,\n",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.088748",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "dataprocjobbaseoperator",
        "google",
        "gcp",
        "bigquery",
        "base",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_dataproccreateworkflowtemplateoperator_20260128_160330",
      "component_name": "DataprocCreateWorkflowTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocCreateWorkflowTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "region",
          "template",
          "gcp_conn_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "template",
          "type": "dict",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates new workflow template.\n\n:param project_id: Optional. The ID of the Google Cloud project the cluster belongs to.\n:param region: Required. The Cloud Dataproc region in which to handle the request.\n:param template: The Dataproc workflow template to create. If a dict is provided,\n    it must be of the same form as the protobuf message WorkflowTemplate.\n:param retry: A retry object used to retry requests. If ``None`` is specified, requests will not be\n    retried.\n:param timeout: The amount o",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.089278",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "template",
        "bigquery",
        "cloud",
        "dataproccreateworkflowtemplateoperator",
        "workflow"
      ]
    },
    {
      "id": "official_dataprocinstantiateworkflowtemplateoperator_20260128_160330",
      "component_name": "DataprocInstantiateWorkflowTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocInstantiateWorkflowTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "template_id",
          "gcp_conn_id",
          "impersonation_chain",
          "request_id",
          "parameters"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "template_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "parameters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "cancel_on_kill",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Instantiate a WorkflowTemplate on Google Cloud Dataproc.\n\nThe operator will wait until the WorkflowTemplate is finished executing.\n\n.. seealso::\n    Please refer to:\n    https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.workflowTemplates/instantiate\n\n:param template_id: The id of the template. (templated)\n:param project_id: The ID of the google cloud project in which\n    the template runs\n:param region: The specified region where the dataproc cluster is created.\n:param pa",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.089835",
      "relevance_keywords": [
        "dataprocinstantiateworkflowtemplateoperator",
        "dataproc",
        "instantiate",
        "gcs",
        "operator",
        "google",
        "gcp",
        "template",
        "bigquery",
        "cloud",
        "workflow"
      ]
    },
    {
      "id": "official_dataprocinstantiateinlineworkflowtemplateoperator_20260128_160330",
      "component_name": "DataprocInstantiateInlineWorkflowTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocInstantiateInlineWorkflowTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "template",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "template",
          "type": "dict",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "cancel_on_kill",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "openlineage_inject_parent_job_info",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "openlineage_inject_transport_info",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "on_kill",
          "_inject_openlineage_properties_into_dataproc_workflow_template"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Instantiate a WorkflowTemplate Inline on Google Cloud Dataproc.\n\nThe operator will wait until the WorkflowTemplate is finished executing.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataprocInstantiateInlineWorkflowTemplateOperator`\n\n    For more detail on about instantiate inline have a look at the reference:\n    https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.workflowTemplates/instantiateInline",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.090379",
      "relevance_keywords": [
        "dataproc",
        "instantiate",
        "gcs",
        "operator",
        "google",
        "gcp",
        "template",
        "bigquery",
        "inline",
        "cloud",
        "dataprocinstantiateinlineworkflowtemplateoperator",
        "workflow"
      ]
    },
    {
      "id": "official_dataprocsubmitjoboperator_20260128_160330",
      "component_name": "DataprocSubmitJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocSubmitJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "job",
          "gcp_conn_id",
          "impersonation_chain",
          "request_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job",
          "type": "dict",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "asynchronous",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "cancel_on_kill",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "wait_timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "openlineage_inject_parent_job_info",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "on_kill",
          "_inject_openlineage_properties_into_dataproc_job"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Submit a job to a cluster.\n\n:param project_id: Optional. The ID of the Google Cloud project that the job belongs to.\n:param region: Required. The Cloud Dataproc region in which to handle the request.\n:param job: Required. The job resource.\n    If a dict is provided, it must be of the same form as the protobuf message\n    :class:`~google.cloud.dataproc_v1.types.Job`.\n    For the complete list of supported job types and their configurations please take a look here\n    https://cloud.google.com/data",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.090914",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "gcp",
        "submit",
        "bigquery",
        "dataprocsubmitjoboperator",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_dataprocupdateclusteroperator_20260128_160330",
      "component_name": "DataprocUpdateClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocUpdateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "cluster_name",
          "cluster",
          "region",
          "request_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_name",
          "type": "str",
          "required": false
        },
        {
          "name": "cluster",
          "type": "Union",
          "required": false
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false
        },
        {
          "name": "graceful_decommission_timeout",
          "type": "Union",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update a cluster in a project.\n\n:param region: Required. The Cloud Dataproc region in which to handle the request.\n:param project_id: Optional. The ID of the Google Cloud project the cluster belongs to.\n:param cluster_name: Required. The cluster name.\n:param cluster: Required. The changes to the cluster.\n\n    If a dict is provided, it must be of the same form as the protobuf message\n    :class:`~google.cloud.dataproc_v1.types.Cluster`\n:param update_mask: Required. Specifies the path, relative to",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.091479",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "update",
        "cluster",
        "gcp",
        "bigquery",
        "dataprocupdateclusteroperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocdiagnoseclusteroperator_20260128_160330",
      "component_name": "DataprocDiagnoseClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocDiagnoseClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "cluster_name",
          "gcp_conn_id",
          "impersonation_chain",
          "tarball_gcs_dir",
          "diagnosis_interval",
          "jobs",
          "yarn_application_ids"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "tarball_gcs_dir",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "diagnosis_interval",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "jobs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "yarn_application_ids",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "float",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Diagnose a cluster in a project.\n\nAfter the operation completes, the response contains the Cloud Storage URI of the diagnostic output report containing a summary of collected diagnostics.\n\n:param region: Required. The Cloud Dataproc region in which to handle the request (templated).\n:param project_id: Optional. The ID of the Google Cloud project that the cluster belongs to (templated).\n:param cluster_name: Required. The cluster name (templated).\n:param tarball_gcs_dir:  The output Cloud Storage ",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.092034",
      "relevance_keywords": [
        "dataprocdiagnoseclusteroperator",
        "dataproc",
        "gcs",
        "operator",
        "google",
        "cluster",
        "diagnose",
        "gcp",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_dataproccreatebatchoperator_20260128_160330",
      "component_name": "DataprocCreateBatchOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocCreateBatchOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "batch",
          "batch_id",
          "region",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "batch",
          "type": "Union",
          "required": false
        },
        {
          "name": "batch_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "num_retries_if_resource_is_not_ready",
          "type": "int",
          "required": false,
          "default": 0
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "result_retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "asynchronous",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "polling_interval_seconds",
          "type": "int",
          "required": false,
          "default": 5
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "hook",
          "execute_complete",
          "on_kill",
          "handle_batch_status",
          "retry_batch_creation",
          "_inject_openlineage_properties_into_dataproc_batch",
          "__update_batch_labels"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a batch workload.\n\n:param project_id: Optional. The ID of the Google Cloud project that the cluster belongs to. (templated)\n:param region: Required. The Cloud Dataproc region in which to handle the request. (templated)\n:param batch: Required. The batch to create. (templated)\n:param batch_id: Required. The ID to use for the batch, which will become the final component\n    of the batch's resource name.\n    This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/. (templated)\n:",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.092583",
      "relevance_keywords": [
        "dataproc",
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "dataproccreatebatchoperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocdeletebatchoperator_20260128_160330",
      "component_name": "DataprocDeleteBatchOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocDeleteBatchOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "batch_id",
          "region",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "batch_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete the batch workload resource.\n\n:param batch_id: Required. The ID to use for the batch, which will become the final component\n    of the batch's resource name.\n    This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.\n:param region: Required. The Cloud Dataproc region in which to handle the request.\n:param project_id: Optional. The ID of the Google Cloud project that the cluster belongs to.\n:param retry: A retry object used to retry requests. If ``None`` is specified, requ",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.093151",
      "relevance_keywords": [
        "dataproc",
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataprocdeletebatchoperator",
        "bigquery",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocgetbatchoperator_20260128_160330",
      "component_name": "DataprocGetBatchOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocGetBatchOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "batch_id",
          "region",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "batch_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Get the batch workload resource representation.\n\n:param batch_id: Required. The ID to use for the batch, which will become the final component\n    of the batch's resource name.\n    This value must be 4-63 characters. Valid characters are /[a-z][0-9]-/.\n:param region: Required. The Cloud Dataproc region in which to handle the request.\n:param project_id: Optional. The ID of the Google Cloud project that the cluster belongs to.\n:param retry: A retry object used to retry requests. If ``None`` is spe",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.094165",
      "relevance_keywords": [
        "dataproc",
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "get",
        "cloud",
        "dataprocgetbatchoperator"
      ]
    },
    {
      "id": "official_dataproclistbatchesoperator_20260128_160330",
      "component_name": "DataprocListBatchesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocListBatchesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "region",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "List batch workloads.\n\n:param region: Required. The Cloud Dataproc region in which to handle the request.\n:param project_id: Optional. The ID of the Google Cloud project that the cluster belongs to.\n:param page_size: Optional. The maximum number of batches to return in each response. The service may\n    return fewer than this value. The default page size is 20; the maximum page size is 1000.\n:param page_token: Optional. A page token received from a previous ``ListBatches`` call.\n    Provide this",
      "success_score": 202,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.094845",
      "relevance_keywords": [
        "batches",
        "dataproclistbatchesoperator",
        "dataproc",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_dataproccanceloperationoperator_20260128_160330",
      "component_name": "DataprocCancelOperationOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocCancelOperationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "operation_name",
          "region",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "operation_name",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport inspect\nimport re\nimport time\nimport warnings\nfrom collections.abc import MutableSequence, Sequence\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom functools import cached_property",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Cancel the batch workload resource.\n\n:param operation_name: Required. The name of the operation resource to be cancelled.\n:param region: Required. The Cloud Dataproc region in which to handle the request.\n:param project_id: Optional. The ID of the Google Cloud project that the cluster belongs to.\n:param retry: A retry object used to retry requests. If ``None`` is specified, requests will not be\n    retried.\n:param timeout: The amount of time, in seconds, to wait for the request to complete. Note",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.095545",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "operation",
        "gcp",
        "cancel",
        "bigquery",
        "dataproccanceloperationoperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocmetastorecreatebackupoperator_20260128_160330",
      "component_name": "DataprocMetastoreCreateBackupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreCreateBackupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "backup",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "backup",
          "type": "Union",
          "required": false
        },
        {
          "name": "backup_id",
          "type": "str",
          "required": false
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry, exponential_sleep_generator\nfrom google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity\nfrom google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a new backup in a given project and location.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param service_id:  Required. The ID of the metastore service, which is used as the final component of\n    the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin\n    with a letter, end with a letter or number, and consist",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.137081",
      "relevance_keywords": [
        "dataproc",
        "backup",
        "gcs",
        "dataprocmetastorecreatebackupoperator",
        "operator",
        "google",
        "create",
        "gcp",
        "metastore",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocmetastorecreatemetadataimportoperator_20260128_160330",
      "component_name": "DataprocMetastoreCreateMetadataImportOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreCreateMetadataImportOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "metadata_import",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "metadata_import",
          "type": "Union",
          "required": false
        },
        {
          "name": "metadata_import_id",
          "type": "str",
          "required": false
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry, exponential_sleep_generator\nfrom google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity\nfrom google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a new MetadataImport in a given project and location.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param service_id:  Required. The ID of the metastore service, which is used as the final component of\n    the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin\n    with a letter, end with a letter or number, and",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.137416",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "dataprocmetastorecreatemetadataimportoperator",
        "operator",
        "google",
        "import",
        "create",
        "gcp",
        "metastore",
        "bigquery",
        "metadata",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocmetastorecreateserviceoperator_20260128_160330",
      "component_name": "DataprocMetastoreCreateServiceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreCreateServiceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "service",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "service",
          "type": "Union",
          "required": false
        },
        {
          "name": "service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry, exponential_sleep_generator\nfrom google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity\nfrom google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a metastore service in a project and location.\n\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param service:  Required. The Metastore service to create. The ``name`` field is ignored. The ID of\n    the created metastore service must be provided in the request's ``service_id`` field.\n\n    This corresponds to the ``service`` field on the ``request`` instanc",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.137711",
      "relevance_keywords": [
        "dataprocmetastorecreateserviceoperator",
        "dataproc",
        "gcs",
        "operator",
        "google",
        "service",
        "create",
        "gcp",
        "metastore",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocmetastoredeletebackupoperator_20260128_160330",
      "component_name": "DataprocMetastoreDeleteBackupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreDeleteBackupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "backup_id",
          "type": "str",
          "required": false
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry, exponential_sleep_generator\nfrom google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity\nfrom google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete a single backup.\n\n:param project_id: Required. The ID of the Google Cloud project that the backup belongs to.\n:param region: Required. The ID of the Google Cloud region that the backup belongs to.\n:param service_id: Required. The ID of the metastore service, which is used as the final component of\n    the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin\n    with a letter, end with a letter or number, and consist of alphanumeric ASCII character",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.137990",
      "relevance_keywords": [
        "dataproc",
        "backup",
        "gcs",
        "operator",
        "google",
        "gcp",
        "metastore",
        "bigquery",
        "delete",
        "cloud",
        "dataprocmetastoredeletebackupoperator"
      ]
    },
    {
      "id": "official_dataprocmetastoredeleteserviceoperator_20260128_160330",
      "component_name": "DataprocMetastoreDeleteServiceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreDeleteServiceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry, exponential_sleep_generator\nfrom google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity\nfrom google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete a single service.\n\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param service_id:  Required. The ID of the metastore service, which is used as the final component of\n    the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin\n    with a letter, end with a letter or number, and consist of alphanumeric ASCII chara",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.138258",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "service",
        "gcp",
        "metastore",
        "bigquery",
        "dataprocmetastoredeleteserviceoperator",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocmetastoreexportmetadataoperator_20260128_160330",
      "component_name": "DataprocMetastoreExportMetadataOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreExportMetadataOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "destination_gcs_folder",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "database_dump_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry, exponential_sleep_generator\nfrom google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity\nfrom google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute",
          "_get_uri_from_destination",
          "_wait_for_export_metadata"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Export metadata from a service.\n\n:param destination_gcs_folder: A Cloud Storage URI of a folder, in the format\n    ``gs://<bucket_name>/<path_inside_bucket>``. A sub-folder\n    ``<export_folder>`` containing exported files will be\n    created below it.\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param service_id:  Required. The ID of the metastore service, whi",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.138547",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "gcp",
        "export",
        "metastore",
        "bigquery",
        "dataprocmetastoreexportmetadataoperator",
        "metadata",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocmetastoregetserviceoperator_20260128_160330",
      "component_name": "DataprocMetastoreGetServiceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreGetServiceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry, exponential_sleep_generator\nfrom google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity\nfrom google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Get the details of a single service.\n\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param service_id:  Required. The ID of the metastore service, which is used as the final component of\n    the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin\n    with a letter, end with a letter or number, and consist of alphanumeric",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.138821",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "dataprocmetastoregetserviceoperator",
        "operator",
        "google",
        "service",
        "gcp",
        "metastore",
        "bigquery",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocmetastorelistbackupsoperator_20260128_160330",
      "component_name": "DataprocMetastoreListBackupsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreListBackupsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry, exponential_sleep_generator\nfrom google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity\nfrom google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "List backups in a service.\n\n:param project_id: Required. The ID of the Google Cloud project that the backup belongs to.\n:param region: Required. The ID of the Google Cloud region that the backup belongs to.\n:param service_id: Required. The ID of the metastore service, which is used as the final component of\n    the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin\n    with a letter, end with a letter or number, and consist of alphanumeric ASCII charac",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.139092",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "gcp",
        "metastore",
        "bigquery",
        "dataprocmetastorelistbackupsoperator",
        "list",
        "backups",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocmetastorerestoreserviceoperator_20260128_160330",
      "component_name": "DataprocMetastoreRestoreServiceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreRestoreServiceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "backup_project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "backup_region",
          "type": "str",
          "required": false
        },
        {
          "name": "backup_service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "backup_id",
          "type": "str",
          "required": false
        },
        {
          "name": "restore_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry, exponential_sleep_generator\nfrom google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity\nfrom google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute",
          "_wait_for_restore_service"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Restore a service from a backup.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param service_id: Required. The ID of the metastore service, which is used as the final component of\n    the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin\n    with a letter, end with a letter or number, and consist of alphanumeric ASCI",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.139368",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "restore",
        "service",
        "gcp",
        "metastore",
        "bigquery",
        "dataprocmetastorerestoreserviceoperator",
        "cloud"
      ]
    },
    {
      "id": "official_dataprocmetastoreupdateserviceoperator_20260128_160330",
      "component_name": "DataprocMetastoreUpdateServiceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreUpdateServiceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "service_id",
          "type": "str",
          "required": false
        },
        {
          "name": "service",
          "type": "Union",
          "required": false
        },
        {
          "name": "update_mask",
          "type": "FieldMask",
          "required": false
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry, exponential_sleep_generator\nfrom google.cloud.metastore_v1 import MetadataExport, MetadataManagementActivity\nfrom google.cloud.metastore_v1.types import Backup, MetadataImport, Service",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Update the parameters of a single service.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param service_id:  Required. The ID of the metastore service, which is used as the final component of\n    the metastore service's name. This value must be between 2 and 63 characters long inclusive, begin\n    with a letter, end with a letter or number, and consist of alphan",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.139700",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "operator",
        "google",
        "update",
        "service",
        "gcp",
        "metastore",
        "bigquery",
        "cloud",
        "dataprocmetastoreupdateserviceoperator"
      ]
    },
    {
      "id": "official_clouddatastoreexportentitiesoperator_20260128_160330",
      "component_name": "CloudDatastoreExportEntitiesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDatastoreExportEntitiesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "bucket",
          "namespace",
          "entity_filter",
          "labels",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "namespace",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "datastore_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "cloud_storage_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "entity_filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "polling_interval_in_seconds",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "overwrite_existing",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.datastore import DatastoreHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.links.datastore import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Export entities from Google Cloud Datastore to Cloud Storage.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDatastoreExportEntitiesOperator`\n\n.. seealso::\n    https://cloud.google.com/datastore/docs/export-import-entities\n\n:param bucket: name of the cloud storage bucket to back up data\n:param namespace: optional namespace path in the specified Cloud Storage bucket\n    to back up data. If this namespace does not exist i",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.178201",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "export",
        "bigquery",
        "clouddatastoreexportentitiesoperator",
        "entities",
        "datastore",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatastoreimportentitiesoperator_20260128_160330",
      "component_name": "CloudDatastoreImportEntitiesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDatastoreImportEntitiesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "bucket",
          "file",
          "namespace",
          "entity_filter",
          "labels",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "file",
          "type": "str",
          "required": false
        },
        {
          "name": "namespace",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "entity_filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "datastore_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "polling_interval_in_seconds",
          "type": "float",
          "required": false,
          "default": 10
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.datastore import DatastoreHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.links.datastore import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Import entities from Cloud Storage to Google Cloud Datastore.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDatastoreImportEntitiesOperator`\n\n.. seealso::\n    https://cloud.google.com/datastore/docs/export-import-entities\n\n:param bucket: container in Cloud Storage to store data\n:param file: path of the backup metadata file in the specified Cloud Storage bucket.\n    It should have the extension .overall_export_metadata\n",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.178406",
      "relevance_keywords": [
        "gcs",
        "import",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "entities",
        "datastore",
        "cloud",
        "clouddatastoreimportentitiesoperator"
      ]
    },
    {
      "id": "official_clouddatastoreallocateidsoperator_20260128_160330",
      "component_name": "CloudDatastoreAllocateIdsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDatastoreAllocateIdsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "partial_keys",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "partial_keys",
          "type": "list",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.datastore import DatastoreHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.links.datastore import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Allocate IDs for incomplete keys. Return list of keys.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDatastoreAllocateIdsOperator`\n\n.. seealso::\n    https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n\n:param partial_keys: a list of partial keys.\n:param project_id: Google Cloud project ID against which to make the request.\n:param gcp_conn_id: The connection ID to use connecting to Google Clou",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.178567",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "ids",
        "gcp",
        "bigquery",
        "allocate",
        "clouddatastoreallocateidsoperator",
        "datastore",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatastorebegintransactionoperator_20260128_160330",
      "component_name": "CloudDatastoreBeginTransactionOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDatastoreBeginTransactionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "transaction_options",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "transaction_options",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.datastore import DatastoreHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.links.datastore import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Begins a new transaction. Returns a transaction handle.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDatastoreBeginTransactionOperator`\n\n.. seealso::\n    https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction\n\n:param transaction_options: Options for a new transaction.\n:param project_id: Google Cloud project ID against which to make the request.\n:param gcp_conn_id: The connection ID to use ",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.178719",
      "relevance_keywords": [
        "clouddatastorebegintransactionoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "datastore",
        "transaction",
        "cloud",
        "begin"
      ]
    },
    {
      "id": "official_clouddatastorecommitoperator_20260128_160330",
      "component_name": "CloudDatastoreCommitOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDatastoreCommitOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "body",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.datastore import DatastoreHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.links.datastore import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Commit a transaction, optionally creating, deleting or modifying some entities.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDatastoreCommitOperator`\n\n.. seealso::\n    https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit\n\n:param body: the body of the commit request.\n:param project_id: Google Cloud project ID against which to make the request.\n:param gcp_conn_id: The connection ID to use connecting",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.178867",
      "relevance_keywords": [
        "commit",
        "gcs",
        "operator",
        "google",
        "clouddatastorecommitoperator",
        "gcp",
        "bigquery",
        "datastore",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatastorerollbackoperator_20260128_160330",
      "component_name": "CloudDatastoreRollbackOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDatastoreRollbackOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "transaction",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "transaction",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.datastore import DatastoreHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.links.datastore import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Roll back a transaction.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDatastoreRollbackOperator`\n\n.. seealso::\n    https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback\n\n:param transaction: the transaction to roll back.\n:param project_id: Google Cloud project ID against which to make the request.\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud.\n:param impersonation_chain: ",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.179009",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "rollback",
        "gcp",
        "bigquery",
        "clouddatastorerollbackoperator",
        "datastore",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatastorerunqueryoperator_20260128_160330",
      "component_name": "CloudDatastoreRunQueryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDatastoreRunQueryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "body",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.datastore import DatastoreHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.links.datastore import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Run a query for entities. Returns the batch of query results.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDatastoreRunQueryOperator`\n\n.. seealso::\n    https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery\n\n:param body: the body of the query request.\n:param project_id: Google Cloud project ID against which to make the request.\n:param gcp_conn_id: The connection ID to use connecting to Google Clou",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.179154",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "clouddatastorerunqueryoperator",
        "bigquery",
        "run",
        "datastore",
        "cloud",
        "query"
      ]
    },
    {
      "id": "official_clouddatastoregetoperationoperator_20260128_160330",
      "component_name": "CloudDatastoreGetOperationOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDatastoreGetOperationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "name",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.datastore import DatastoreHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.links.datastore import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets the latest state of a long-running operation.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDatastoreGetOperationOperator`\n\n.. seealso::\n    https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get\n\n:param name: the name of the operation resource.\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud.\n:param impersonation_chain: Optional service account to impersonat",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.179296",
      "relevance_keywords": [
        "clouddatastoregetoperationoperator",
        "gcs",
        "operator",
        "google",
        "operation",
        "gcp",
        "bigquery",
        "datastore",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_clouddatastoredeleteoperationoperator_20260128_160330",
      "component_name": "CloudDatastoreDeleteOperationOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDatastoreDeleteOperationOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "name",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.datastore import DatastoreHook\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.links.datastore import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes the long-running operation.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDatastoreDeleteOperationOperator`\n\n.. seealso::\n    https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete\n\n:param name: the name of the operation resource.\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud.\n:param impersonation_chain: Optional service account to impersonate using s",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.179448",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "operation",
        "gcp",
        "bigquery",
        "delete",
        "datastore",
        "clouddatastoredeleteoperationoperator",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpcanceldlpjoboperator_20260128_160330",
      "component_name": "CloudDLPCancelDLPJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPCancelDLPJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dlp_job_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dlp_job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Starts asynchronous cancellation on a long-running DlpJob.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPCancelDLPJobOperator`\n\n:param dlp_job_id: ID of the DLP job resource to be cancelled.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default project_id\n    from the Google Cloud connection is used.\n:param retry: (Optional) A retry object use",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.232407",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "cancel",
        "bigquery",
        "cloud",
        "job",
        "clouddlpcanceldlpjoboperator"
      ]
    },
    {
      "id": "official_clouddlpcreatedeidentifytemplateoperator_20260128_160330",
      "component_name": "CloudDLPCreateDeidentifyTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPCreateDeidentifyTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "organization_id",
          "project_id",
          "deidentify_template",
          "template_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "deidentify_template",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "template_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a deidentify template to reuse frequently-used configurations for content, images, and storage.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPCreateDeidentifyTemplateOperator`\n\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.233339",
      "relevance_keywords": [
        "clouddlpcreatedeidentifytemplateoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "deidentify",
        "template",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpcreatedlpjoboperator_20260128_160330",
      "component_name": "CloudDLPCreateDLPJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPCreateDLPJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "inspect_job",
          "risk_job",
          "job_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "inspect_job",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "risk_job",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "wait_until_finished",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new job to inspect storage or calculate risk metrics.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPCreateDLPJobOperator`\n\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param inspect_job: (Optional) The configuration for the inspect job.\n:param risk_job: (Optional) The",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.234260",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "clouddlpcreatedlpjoboperator",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddlpcreateinspecttemplateoperator_20260128_160330",
      "component_name": "CloudDLPCreateInspectTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPCreateInspectTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "organization_id",
          "project_id",
          "inspect_template",
          "template_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "inspect_template",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "template_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create an InspectTemplate to reuse frequently-used configurations for content, images, and storage.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPCreateInspectTemplateOperator`\n\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.235199",
      "relevance_keywords": [
        "inspect",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "template",
        "bigquery",
        "clouddlpcreateinspecttemplateoperator",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpcreatejobtriggeroperator_20260128_160330",
      "component_name": "CloudDLPCreateJobTriggerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPCreateJobTriggerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "job_trigger",
          "trigger_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "job_trigger",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "trigger_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a job trigger to run DLP actions such as scanning storage for sensitive info on a set schedule.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPCreateJobTriggerOperator`\n\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param job_trigger: (Optional) The JobTrigger to create.\n:",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.236208",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "trigger",
        "create",
        "gcp",
        "bigquery",
        "clouddlpcreatejobtriggeroperator",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddlpcreatestoredinfotypeoperator_20260128_160330",
      "component_name": "CloudDLPCreateStoredInfoTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPCreateStoredInfoTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "organization_id",
          "project_id",
          "config",
          "stored_info_type_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "stored_info_type_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a pre-built stored infoType to be used for inspection.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPCreateStoredInfoTypeOperator`\n\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the parent resource is\n    a project inst",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.236837",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "stored",
        "create",
        "gcp",
        "bigquery",
        "info",
        "clouddlpcreatestoredinfotypeoperator",
        "type",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpdeidentifycontentoperator_20260128_160330",
      "component_name": "CloudDLPDeidentifyContentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPDeidentifyContentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "deidentify_config",
          "inspect_config",
          "item",
          "inspect_template_name",
          "deidentify_template_name",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "deidentify_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "inspect_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "item",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "inspect_template_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deidentify_template_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "De-identifies potentially sensitive info from a content item; limits input size and output size.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPDeidentifyContentOperator`\n\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param deidentify_config: (Optional) Configuration for the de-i",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.237429",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "deidentify",
        "bigquery",
        "clouddlpdeidentifycontentoperator",
        "content",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpdeletedeidentifytemplateoperator_20260128_160330",
      "component_name": "CloudDLPDeleteDeidentifyTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPDeleteDeidentifyTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "template_id",
          "organization_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "template_id",
          "type": "str",
          "required": false
        },
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a DeidentifyTemplate.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPDeleteDeidentifyTemplateOperator`\n\n:param template_id: The ID of deidentify template to be deleted.\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the p",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.237975",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "deidentify",
        "template",
        "bigquery",
        "delete",
        "clouddlpdeletedeidentifytemplateoperator",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpdeletedlpjoboperator_20260128_160330",
      "component_name": "CloudDLPDeleteDLPJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPDeleteDLPJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dlp_job_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dlp_job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a long-running DlpJob.\n\nThis method indicates that the client is no longer interested\nin the DlpJob result. The job will be cancelled if possible.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPDeleteDLPJobOperator`\n\n:param dlp_job_id: The ID of the DLP job resource to be deleted.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n  ",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.238531",
      "relevance_keywords": [
        "gcs",
        "clouddlpdeletedlpjoboperator",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddlpdeleteinspecttemplateoperator_20260128_160330",
      "component_name": "CloudDLPDeleteInspectTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPDeleteInspectTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "template_id",
          "organization_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "template_id",
          "type": "str",
          "required": false
        },
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes an InspectTemplate.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPDeleteInspectTemplateOperator`\n\n:param template_id: The ID of the inspect template to be deleted.\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the paren",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.239061",
      "relevance_keywords": [
        "inspect",
        "gcs",
        "operator",
        "google",
        "gcp",
        "template",
        "bigquery",
        "clouddlpdeleteinspecttemplateoperator",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpdeletejobtriggeroperator_20260128_160330",
      "component_name": "CloudDLPDeleteJobTriggerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPDeleteJobTriggerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "job_trigger_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_trigger_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a job trigger.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPDeleteJobTriggerOperator`\n\n:param job_trigger_id: The ID of the DLP job trigger to be deleted.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param retry: (Optional) A retry object used to retry requests.\n    If",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.239581",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "trigger",
        "gcp",
        "bigquery",
        "delete",
        "clouddlpdeletejobtriggeroperator",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddlpdeletestoredinfotypeoperator_20260128_160330",
      "component_name": "CloudDLPDeleteStoredInfoTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPDeleteStoredInfoTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "stored_info_type_id",
          "organization_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "stored_info_type_id",
          "type": "str",
          "required": false
        },
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a stored infoType.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPDeleteStoredInfoTypeOperator`\n\n:param stored_info_type_id: The ID of the stored info type to be deleted.\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.240135",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "stored",
        "gcp",
        "bigquery",
        "delete",
        "info",
        "clouddlpdeletestoredinfotypeoperator",
        "type",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpgetdeidentifytemplateoperator_20260128_160330",
      "component_name": "CloudDLPGetDeidentifyTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPGetDeidentifyTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "template_id",
          "organization_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "template_id",
          "type": "str",
          "required": false
        },
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets a DeidentifyTemplate.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPGetDeidentifyTemplateOperator`\n\n:param template_id: The ID of deidentify template to be read.\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the parent res",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.240666",
      "relevance_keywords": [
        "gcs",
        "clouddlpgetdeidentifytemplateoperator",
        "operator",
        "google",
        "gcp",
        "deidentify",
        "template",
        "bigquery",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpgetdlpjoboperator_20260128_160330",
      "component_name": "CloudDLPGetDLPJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPGetDLPJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dlp_job_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dlp_job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets the latest state of a long-running DlpJob.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPGetDLPJobOperator`\n\n:param dlp_job_id: The ID of the DLP job resource to be read.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param retry: (Optional) A retry object used to retry requ",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.241179",
      "relevance_keywords": [
        "gcs",
        "clouddlpgetdlpjoboperator",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "get",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddlpgetinspecttemplateoperator_20260128_160330",
      "component_name": "CloudDLPGetInspectTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPGetInspectTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "template_id",
          "organization_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "template_id",
          "type": "str",
          "required": false
        },
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets an InspectTemplate.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPGetInspectTemplateOperator`\n\n:param template_id: The ID of inspect template to be read.\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the parent resource is",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.241691",
      "relevance_keywords": [
        "inspect",
        "gcs",
        "clouddlpgetinspecttemplateoperator",
        "operator",
        "google",
        "template",
        "gcp",
        "bigquery",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpgetdlpjobtriggeroperator_20260128_160330",
      "component_name": "CloudDLPGetDLPJobTriggerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPGetDLPJobTriggerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "job_trigger_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_trigger_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets a job trigger.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPGetDLPJobTriggerOperator`\n\n:param job_trigger_id: The ID of the DLP job trigger to be read.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param retry: (Optional) A retry object used to retry requests.\n    If None ",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.242208",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "trigger",
        "gcp",
        "clouddlpgetdlpjobtriggeroperator",
        "bigquery",
        "get",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddlpgetstoredinfotypeoperator_20260128_160330",
      "component_name": "CloudDLPGetStoredInfoTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPGetStoredInfoTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "stored_info_type_id",
          "organization_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "stored_info_type_id",
          "type": "str",
          "required": false
        },
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets a stored infoType.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPGetStoredInfoTypeOperator`\n\n:param stored_info_type_id: The ID of the stored info type to be read.\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the parent r",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.242727",
      "relevance_keywords": [
        "clouddlpgetstoredinfotypeoperator",
        "gcs",
        "operator",
        "google",
        "stored",
        "gcp",
        "bigquery",
        "info",
        "get",
        "type",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpinspectcontentoperator_20260128_160330",
      "component_name": "CloudDLPInspectContentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPInspectContentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "inspect_config",
          "item",
          "inspect_template_name",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "inspect_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "item",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "inspect_template_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Finds potentially sensitive info in content; limits input size, processing time, and output size.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPInspectContentOperator`\n\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param inspect_config: (Optional) Configuration for the inspector",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.243249",
      "relevance_keywords": [
        "inspect",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "clouddlpinspectcontentoperator",
        "content",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlplistdeidentifytemplatesoperator_20260128_160330",
      "component_name": "CloudDLPListDeidentifyTemplatesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPListDeidentifyTemplatesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "organization_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists DeidentifyTemplates.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPListDeidentifyTemplatesOperator`\n\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the parent resource is\n    a project instead of an organization.\n:param pa",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.243758",
      "relevance_keywords": [
        "gcs",
        "templates",
        "operator",
        "google",
        "gcp",
        "deidentify",
        "bigquery",
        "list",
        "clouddlplistdeidentifytemplatesoperator",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlplistdlpjobsoperator_20260128_160330",
      "component_name": "CloudDLPListDLPJobsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPListDLPJobsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "results_filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists DlpJobs that match the specified filter in the request.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPListDLPJobsOperator`\n\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param results_filter: (Optional) Filter used to specify a subset of results.\n:param page_size: (Optiona",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.244277",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "jobs",
        "bigquery",
        "list",
        "cloud",
        "clouddlplistdlpjobsoperator"
      ]
    },
    {
      "id": "official_clouddlplistinfotypesoperator_20260128_160330",
      "component_name": "CloudDLPListInfoTypesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPListInfoTypesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "language_code",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "language_code",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "results_filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Returns a list of the sensitive information types that the DLP API supports.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPListInfoTypesOperator`\n\n:param language_code: (Optional) Optional BCP-47 language code for localized infoType\n    friendly names. If omitted, or if localized strings are not available, en-US\n    strings will be returned.\n:param results_filter: (Optional) Filter used to specify a subset of result",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.244801",
      "relevance_keywords": [
        "clouddlplistinfotypesoperator",
        "gcs",
        "operator",
        "google",
        "types",
        "gcp",
        "bigquery",
        "info",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlplistinspecttemplatesoperator_20260128_160330",
      "component_name": "CloudDLPListInspectTemplatesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPListInspectTemplatesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "organization_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists InspectTemplates.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPListInspectTemplatesOperator`\n\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the parent resource is\n    a project instead of an organization.\n:param page_siz",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.245330",
      "relevance_keywords": [
        "inspect",
        "gcs",
        "templates",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "clouddlplistinspecttemplatesoperator",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlplistjobtriggersoperator_20260128_160330",
      "component_name": "CloudDLPListJobTriggersOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPListJobTriggersOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "results_filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists job triggers.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPListJobTriggersOperator`\n\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param page_size: (Optional) The maximum number of resources contained in the\n    underlying API response.\n:param order_by: (Optional) Optiona",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.245846",
      "relevance_keywords": [
        "clouddlplistjobtriggersoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "triggers",
        "list",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddlpliststoredinfotypesoperator_20260128_160330",
      "component_name": "CloudDLPListStoredInfoTypesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPListStoredInfoTypesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "organization_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists stored infoTypes.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPListStoredInfoTypesOperator`\n\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the parent resource is\n    a project instead of an organization.\n:param page_size",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.246380",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "stored",
        "types",
        "gcp",
        "bigquery",
        "info",
        "list",
        "cloud",
        "clouddlpliststoredinfotypesoperator"
      ]
    },
    {
      "id": "official_clouddlpredactimageoperator_20260128_160330",
      "component_name": "CloudDLPRedactImageOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPRedactImageOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "inspect_config",
          "image_redaction_configs",
          "include_findings",
          "byte_item",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "inspect_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "image_redaction_configs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "include_findings",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "byte_item",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Redacts potentially sensitive info from an image; limits input size, processing time, and output size.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPRedactImageOperator`\n\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param inspect_config: (Optional) Configuration for the inspect",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.246916",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "redact",
        "image",
        "bigquery",
        "clouddlpredactimageoperator",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpreidentifycontentoperator_20260128_160330",
      "component_name": "CloudDLPReidentifyContentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPReidentifyContentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "reidentify_config",
          "inspect_config",
          "item",
          "inspect_template_name",
          "reidentify_template_name",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "reidentify_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "inspect_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "item",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "inspect_template_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "reidentify_template_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Re-identifies content that has been de-identified.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPReidentifyContentOperator`\n\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param reidentify_config: (Optional) Configuration for the re-identification of\n    the content item.\n:param ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.247453",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "clouddlpreidentifycontentoperator",
        "content",
        "cloud",
        "reidentify"
      ]
    },
    {
      "id": "official_clouddlpupdatedeidentifytemplateoperator_20260128_160330",
      "component_name": "CloudDLPUpdateDeidentifyTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPUpdateDeidentifyTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "template_id",
          "organization_id",
          "project_id",
          "deidentify_template",
          "update_mask",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "template_id",
          "type": "str",
          "required": false
        },
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "deidentify_template",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates the DeidentifyTemplate.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPUpdateDeidentifyTemplateOperator`\n\n:param template_id: The ID of deidentify template to be updated.\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.248061",
      "relevance_keywords": [
        "gcs",
        "update",
        "operator",
        "google",
        "deidentify",
        "template",
        "gcp",
        "bigquery",
        "clouddlpupdatedeidentifytemplateoperator",
        "cloud"
      ]
    },
    {
      "id": "official_clouddlpupdateinspecttemplateoperator_20260128_160330",
      "component_name": "CloudDLPUpdateInspectTemplateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPUpdateInspectTemplateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "template_id",
          "organization_id",
          "project_id",
          "inspect_template",
          "update_mask",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "template_id",
          "type": "str",
          "required": false
        },
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "inspect_template",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates the InspectTemplate.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPUpdateInspectTemplateOperator`\n\n:param template_id: The ID of the inspect template to be updated.\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. Only set this field if the pare",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.248652",
      "relevance_keywords": [
        "inspect",
        "gcs",
        "update",
        "operator",
        "google",
        "template",
        "gcp",
        "bigquery",
        "cloud",
        "clouddlpupdateinspecttemplateoperator"
      ]
    },
    {
      "id": "official_clouddlpupdatejobtriggeroperator_20260128_160330",
      "component_name": "CloudDLPUpdateJobTriggerOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPUpdateJobTriggerOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "job_trigger_id",
          "project_id",
          "job_trigger",
          "update_mask",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_trigger_id",
          "type": "Any",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "job_trigger",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates a job trigger.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPUpdateJobTriggerOperator`\n\n:param job_trigger_id: The ID of the DLP job trigger to be updated.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists. If set to None or missing, the default\n    project_id from the Google Cloud connection is used.\n:param job_trigger: New JobTrigger value.\n:param update_mask: Mask to ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.249191",
      "relevance_keywords": [
        "gcs",
        "update",
        "operator",
        "google",
        "trigger",
        "gcp",
        "clouddlpupdatejobtriggeroperator",
        "bigquery",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_clouddlpupdatestoredinfotypeoperator_20260128_160330",
      "component_name": "CloudDLPUpdateStoredInfoTypeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPUpdateStoredInfoTypeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "stored_info_type_id",
          "organization_id",
          "project_id",
          "config",
          "update_mask",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "stored_info_type_id",
          "type": "Any",
          "required": false
        },
        {
          "name": "organization_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists, InvalidArgument, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dlp_v2.types import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates the stored infoType by creating a new version.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDLPUpdateStoredInfoTypeOperator`\n\n:param stored_info_type_id: The ID of the stored info type to be updated.\n:param organization_id: (Optional) The organization ID. Required to set this\n    field if parent resource is an organization.\n:param project_id: (Optional) Google Cloud project ID where the\n    DLP Instance exists",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.249918",
      "relevance_keywords": [
        "gcs",
        "clouddlpupdatestoredinfotypeoperator",
        "update",
        "operator",
        "google",
        "stored",
        "gcp",
        "bigquery",
        "info",
        "type",
        "cloud"
      ]
    },
    {
      "id": "official_cloudfunctiondeployfunctionoperator_20260128_160330",
      "component_name": "CloudFunctionDeployFunctionOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFunctionDeployFunctionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "body",
          "project_id",
          "location",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "zip_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "validate_body",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.functions import CloudFunctionsHook\nfrom airflow.providers.google.cloud.links.cloud_functions import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "_validate_all_body_fields",
          "_create_new_function",
          "_update_function",
          "_check_if_function_exists",
          "_upload_source_code",
          "_set_airflow_version_label",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create or update a function in Google Cloud Functions.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudFunctionDeployFunctionOperator`\n\n:param location: Google Cloud region where the function should be created.\n:param body: Body of the Cloud Functions definition. The body must be a\n    Cloud Functions dictionary as described in:\n    https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions\n  ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.294961",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "function",
        "cloud",
        "cloudfunctiondeployfunctionoperator",
        "deploy"
      ]
    },
    {
      "id": "official_cloudfunctiondeletefunctionoperator_20260128_160330",
      "component_name": "CloudFunctionDeleteFunctionOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFunctionDeleteFunctionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "name",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.functions import CloudFunctionsHook\nfrom airflow.providers.google.cloud.links.cloud_functions import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes the specified function from Google Cloud Functions.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudFunctionDeleteFunctionOperator`\n\n:param name: A fully-qualified function name, matching\n    the pattern: `^projects/[^/]+/locations/[^/]+/functions/[^/]+$`\n:param gcp_conn_id: The connection ID to use to connect to Google Cloud.\n:param api_version: API version used (for example v1 or v1beta1).\n:param impersonation_",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.295203",
      "relevance_keywords": [
        "cloudfunctiondeletefunctionoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "function",
        "cloud"
      ]
    },
    {
      "id": "official_cloudfunctioninvokefunctionoperator_20260128_160330",
      "component_name": "CloudFunctionInvokeFunctionOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFunctionInvokeFunctionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "function_id",
          "input_data",
          "location",
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "function_id",
          "type": "str",
          "required": false
        },
        {
          "name": "input_data",
          "type": "dict",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.errors import HttpError\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.functions import CloudFunctionsHook\nfrom airflow.providers.google.cloud.links.cloud_functions import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Invokes a deployed Cloud Function. To be used for testing purposes as very limited traffic is allowed.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudFunctionDeployFunctionOperator`\n\n:param function_id: ID of the function to be called\n:param input_data: Input to be passed to the function\n:param location: The location where the function is located.\n:param project_id: Optional, Google Cloud Project project_id where the fu",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.295349",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "cloudfunctioninvokefunctionoperator",
        "bigquery",
        "function",
        "invoke",
        "cloud"
      ]
    },
    {
      "id": "official_gcscreatebucketoperator_20260128_160330",
      "component_name": "GCSCreateBucketOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSCreateBucketOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "bucket_name",
          "storage_class",
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket_name",
          "type": "str",
          "required": false
        },
        {
          "name": "resource",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "storage_class",
          "type": "str",
          "required": false,
          "default": "MULTI_REGIONAL"
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "US"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport subprocess\nimport sys\nimport warnings\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory\nfrom typing import TYPE_CHECKING",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new bucket.\n\nGoogle Cloud Storage uses a flat namespace, so you\ncan't create a bucket with a name that is already in use.\n\n    .. seealso::\n        For more information, see Bucket Naming Guidelines:\n        https://cloud.google.com/storage/docs/bucketnaming.html#requirements\n\n:param bucket_name: The name of the bucket. (templated)\n:param resource: An optional dict with parameters for creating the bucket.\n        For information on available parameters, see Cloud Storage API doc:\n     ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.315967",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "bucket",
        "gcscreatebucketoperator",
        "cloud"
      ]
    },
    {
      "id": "official_gcslistobjectsoperator_20260128_160330",
      "component_name": "GCSListObjectsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSListObjectsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "bucket",
          "prefix",
          "delimiter",
          "match_glob",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "prefix",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "delimiter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "match_glob",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport subprocess\nimport sys\nimport warnings\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory\nfrom typing import TYPE_CHECKING",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "List all objects from the bucket filtered by given string prefix and delimiter in name or match_glob.\n\nThis operator returns a python list with the name of objects which can be used by\nXCom in the downstream task.\n\n:param bucket: The Google Cloud Storage bucket to find the objects. (templated)\n:param prefix: String or list of strings, which filter objects whose name begins with\n       it/them. (templated)\n:param delimiter: (Deprecated) The delimiter by which you want to filter the objects. (temp",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.316289",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcslistobjectsoperator",
        "objects",
        "gcp",
        "bigquery",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_gcsdeleteobjectsoperator_20260128_160330",
      "component_name": "GCSDeleteObjectsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSDeleteObjectsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "bucket_name",
          "prefix",
          "objects",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket_name",
          "type": "str",
          "required": false
        },
        {
          "name": "objects",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "prefix",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport subprocess\nimport sys\nimport warnings\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory\nfrom typing import TYPE_CHECKING",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_start"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes objects from a list or all objects matching a prefix from a Google Cloud Storage bucket.\n\n:param bucket_name: The GCS bucket to delete from\n:param objects: List of objects to delete. These should be the names\n    of objects in the bucket, not including gs://bucket/\n:param prefix: String or list of strings, which filter objects whose name begin with\n       it/them. (templated)\n:param gcp_conn_id: (Optional) The connection ID used to connect to Google Cloud.\n:param impersonation_chain: Opt",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.316544",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "objects",
        "gcp",
        "bigquery",
        "delete",
        "gcsdeleteobjectsoperator",
        "cloud"
      ]
    },
    {
      "id": "official_gcsbucketcreateaclentryoperator_20260128_160330",
      "component_name": "GCSBucketCreateAclEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSBucketCreateAclEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "bucket",
          "entity",
          "role",
          "user_project",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "entity",
          "type": "str",
          "required": false
        },
        {
          "name": "role",
          "type": "str",
          "required": false
        },
        {
          "name": "user_project",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport subprocess\nimport sys\nimport warnings\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory\nfrom typing import TYPE_CHECKING",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new ACL entry on the specified bucket.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GCSBucketCreateAclEntryOperator`\n\n:param bucket: Name of a bucket.\n:param entity: The entity holding the permission, in one of the following forms:\n    user-userId, user-email, group-groupId, group-email, domain-domain,\n    project-team-projectId, allUsers, allAuthenticatedUsers\n:param role: The access permission for the entity.\n ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.316948",
      "relevance_keywords": [
        "gcs",
        "operator",
        "gcsbucketcreateaclentryoperator",
        "google",
        "create",
        "entry",
        "gcp",
        "bigquery",
        "bucket",
        "cloud",
        "acl"
      ]
    },
    {
      "id": "official_gcsobjectcreateaclentryoperator_20260128_160330",
      "component_name": "GCSObjectCreateAclEntryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSObjectCreateAclEntryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "bucket",
          "object_name",
          "entity",
          "generation",
          "role",
          "user_project",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "object_name",
          "type": "str",
          "required": false
        },
        {
          "name": "entity",
          "type": "str",
          "required": false
        },
        {
          "name": "role",
          "type": "str",
          "required": false
        },
        {
          "name": "generation",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "user_project",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport subprocess\nimport sys\nimport warnings\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory\nfrom typing import TYPE_CHECKING",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new ACL entry on the specified object.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GCSObjectCreateAclEntryOperator`\n\n:param bucket: Name of a bucket.\n:param object_name: Name of the object. For information about how to URL encode object\n    names to be path safe, see:\n    https://cloud.google.com/storage/docs/json_api/#encoding\n:param entity: The entity holding the permission, in one of the following forms:\n    ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.317512",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "object",
        "create",
        "entry",
        "gcp",
        "bigquery",
        "gcsobjectcreateaclentryoperator",
        "cloud",
        "acl"
      ]
    },
    {
      "id": "official_gcsfiletransformoperator_20260128_160330",
      "component_name": "GCSFileTransformOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSFileTransformOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "source_bucket",
          "source_object",
          "destination_bucket",
          "destination_object",
          "transform_script",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source_bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "source_object",
          "type": "str",
          "required": false
        },
        {
          "name": "transform_script",
          "type": "Union",
          "required": false
        },
        {
          "name": "destination_bucket",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "destination_object",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport subprocess\nimport sys\nimport warnings\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory\nfrom typing import TYPE_CHECKING",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_start"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Copies data from a source GCS location to a temporary location on the local filesystem.\n\nRuns a transformation on this file as specified by the transformation script\nand uploads the output to a destination bucket. If the output bucket is not\nspecified the original file will be overwritten.\n\nThe locations of the source and the destination files in the local\nfilesystem is provided as an first and second arguments to the\ntransformation script. The transformation script is expected to read the\ndata ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.317964",
      "relevance_keywords": [
        "gcs",
        "transform",
        "file",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "gcsfiletransformoperator"
      ]
    },
    {
      "id": "official_gcstimespanfiletransformoperator_20260128_160330",
      "component_name": "GCSTimeSpanFileTransformOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSTimeSpanFileTransformOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "source_bucket",
          "source_prefix",
          "destination_bucket",
          "destination_prefix",
          "transform_script",
          "source_gcp_conn_id",
          "source_impersonation_chain",
          "destination_gcp_conn_id",
          "destination_impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source_bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "source_prefix",
          "type": "str",
          "required": false
        },
        {
          "name": "source_gcp_conn_id",
          "type": "str",
          "required": false
        },
        {
          "name": "destination_bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "destination_prefix",
          "type": "str",
          "required": false
        },
        {
          "name": "destination_gcp_conn_id",
          "type": "str",
          "required": false
        },
        {
          "name": "transform_script",
          "type": "Union",
          "required": false
        },
        {
          "name": "source_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "destination_impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "chunk_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "download_continue_on_fail",
          "type": "Union",
          "required": false,
          "default": false
        },
        {
          "name": "download_num_attempts",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "upload_continue_on_fail",
          "type": "Union",
          "required": false,
          "default": false
        },
        {
          "name": "upload_num_attempts",
          "type": "int",
          "required": false,
          "default": 1
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport subprocess\nimport sys\nimport warnings\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory\nfrom typing import TYPE_CHECKING",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "interpolate_prefix",
          "__init__",
          "execute",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Copy objects that were modified during a time span, run a transform, and upload results to a bucket.\n\nDetermines a list of objects that were added or modified at a GCS source\nlocation during a specific time-span, copies them to a temporary location\non the local file system, runs a transform on this file as specified by\nthe transformation script and uploads the output to the destination bucket.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.318391",
      "relevance_keywords": [
        "gcstimespanfiletransformoperator",
        "gcs",
        "transform",
        "file",
        "operator",
        "google",
        "gcp",
        "span",
        "bigquery",
        "time",
        "cloud"
      ]
    },
    {
      "id": "official_gcsdeletebucketoperator_20260128_160330",
      "component_name": "GCSDeleteBucketOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSDeleteBucketOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "bucket_name",
          "gcp_conn_id",
          "impersonation_chain",
          "user_project"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket_name",
          "type": "str",
          "required": false
        },
        {
          "name": "force",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "user_project",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport subprocess\nimport sys\nimport warnings\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory\nfrom typing import TYPE_CHECKING",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes bucket from a Google Cloud Storage.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GCSDeleteBucketOperator`\n\n:param bucket_name: name of the bucket which will be deleted\n:param force: false not allow to delete non empty bucket, set force=True\n    allows to delete non empty bucket\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud.\n:param impersonation_chain: Optional service account to impersonat",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.318915",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcsdeletebucketoperator",
        "gcp",
        "bigquery",
        "delete",
        "bucket",
        "cloud"
      ]
    },
    {
      "id": "official_gcssynchronizebucketsoperator_20260128_160330",
      "component_name": "GCSSynchronizeBucketsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSSynchronizeBucketsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "source_bucket",
          "destination_bucket",
          "source_object",
          "destination_object",
          "recursive",
          "delete_extra_files",
          "allow_overwrite",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "source_bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "destination_bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "source_object",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "destination_object",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "recursive",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "delete_extra_files",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "allow_overwrite",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport subprocess\nimport sys\nimport warnings\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile, TemporaryDirectory\nfrom typing import TYPE_CHECKING",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "_get_uri"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Synchronizes the contents of the buckets or bucket's directories in the Google Cloud Services.\n\nParameters ``source_object`` and ``destination_object`` describe the root sync directory. If they are\nnot passed, the entire bucket will be synchronized. They should point to directories.\n\n.. note::\n    The synchronization of individual files is not supported. Only entire directories can be\n    synchronized.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.319210",
      "relevance_keywords": [
        "synchronize",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "buckets",
        "cloud",
        "gcssynchronizebucketsoperator"
      ]
    },
    {
      "id": "official_genaigenerateembeddingsoperator_20260128_160330",
      "component_name": "GenAIGenerateEmbeddingsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGenerateEmbeddingsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "contents",
          "model",
          "config"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "model",
          "type": "str",
          "required": false
        },
        {
          "name": "contents",
          "type": "Union",
          "required": false
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Uses the Gemini AI Embeddings API to generate embeddings for words, phrases, sentences, and code.\n\n:param project_id: Required. The ID of the Google Cloud project that the\n    service belongs to (templated).\n:param location: Required. The ID of the Google Cloud location that the\n    service belongs to (templated).\n:param model: Required. The name of the model to use for content generation,\n    which can be a text-only or multimodal model. For example, `gemini-pro` or\n    `gemini-pro-vision`.\n:pa",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.355969",
      "relevance_keywords": [
        "gcs",
        "genaigenerateembeddingsoperator",
        "operator",
        "google",
        "generate",
        "gcp",
        "embeddings",
        "bigquery",
        "cloud",
        "gen"
      ]
    },
    {
      "id": "official_genaigeneratecontentoperator_20260128_160330",
      "component_name": "GenAIGenerateContentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGenerateContentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "generation_config",
          "location",
          "project_id",
          "impersonation_chain",
          "contents",
          "model"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "contents",
          "type": "ContentListUnionDict",
          "required": false
        },
        {
          "name": "model",
          "type": "str",
          "required": false
        },
        {
          "name": "generation_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Generate a model response based on given configuration. Input capabilities differ between models, including tuned models.\n\n:param project_id: Required. The ID of the Google Cloud project that the\n    service belongs to (templated).\n:param location: Required. The ID of the Google Cloud location that the\n    service belongs to (templated).\n:param model: Required. The name of the model to use for content generation,\n    which can be a text-only or multimodal model. For example, `gemini-pro` or\n    ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.356312",
      "relevance_keywords": [
        "genaigeneratecontentoperator",
        "gcs",
        "operator",
        "google",
        "generate",
        "gcp",
        "bigquery",
        "content",
        "cloud",
        "gen"
      ]
    },
    {
      "id": "official_genaisupervisedfinetuningtrainoperator_20260128_160330",
      "component_name": "GenAISupervisedFineTuningTrainOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAISupervisedFineTuningTrainOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "training_dataset",
          "tuning_job_config",
          "source_model"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "source_model",
          "type": "str",
          "required": false
        },
        {
          "name": "training_dataset",
          "type": "TuningDatasetOrDict",
          "required": false
        },
        {
          "name": "tuning_job_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a tuning job to adapt model behavior with a labeled dataset.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param source_model: Required. A pre-trained model optimized for performing natural\n    language tasks such as classification, summarization, extraction, content\n    creation, and ideation.\n:param training_dataset: Required. Cloud Storage URI of ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.356649",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "fine",
        "train",
        "gcp",
        "bigquery",
        "genaisupervisedfinetuningtrainoperator",
        "tuning",
        "supervised",
        "cloud",
        "gen"
      ]
    },
    {
      "id": "official_genaicounttokensoperator_20260128_160330",
      "component_name": "GenAICountTokensOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAICountTokensOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "contents",
          "model",
          "config"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "contents",
          "type": "Union",
          "required": false
        },
        {
          "name": "model",
          "type": "str",
          "required": false
        },
        {
          "name": "config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Use Count Tokens API to calculate the number of input tokens before sending a request to Gemini API.\n\n:param project_id: Required. The ID of the Google Cloud project that the\n    service belongs to (templated).\n:param location: Required. The ID of the Google Cloud location that the\n    service belongs to (templated).\n:param contents: Required. The multi-part content of a message that a user or a program\n    gives to the generative model, in order to elicit a specific response.\n:param model: Requ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.357004",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "tokens",
        "bigquery",
        "genaicounttokensoperator",
        "count",
        "cloud",
        "gen"
      ]
    },
    {
      "id": "official_genaicreatecachedcontentoperator_20260128_160330",
      "component_name": "GenAICreateCachedContentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAICreateCachedContentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "model",
          "cached_content_config"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "model",
          "type": "str",
          "required": false
        },
        {
          "name": "cached_content_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create CachedContent resource to reduce the cost of requests that contain repeat content with high input token counts.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param model: Required. The name of the publisher model to use for cached content.\n:param cached_content_config: Optional. Configuration of the Cached Content.\n:param gcp_conn_id: The connection ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.357308",
      "relevance_keywords": [
        "cached",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "content",
        "cloud",
        "genaicreatecachedcontentoperator",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminicreatebatchjoboperator_20260128_160330",
      "component_name": "GenAIGeminiCreateBatchJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiCreateBatchJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "model",
          "create_batch_job_config",
          "gemini_api_key",
          "input_source"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "model",
          "type": "str",
          "required": false
        },
        {
          "name": "input_source",
          "type": "Union",
          "required": false
        },
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": false
        },
        {
          "name": "create_batch_job_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "results_folder",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retrieve_result",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "wait_until_complete",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "polling_interval",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_wait_until_complete",
          "_prepare_results_for_xcom",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create Batch job using Gemini Batch API. Use to generate model response for several requests.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param model: Required. The name of the publisher model to use for Batch job.\n:param gemini_api_key: Required. Key to interact with Gemini Batch API.\n:param input_source: Required. Source of requests, could be inline req",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.357774",
      "relevance_keywords": [
        "gemini",
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "genaigeminicreatebatchjoboperator",
        "bigquery",
        "cloud",
        "job",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminigetbatchjoboperator_20260128_160330",
      "component_name": "GenAIGeminiGetBatchJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiGetBatchJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "job_name",
          "gemini_api_key"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get Batch job using Gemini API.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param model: Required. The name of the publisher model to use for Batch job.\n:param gemini_api_key: Required. Key to interact with Gemini Batch API.\n:param job_name: Required. Name of the batch job.\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud.\n:param im",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.358220",
      "relevance_keywords": [
        "gemini",
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "get",
        "cloud",
        "job",
        "genaigeminigetbatchjoboperator",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminilistbatchjobsoperator_20260128_160330",
      "component_name": "GenAIGeminiListBatchJobsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiListBatchJobsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "list_batch_jobs_config",
          "gemini_api_key"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": false
        },
        {
          "name": "list_batch_jobs_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get list of Batch jobs metadata using Gemini API.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param model: Required. The name of the publisher model to use for Batch job.\n:param gemini_api_key: Required. Key to interact with Gemini Batch API.\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud.\n:param impersonation_chain: Optional serv",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.358555",
      "relevance_keywords": [
        "genaigeminilistbatchjobsoperator",
        "gemini",
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "jobs",
        "bigquery",
        "list",
        "cloud",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminideletebatchjoboperator_20260128_160330",
      "component_name": "GenAIGeminiDeleteBatchJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiDeleteBatchJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "job_name",
          "gemini_api_key"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Queue a batch job for deletion using the Gemini API.\n\nThe job will not be deleted immediately. After submitting it for deletion, it will still be available\nthrough GenAIGeminiListBatchJobsOperator or GenAIGeminiGetBatchJobOperator for some time.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param model: Required. The name of the publisher model to use for B",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.358952",
      "relevance_keywords": [
        "gemini",
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "genaigeminideletebatchjoboperator",
        "bigquery",
        "delete",
        "cloud",
        "job",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminicancelbatchjoboperator_20260128_160330",
      "component_name": "GenAIGeminiCancelBatchJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiCancelBatchJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "job_name",
          "gemini_api_key"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Cancel Batch job using Gemini API.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param model: Required. The name of the publisher model to use for Batch job.\n:param gemini_api_key: Required. Key to interact with Gemini Batch API.\n:param job_name: Required. Name of the batch job.\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud.\n:param",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.359631",
      "relevance_keywords": [
        "genaigeminicancelbatchjoboperator",
        "gemini",
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "cancel",
        "bigquery",
        "cloud",
        "job",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminicreateembeddingsbatchjoboperator_20260128_160330",
      "component_name": "GenAIGeminiCreateEmbeddingsBatchJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiCreateEmbeddingsBatchJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "model",
          "create_embeddings_config",
          "gemini_api_key",
          "input_source"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "model",
          "type": "str",
          "required": false
        },
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": false
        },
        {
          "name": "input_source",
          "type": "Union",
          "required": false
        },
        {
          "name": "results_folder",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "create_embeddings_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_until_complete",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "retrieve_result",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "polling_interval",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_wait_until_complete",
          "_prepare_results_for_xcom",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create embeddings Batch job using Gemini Batch API.\n\nUse to generate embeddings for words, phrases, sentences, and code for several requests.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param model: Required. The name of the publisher model to use for Batch job.\n:param gemini_api_key: Required. Key to interact with Gemini Batch API.\n:param input_source: R",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.359988",
      "relevance_keywords": [
        "gemini",
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "genaigeminicreateembeddingsbatchjoboperator",
        "embeddings",
        "bigquery",
        "cloud",
        "job",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminiuploadfileoperator_20260128_160330",
      "component_name": "GenAIGeminiUploadFileOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiUploadFileOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "file_path",
          "gemini_api_key"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "file_path",
          "type": "str",
          "required": false
        },
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": false
        },
        {
          "name": "upload_file_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get file uploaded to Gemini Files API.\n\nThe Files API lets you store up to 20GB of files per project, with each file not exceeding 2GB in size.\nSupported types are audio files, images, videos, documents, and others. Files are stored for 48 hours.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param gemini_api_key: Required. Key to interact with Gemini Batch ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.360267",
      "relevance_keywords": [
        "gemini",
        "gcs",
        "file",
        "operator",
        "google",
        "upload",
        "gcp",
        "genaigeminiuploadfileoperator",
        "bigquery",
        "cloud",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminigetfileoperator_20260128_160330",
      "component_name": "GenAIGeminiGetFileOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiGetFileOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "file_name",
          "gemini_api_key"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "file_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get file's metadata uploaded to Gemini Files API by using GenAIGeminiUploadFileOperator.\n\nThe Files API lets you store up to 20GB of files per project, with each file not exceeding 2GB in size.\nFiles are stored for 48 hours.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param gemini_api_key: Required. Key to interact with Gemini Batch API.\n:param file_name:",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.360574",
      "relevance_keywords": [
        "gemini",
        "gcs",
        "file",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "genaigeminigetfileoperator",
        "get",
        "cloud",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminilistfilesoperator_20260128_160330",
      "component_name": "GenAIGeminiListFilesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiListFilesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "gemini_api_key"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "List files uploaded to Gemini Files API.\n\nThe Files API lets you store up to 20GB of files per project, with each file not exceeding 2GB in size.\nFiles are stored for 48 hours.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param gemini_api_key: Required. Key to interact with Gemini Batch API.\n:param gcp_conn_id: The connection ID to use connecting to Google",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.360932",
      "relevance_keywords": [
        "gemini",
        "gcs",
        "operator",
        "google",
        "gcp",
        "files",
        "bigquery",
        "genaigeminilistfilesoperator",
        "list",
        "cloud",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminideletefileoperator_20260128_160330",
      "component_name": "GenAIGeminiDeleteFileOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiDeleteFileOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "impersonation_chain",
          "file_name",
          "gemini_api_key"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "file_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport enum\nimport os.path\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.genai.errors import ClientError\nfrom airflow.exceptions import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete file uploaded to Gemini Files API.\n\nThe Files API lets you store up to 20GB of files per project, with each file not exceeding 2GB in size.\nFiles are stored for 48 hours.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud location that the service belongs to.\n:param gemini_api_key: Required. Key to interact with Gemini Batch API.\n:param file_name: Required. File name in Gemini Files API to del",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.361241",
      "relevance_keywords": [
        "gemini",
        "gcs",
        "file",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "cloud",
        "genaigeminideletefileoperator",
        "gen"
      ]
    },
    {
      "id": "official_gkedeleteclusteroperator_20260128_160330",
      "component_name": "GKEDeleteClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEDeleteClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v2"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_check_input",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Deletes the cluster, including the Kubernetes endpoint and all worker nodes.\n\nTo delete a certain cluster, you must specify the ``project_id``, the ``cluster_name``\nof the cluster, the ``location`` that the cluster is in, and the ``task_id``.\n\n**Operator Creation**: ::\n\n    operator = GKEClusterDeleteOperator(\n                task_id='cluster_delete',\n                project_id='my-project',\n                location='cluster-location'\n                cluster_name='cluster-name')\n\n.. seealso::\n  ",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.402142",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "cluster",
        "gcp",
        "bigquery",
        "delete",
        "gkedeleteclusteroperator",
        "cloud"
      ]
    },
    {
      "id": "official_gkecreateclusteroperator_20260128_160330",
      "component_name": "GKECreateClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKECreateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "Union",
          "required": true
        },
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v2"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_input",
          "_body_field",
          "_alert_deprecated_body_fields",
          "extra_links_params",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a Google Kubernetes Engine Cluster of specified dimensions and wait until the cluster is created.\n\nThe **minimum** required to define a cluster to create is:\n\n``dict()`` ::\n    cluster_def = {\"name\": \"my-cluster-name\", \"initial_node_count\": 1}\n\nor\n\n``Cluster`` proto ::\n    from google.cloud.container_v1.types import Cluster\n\n    cluster_def = Cluster(name=\"my-cluster-name\", initial_node_count=1)\n\n**Operator Creation**: ::\n\n    operator = GKEClusterCreateOperator(\n                task_id='",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.402379",
      "relevance_keywords": [
        "gkecreateclusteroperator",
        "gcs",
        "operator",
        "google",
        "cluster",
        "create",
        "gcp",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_gkestartkueueinsideclusteroperator_20260128_160330",
      "component_name": "GKEStartKueueInsideClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEStartKueueInsideClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "KubernetesInstallKueueOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, KubernetesInstallKueueOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "enable_tcp_keepalive",
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Installs Kueue of specific version inside Cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GKEStartKueueInsideClusterOperator`\n\n.. seealso::\n    For more details about Kueue have a look at the reference:\n    https://kueue.sigs.k8s.io/docs/overview/\n\n:param location: The name of the Google Kubernetes Engine zone or region in which the\n    cluster resides, e.g. 'us-central1-a'\n:param cluster_name: The name of the Google",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.402568",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "cluster",
        "start",
        "kueue",
        "gcp",
        "bigquery",
        "gkestartkueueinsideclusteroperator",
        "inside",
        "cloud"
      ]
    },
    {
      "id": "official_gkestartpodoperator_20260128_160330",
      "component_name": "GKEStartPodOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEStartPodOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "KubernetesPodOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "on_finish_action",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, KubernetesPodOperator",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "invoke_defer_method"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Executes a task in a Kubernetes pod in the specified Google Kubernetes Engine cluster.\n\nThis Operator assumes that the system has gcloud installed and has configured a\nconnection id with a service account.\n\nThe **minimum** required to define a cluster to create are the variables\n``task_id``, ``project_id``, ``location``, ``cluster_name``, ``name``,\n``namespace``, and ``image``\n\n.. seealso::\n    For more detail about Kubernetes Engine authentication have a look at the reference:\n    https://cloud",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.402758",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "pod",
        "start",
        "gcp",
        "bigquery",
        "gkestartpodoperator",
        "cloud"
      ]
    },
    {
      "id": "official_gkestartjoboperator_20260128_160330",
      "component_name": "GKEStartJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEStartJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "KubernetesJobOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "job_poll_interval",
          "type": "float",
          "required": false,
          "default": 10.0
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, KubernetesJobOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_deferrable"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Executes a Kubernetes job in the specified Google Kubernetes Engine cluster.\n\nThis Operator assumes that the system has gcloud installed and has configured a\nconnection id with a service account.\n\nThe **minimum** required to define a cluster to create are the variables\n``task_id``, ``project_id``, ``location``, ``cluster_name``, ``name``,\n``namespace``, and ``image``\n\n.. seealso::\n    For more detail about Kubernetes Engine authentication have a look at the reference:\n    https://cloud.google.co",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.402942",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "start",
        "bigquery",
        "gkestartjoboperator",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_gkedescribejoboperator_20260128_160330",
      "component_name": "GKEDescribeJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEDescribeJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": true
        },
        {
          "name": "namespace",
          "type": "str",
          "required": true
        },
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Retrieve information about Job by given name.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GKEDescribeJobOperator`\n\n:param job_name: The name of the Job to delete\n:param namespace: The name of the Google Kubernetes Engine namespace.\n:param location: The name of the Google Kubernetes Engine zone or region in which the\n    cluster resides, e.g. 'us-central1-a'\n:param cluster_name: The name of the Google Kubernetes Engine clu",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.403118",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "describe",
        "gkedescribejoboperator",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_gkelistjobsoperator_20260128_160330",
      "component_name": "GKEListJobsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEListJobsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "namespace",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "do_xcom_push",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Retrieve list of Jobs.\n\nIf namespace parameter is specified, the list of Jobs from dedicated\nnamespace will be retrieved. If no namespace specified, it will output Jobs from all namespaces.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GKEListJobsOperator`\n\n:param location: The name of the Google Kubernetes Engine zone or region in which the\n    cluster resides, e.g. 'us-central1-a'\n:param cluster_name: The name of the Goog",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.403295",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "jobs",
        "bigquery",
        "gkelistjobsoperator",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_gkecreatecustomresourceoperator_20260128_160330",
      "component_name": "GKECreateCustomResourceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKECreateCustomResourceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "KubernetesCreateResourceOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, KubernetesCreateResourceOperator",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a resource in the specified Google Kubernetes Engine cluster.\n\nThis Operator assumes that the system has gcloud installed and has configured a\nconnection id with a service account.\n\n.. seealso::\n    For more detail about Kubernetes Engine authentication have a look at the reference:\n    https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#internal_ip\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/o",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.403463",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "custom",
        "create",
        "resource",
        "gcp",
        "bigquery",
        "cloud",
        "gkecreatecustomresourceoperator"
      ]
    },
    {
      "id": "official_gkedeletecustomresourceoperator_20260128_160330",
      "component_name": "GKEDeleteCustomResourceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEDeleteCustomResourceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "KubernetesDeleteResourceOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, KubernetesDeleteResourceOperator",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a resource in the specified Google Kubernetes Engine cluster.\n\nThis Operator assumes that the system has gcloud installed and has configured a\nconnection id with a service account.\n\n.. seealso::\n    For more detail about Kubernetes Engine authentication have a look at the reference:\n    https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#internal_ip\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/o",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.403641",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "custom",
        "resource",
        "gcp",
        "bigquery",
        "delete",
        "gkedeletecustomresourceoperator",
        "cloud"
      ]
    },
    {
      "id": "official_gkestartkueuejoboperator_20260128_160330",
      "component_name": "GKEStartKueueJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEStartKueueJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "KubernetesStartKueueJobOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, KubernetesStartKueueJobOperator",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Executes a Kubernetes Job in Kueue in the specified Google Kubernetes Engine cluster.\n\n:param location: The name of the Google Kubernetes Engine zone or region in which the\n    cluster resides, e.g. 'us-central1-a'\n:param cluster_name: The name of the Google Kubernetes Engine cluster.\n:param use_internal_ip: Use the internal IP address as the endpoint.\n:param use_dns_endpoint: Use the DNS address as the endpoint.\n:param project_id: The Google Developers Console project id\n:param gcp_conn_id: The",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.403827",
      "relevance_keywords": [
        "gcs",
        "gkestartkueuejoboperator",
        "operator",
        "google",
        "gcp",
        "start",
        "kueue",
        "bigquery",
        "cloud",
        "job"
      ]
    },
    {
      "id": "official_gkedeletejoboperator_20260128_160330",
      "component_name": "GKEDeleteJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEDeleteJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "KubernetesDeleteJobOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, KubernetesDeleteJobOperator",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a Kubernetes job in the specified Google Kubernetes Engine cluster.\n\nThis Operator assumes that the system has gcloud installed and has configured a\nconnection id with a service account.\n\nThe **minimum** required to define a cluster to create are the variables\n``task_id``, ``project_id``, ``location``, ``cluster_name``, ``name``,\n``namespace``\n\n.. seealso::\n    For more detail about Kubernetes Engine authentication have a look at the reference:\n    https://cloud.google.com/kubernetes-engi",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.404005",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "cloud",
        "job",
        "gkedeletejoboperator"
      ]
    },
    {
      "id": "official_gkesuspendjoboperator_20260128_160330",
      "component_name": "GKESuspendJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKESuspendJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": true
        },
        {
          "name": "namespace",
          "type": "str",
          "required": true
        },
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Suspend Job by given name.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GKESuspendJobOperator`\n\n:param name: The name of the Job to suspend\n:param namespace: The name of the Google Kubernetes Engine namespace.\n:param location: The name of the Google Kubernetes Engine zone or region in which the\n    cluster resides, e.g. 'us-central1-a'\n:param cluster_name: The name of the Google Kubernetes Engine cluster.\n:param use_intern",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.404183",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "gkesuspendjoboperator",
        "cloud",
        "job",
        "suspend"
      ]
    },
    {
      "id": "official_lookerstartpdtbuildoperator_20260128_160330",
      "component_name": "LookerStartPdtBuildOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "LookerStartPdtBuildOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "looker_conn_id",
          "type": "str",
          "required": true
        },
        {
          "name": "model",
          "type": "str",
          "required": true
        },
        {
          "name": "view",
          "type": "str",
          "required": true
        },
        {
          "name": "query_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "asynchronous",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "cancel_on_kill",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "wait_time",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "wait_timeout",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.looker import LookerHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [],
        "methods": [
          "__init__",
          "execute",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Submits a PDT materialization job to Looker.\n\n:param looker_conn_id: Required. The connection ID to use connecting to Looker.\n:param model: Required. The model of the PDT to start building.\n:param view: Required. The view of the PDT to start building.\n:param query_params: Optional. Additional materialization parameters.\n:param asynchronous: Optional. Flag indicating whether to wait for the job\n    to finish or return immediately.\n    This is useful for submitting long running jobs and\n    waitin",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.429607",
      "relevance_keywords": [
        "gcs",
        "build",
        "operator",
        "google",
        "gcp",
        "start",
        "looker",
        "pdt",
        "bigquery",
        "lookerstartpdtbuildoperator",
        "cloud"
      ]
    },
    {
      "id": "official_managedkafkabaseoperator_20260128_160330",
      "component_name": "ManagedKafkaBaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaBaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "gcp_conn_id",
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "hook"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Base class for Managed Kafka operators.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata: Strings which should be sent along with the request as metadata.\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud.\n:param",
      "success_score": 187,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.446456",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "kafka",
        "bigquery",
        "base",
        "managedkafkabaseoperator",
        "cloud",
        "managed"
      ]
    },
    {
      "id": "official_managedkafkacreateclusteroperator_20260128_160330",
      "component_name": "ManagedKafkaCreateClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaCreateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster",
          "type": "Union",
          "required": true
        },
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a new Apache Kafka cluster.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster: Required. Configuration of the cluster to create. Its ``name`` field is ignored.\n:param cluster_id: Required. The ID to use for the cluster, which will become the final component of\n    the cluster's name. The ID must be 1-63 characters long, and match the regular ",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.447288",
      "relevance_keywords": [
        "managedkafkacreateclusteroperator",
        "gcs",
        "operator",
        "google",
        "cluster",
        "kafka",
        "create",
        "gcp",
        "bigquery",
        "cloud",
        "managed"
      ]
    },
    {
      "id": "official_managedkafkalistclustersoperator_20260128_160330",
      "component_name": "ManagedKafkaListClustersOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaListClustersOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "List the clusters in a given project and location.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param page_size: Optional. The maximum number of clusters to return. The service may return fewer\n    than this value. If unspecified, server will pick an appropriate default.\n:param page_token: Optional. A page token, received from a previous ``ListClusters`` cal",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.447711",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "kafka",
        "clusters",
        "bigquery",
        "list",
        "cloud",
        "managed",
        "managedkafkalistclustersoperator"
      ]
    },
    {
      "id": "official_managedkafkagetclusteroperator_20260128_160330",
      "component_name": "ManagedKafkaGetClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaGetClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Get an Apache Kafka cluster.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster whose configuration to return.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata: Strings which should be sent along with the request as metadata.\n:param g",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.448017",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "cluster",
        "kafka",
        "gcp",
        "bigquery",
        "managedkafkagetclusteroperator",
        "get",
        "cloud",
        "managed"
      ]
    },
    {
      "id": "official_managedkafkaupdateclusteroperator_20260128_160330",
      "component_name": "ManagedKafkaUpdateClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaUpdateClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": true
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Update the properties of a single cluster.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster whose configuration to update.\n:param cluster: Required. The cluster to update.\n:param update_mask: Required. Field mask is used to specify the fields to be overwritten in the\n    cluster resource by the update. The fields",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.448274",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "managedkafkaupdateclusteroperator",
        "kafka",
        "cluster",
        "gcp",
        "bigquery",
        "cloud",
        "managed"
      ]
    },
    {
      "id": "official_managedkafkadeleteclusteroperator_20260128_160330",
      "component_name": "ManagedKafkaDeleteClusterOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaDeleteClusterOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "request_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete an Apache Kafka cluster.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster to delete.\n:param request_id: Optional. An optional request ID to identify requests. Specify a unique request ID\n    to avoid duplication of requests. If a request times out or fails, retrying with the same ID\n    allows the server t",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.448513",
      "relevance_keywords": [
        "gcs",
        "managedkafkadeleteclusteroperator",
        "operator",
        "google",
        "cluster",
        "kafka",
        "gcp",
        "bigquery",
        "delete",
        "cloud",
        "managed"
      ]
    },
    {
      "id": "official_managedkafkacreatetopicoperator_20260128_160330",
      "component_name": "ManagedKafkaCreateTopicOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaCreateTopicOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "topic_id",
          "type": "str",
          "required": true
        },
        {
          "name": "topic",
          "type": "Union",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a new topic in a given project and location.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster in which to create the topic.\n:param topic_id: Required. The ID to use for the topic, which will become the final component of the\n    topic's name.\n:param topic: Required. Configuration of the topic to create.\n:p",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.448745",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "kafka",
        "create",
        "bigquery",
        "managedkafkacreatetopicoperator",
        "topic",
        "cloud",
        "managed"
      ]
    },
    {
      "id": "official_managedkafkalisttopicsoperator_20260128_160330",
      "component_name": "ManagedKafkaListTopicsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaListTopicsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "List the topics in a given cluster.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster whose topics are to be listed.\n:param page_size: Optional. The maximum number of topics to return. The service may return fewer than\n    this value. If unset or zero, all topics for the parent is returned.\n:param page_token: Opti",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.449127",
      "relevance_keywords": [
        "managedkafkalisttopicsoperator",
        "gcs",
        "topics",
        "operator",
        "google",
        "gcp",
        "kafka",
        "bigquery",
        "list",
        "cloud",
        "managed"
      ]
    },
    {
      "id": "official_managedkafkagettopicoperator_20260128_160330",
      "component_name": "ManagedKafkaGetTopicOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaGetTopicOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "topic_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Return the properties of a single topic.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster whose topic is to be returned.\n:param topic_id: Required. The ID of the topic whose configuration to return.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.449403",
      "relevance_keywords": [
        "managedkafkagettopicoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "kafka",
        "bigquery",
        "topic",
        "get",
        "cloud",
        "managed"
      ]
    },
    {
      "id": "official_managedkafkaupdatetopicoperator_20260128_160330",
      "component_name": "ManagedKafkaUpdateTopicOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaUpdateTopicOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "topic_id",
          "type": "str",
          "required": true
        },
        {
          "name": "topic",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Update the properties of a single topic.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster whose topic is to be updated.\n:param topic_id: Required. The ID of the topic whose configuration to update.\n:param topic: Required. The topic to update. Its ``name`` field must be populated.\n:param update_mask: Required. Fie",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.449641",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "kafka",
        "gcp",
        "managedkafkaupdatetopicoperator",
        "bigquery",
        "topic",
        "cloud",
        "managed"
      ]
    },
    {
      "id": "official_managedkafkadeletetopicoperator_20260128_160330",
      "component_name": "ManagedKafkaDeleteTopicOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaDeleteTopicOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "topic_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete a single topic.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster whose topic is to be deleted.\n:param topic_id: Required. The ID of the topic to delete.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata: Strings which should b",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.449862",
      "relevance_keywords": [
        "gcs",
        "operator",
        "managedkafkadeletetopicoperator",
        "google",
        "kafka",
        "gcp",
        "bigquery",
        "delete",
        "topic",
        "cloud",
        "managed"
      ]
    },
    {
      "id": "official_managedkafkalistconsumergroupsoperator_20260128_160330",
      "component_name": "ManagedKafkaListConsumerGroupsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaListConsumerGroupsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "List the consumer groups in a given cluster.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster whose consumer groups are to be listed.\n:param page_size: Optional. The maximum number of consumer groups to return. The service may return\n    fewer than this value. If unset or zero, all consumer groups for the parent ",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.450100",
      "relevance_keywords": [
        "gcs",
        "managedkafkalistconsumergroupsoperator",
        "operator",
        "google",
        "kafka",
        "groups",
        "gcp",
        "bigquery",
        "list",
        "cloud",
        "managed",
        "consumer"
      ]
    },
    {
      "id": "official_managedkafkagetconsumergroupoperator_20260128_160330",
      "component_name": "ManagedKafkaGetConsumerGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaGetConsumerGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "consumer_group_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Return the properties of a single consumer group.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster whose consumer group is to be returned.\n:param consumer_group_id: Required. The ID of the consumer group whose configuration to return.\n:param retry: Designation of what errors, if any, should be retried.\n:param tim",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.450349",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "kafka",
        "bigquery",
        "managedkafkagetconsumergroupoperator",
        "get",
        "group",
        "cloud",
        "managed",
        "consumer"
      ]
    },
    {
      "id": "official_managedkafkaupdateconsumergroupoperator_20260128_160330",
      "component_name": "ManagedKafkaUpdateConsumerGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaUpdateConsumerGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "consumer_group_id",
          "type": "str",
          "required": true
        },
        {
          "name": "consumer_group",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Update the properties of a single consumer group.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster whose topic is to be updated.\n:param consumer_group_id: Required. The ID of the consumer group whose configuration to update.\n:param consumer_group: Required. The consumer_group to update. Its ``name`` field must be",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.450578",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "kafka",
        "gcp",
        "managedkafkaupdateconsumergroupoperator",
        "bigquery",
        "group",
        "cloud",
        "managed",
        "consumer"
      ]
    },
    {
      "id": "official_managedkafkadeleteconsumergroupoperator_20260128_160330",
      "component_name": "ManagedKafkaDeleteConsumerGroupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaDeleteConsumerGroupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "ManagedKafkaBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "consumer_group_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from ManagedKafkaBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists, NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.managedkafka_v1 import types\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete a single consumer group.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param location: Required. The ID of the Google Cloud region that the service belongs to.\n:param cluster_id: Required. The ID of the cluster whose consumer group is to be deleted.\n:param consumer_group_id: Required. The ID of the consumer group to delete.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:pa",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.450785",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "kafka",
        "bigquery",
        "managedkafkadeleteconsumergroupoperator",
        "delete",
        "group",
        "cloud",
        "managed",
        "consumer"
      ]
    },
    {
      "id": "official_cloudnaturallanguageanalyzeentitiesoperator_20260128_160330",
      "component_name": "CloudNaturalLanguageAnalyzeEntitiesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudNaturalLanguageAnalyzeEntitiesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "document",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "document",
          "type": "Union",
          "required": false
        },
        {
          "name": "encoding_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.google.cloud.hooks.natural_language import CloudNaturalLanguageHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Finds named entities in the text along with various properties.\n\nExamples properties: entity types, salience, mentions for each entity, and others.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudNaturalLanguageAnalyzeEntitiesOperator`\n\n:param document: Input document.\n    If a dict is provided, it must be of the same form as the protobuf message Document\n:param encoding_type: The encoding type used by the API to calcula",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.481770",
      "relevance_keywords": [
        "gcs",
        "cloudnaturallanguageanalyzeentitiesoperator",
        "operator",
        "google",
        "language",
        "gcp",
        "bigquery",
        "entities",
        "analyze",
        "cloud",
        "natural"
      ]
    },
    {
      "id": "official_cloudnaturallanguageanalyzeentitysentimentoperator_20260128_160330",
      "component_name": "CloudNaturalLanguageAnalyzeEntitySentimentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudNaturalLanguageAnalyzeEntitySentimentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "document",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "document",
          "type": "Union",
          "required": false
        },
        {
          "name": "encoding_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.google.cloud.hooks.natural_language import CloudNaturalLanguageHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Similar to AnalyzeEntities, also analyzes sentiment associated with each entity and its mentions.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudNaturalLanguageAnalyzeEntitySentimentOperator`\n\n:param document: Input document.\n    If a dict is provided, it must be of the same form as the protobuf message Document\n:param encoding_type: The encoding type used by the API to calculate offsets.\n:param retry: A retry object us",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.481913",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "language",
        "gcp",
        "bigquery",
        "entity",
        "sentiment",
        "analyze",
        "cloudnaturallanguageanalyzeentitysentimentoperator",
        "cloud",
        "natural"
      ]
    },
    {
      "id": "official_cloudnaturallanguageanalyzesentimentoperator_20260128_160330",
      "component_name": "CloudNaturalLanguageAnalyzeSentimentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudNaturalLanguageAnalyzeSentimentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "document",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "document",
          "type": "Union",
          "required": false
        },
        {
          "name": "encoding_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.google.cloud.hooks.natural_language import CloudNaturalLanguageHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Analyzes the sentiment of the provided text.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudNaturalLanguageAnalyzeSentimentOperator`\n\n:param document: Input document.\n    If a dict is provided, it must be of the same form as the protobuf message Document\n:param encoding_type: The encoding type used by the API to calculate offsets.\n:param retry: A retry object used to retry requests. If None is specified, requests will n",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.482024",
      "relevance_keywords": [
        "gcs",
        "cloudnaturallanguageanalyzesentimentoperator",
        "operator",
        "google",
        "language",
        "gcp",
        "bigquery",
        "sentiment",
        "analyze",
        "cloud",
        "natural"
      ]
    },
    {
      "id": "official_cloudnaturallanguageclassifytextoperator_20260128_160330",
      "component_name": "CloudNaturalLanguageClassifyTextOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudNaturalLanguageClassifyTextOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "document",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "document",
          "type": "Union",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.google.cloud.hooks.natural_language import CloudNaturalLanguageHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Classifies a document into categories.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudNaturalLanguageClassifyTextOperator`\n\n:param document: Input document.\n    If a dict is provided, it must be of the same form as the protobuf message Document\n:param retry: A retry object used to retry requests. If None is specified, requests will not be\n    retried.\n:param timeout: The amount of time, in seconds, to wait for the reque",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.482138",
      "relevance_keywords": [
        "text",
        "gcs",
        "operator",
        "google",
        "language",
        "classify",
        "gcp",
        "bigquery",
        "cloudnaturallanguageclassifytextoperator",
        "cloud",
        "natural"
      ]
    },
    {
      "id": "official_pubsubcreatetopicoperator_20260128_160330",
      "component_name": "PubSubCreateTopicOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "PubSubCreateTopicOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "topic",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "topic",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "fail_if_exists",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "message_storage_policy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "kms_key_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "schema_settings",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "message_retention_duration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a PubSub topic.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:PubSubCreateTopicOperator`\n\nBy default, if the topic already exists, this operator will\nnot cause the DAG to fail. ::\n\n    with DAG(\"successful DAG\") as dag:\n        create_topic = PubSubCreateTopicOperator(project_id=\"my-project\", topic=\"my_new_topic\")\n        create_topic_again = PubSubCreateTopicOperator(project_id=\"my-project\", topic=\"my_new_topic\")\n\n ",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.515969",
      "relevance_keywords": [
        "gcs",
        "pubsubcreatetopicoperator",
        "operator",
        "google",
        "gcp",
        "create",
        "pub",
        "bigquery",
        "sub",
        "topic",
        "cloud"
      ]
    },
    {
      "id": "official_pubsubcreatesubscriptionoperator_20260128_160330",
      "component_name": "PubSubCreateSubscriptionOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "PubSubCreateSubscriptionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "topic",
          "subscription",
          "subscription_project_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "topic",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "subscription",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "subscription_project_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ack_deadline_secs",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "fail_if_exists",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "push_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retain_acked_messages",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "message_retention_duration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "enable_message_ordering",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "expiration_policy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "dead_letter_policy",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "pubsub_hook",
          "execute",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Create a PubSub subscription.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:PubSubCreateSubscriptionOperator`\n\nBy default, the subscription will be created in ``project_id``. If\n``subscription_project_id`` is specified and the Google Cloud credentials allow, the\nSubscription can be created in a different project from its topic.\n\nBy default, if the subscription already exists, this operator will\nnot cause the DAG to fail. Ho",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.516304",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "pub",
        "bigquery",
        "sub",
        "pubsubcreatesubscriptionoperator",
        "cloud",
        "subscription"
      ]
    },
    {
      "id": "official_pubsubdeletetopicoperator_20260128_160330",
      "component_name": "PubSubDeleteTopicOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "PubSubDeleteTopicOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "topic",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "topic",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "fail_if_not_exists",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a PubSub topic.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:PubSubDeleteTopicOperator`\n\nBy default, if the topic does not exist, this operator will\nnot cause the DAG to fail. ::\n\n    with DAG(\"successful DAG\") as dag:\n        PubSubDeleteTopicOperator(project_id=\"my-project\", topic=\"non_existing_topic\")\n\nThe operator can be configured to fail if the topic does not exist. ::\n\n    with DAG(\"failing DAG\") as dag:\n    ",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.516529",
      "relevance_keywords": [
        "gcs",
        "pubsubdeletetopicoperator",
        "operator",
        "google",
        "gcp",
        "pub",
        "bigquery",
        "sub",
        "delete",
        "topic",
        "cloud"
      ]
    },
    {
      "id": "official_pubsubdeletesubscriptionoperator_20260128_160330",
      "component_name": "PubSubDeleteSubscriptionOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "PubSubDeleteSubscriptionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "subscription",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "subscription",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "fail_if_not_exists",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a PubSub subscription.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:PubSubDeleteSubscriptionOperator`\n\nBy default, if the subscription does not exist, this operator will\nnot cause the DAG to fail. ::\n\n    with DAG(\"successful DAG\") as dag:\n        PubSubDeleteSubscriptionOperator(project_id=\"my-project\", subscription=\"non-existing\")\n\nThe operator can be configured to fail if the subscription already exists.\n\n::\n\n   ",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.516729",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "pub",
        "bigquery",
        "sub",
        "delete",
        "pubsubdeletesubscriptionoperator",
        "cloud",
        "subscription"
      ]
    },
    {
      "id": "official_pubsubpublishmessageoperator_20260128_160330",
      "component_name": "PubSubPublishMessageOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "PubSubPublishMessageOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "topic",
          "messages",
          "enable_message_ordering",
          "enable_open_telemetry_tracing",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "topic",
          "type": "str",
          "required": false
        },
        {
          "name": "messages",
          "type": "list",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "enable_message_ordering",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "enable_open_telemetry_tracing",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "pubsub_hook",
          "execute",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Publish messages to a PubSub topic.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:PubSubPublishMessageOperator`\n\nEach Task publishes all provided messages to the same topic\nin a single Google Cloud project. If the topic does not exist, this\ntask will fail. ::\n\n    m1 = {\"data\": b\"Hello, World!\", \"attributes\": {\"type\": \"greeting\"}}\n    m2 = {\"data\": b\"Knock, knock\"}\n    m3 = {\"attributes\": {\"foo\": \"\"}}\n    m4 = {\"data\": b\"Wh",
      "success_score": 205,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.516913",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "pubsubpublishmessageoperator",
        "pub",
        "message",
        "bigquery",
        "sub",
        "publish",
        "cloud"
      ]
    },
    {
      "id": "official_pubsubpulloperator_20260128_160330",
      "component_name": "PubSubPullOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "PubSubPullOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "subscription",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "subscription",
          "type": "str",
          "required": false
        },
        {
          "name": "max_messages",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "ack_messages",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "messages_callback",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 300
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "execute_complete",
          "_default_message_callback",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Pulls messages from a PubSub subscription and passes them through XCom.\n\nIf the queue is empty, returns empty list - never waits for messages.\nIf you do need to wait, please use :class:`airflow.providers.google.cloud.sensors.PubSubPullSensor`\ninstead.\n\n.. seealso::\n    For more information on how to use this operator and the PubSubPullSensor, take a look at the guide:\n    :ref:`howto/operator:PubSubPullSensor`\n\nThis operator will pull up to ``max_messages`` messages from the\nspecified PubSub sub",
      "success_score": 199,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.517093",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "pub",
        "pubsubpulloperator",
        "bigquery",
        "sub",
        "pull",
        "cloud"
      ]
    },
    {
      "id": "official_spannerdeployinstanceoperator_20260128_160330",
      "component_name": "SpannerDeployInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SpannerDeployInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "configuration_name",
          "display_name",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "configuration_name",
          "type": "str",
          "required": false
        },
        {
          "name": "node_count",
          "type": "int",
          "required": false
        },
        {
          "name": "display_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.spanner import SpannerHook\nfrom airflow.providers.google.cloud.links.spanner import SpannerDatabaseLink, SpannerInstanceLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create or update a Cloud Spanner instance.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SpannerDeployInstanceOperator`\n\n:param instance_id: Cloud Spanner instance ID.\n:param configuration_name:  The name of the Cloud Spanner instance configuration\n  defining how the instance will be created. Required for\n  instances that do not yet exist.\n:param node_count: (Optional) The number of nodes allocated to the Cloud Spanner\n  in",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.547238",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "cloud",
        "deploy",
        "spanner",
        "spannerdeployinstanceoperator"
      ]
    },
    {
      "id": "official_spannerdeleteinstanceoperator_20260128_160330",
      "component_name": "SpannerDeleteInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SpannerDeleteInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.spanner import SpannerHook\nfrom airflow.providers.google.cloud.links.spanner import SpannerDatabaseLink, SpannerInstanceLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete a Cloud Spanner instance; if an instance does not exist, no action is taken and the task succeeds.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SpannerDeleteInstanceOperator`\n\n:param instance_id: The Cloud Spanner instance ID.\n:param project_id: Optional, the ID of the project that owns the Cloud Spanner\n    Database.  If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param",
      "success_score": 187,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.547684",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "delete",
        "spannerdeleteinstanceoperator",
        "cloud",
        "spanner"
      ]
    },
    {
      "id": "official_spannerquerydatabaseinstanceoperator_20260128_160330",
      "component_name": "SpannerQueryDatabaseInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SpannerQueryDatabaseInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "database_id",
          "query",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "database_id",
          "type": "str",
          "required": false
        },
        {
          "name": "query",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.spanner import SpannerHook\nfrom airflow.providers.google.cloud.links.spanner import SpannerDatabaseLink, SpannerInstanceLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "hook",
          "_validate_inputs",
          "execute",
          "sanitize_queries",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Executes an arbitrary DML query (INSERT, UPDATE, DELETE).\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SpannerQueryDatabaseInstanceOperator`\n\n:param instance_id: The Cloud Spanner instance ID.\n:param database_id: The Cloud Spanner database ID.\n:param query: The query or list of queries to be executed. Can be a path to a SQL\n   file.\n:param project_id: Optional, the ID of the project that owns the Cloud Spanner\n    Database",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.547989",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "cloud",
        "database",
        "spannerquerydatabaseinstanceoperator",
        "spanner",
        "query"
      ]
    },
    {
      "id": "official_spannerdeploydatabaseinstanceoperator_20260128_160330",
      "component_name": "SpannerDeployDatabaseInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SpannerDeployDatabaseInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "database_id",
          "ddl_statements",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "database_id",
          "type": "str",
          "required": false
        },
        {
          "name": "ddl_statements",
          "type": "list[...]",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.spanner import SpannerHook\nfrom airflow.providers.google.cloud.links.spanner import SpannerDatabaseLink, SpannerInstanceLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new Cloud Spanner database; if database exists, the operator does nothing.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SpannerDeployDatabaseInstanceOperator`\n\n:param instance_id: The Cloud Spanner instance ID.\n:param database_id: The Cloud Spanner database ID.\n:param ddl_statements: The string list containing DDL for the new database.\n:param project_id: Optional, the ID of the project that owns the Cloud Spanner",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.548226",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "database",
        "cloud",
        "spannerdeploydatabaseinstanceoperator",
        "deploy",
        "spanner"
      ]
    },
    {
      "id": "official_spannerupdatedatabaseinstanceoperator_20260128_160330",
      "component_name": "SpannerUpdateDatabaseInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SpannerUpdateDatabaseInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "database_id",
          "ddl_statements",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "database_id",
          "type": "str",
          "required": false
        },
        {
          "name": "ddl_statements",
          "type": "list[...]",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "operation_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.spanner import SpannerHook\nfrom airflow.providers.google.cloud.links.spanner import SpannerDatabaseLink, SpannerInstanceLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates a Cloud Spanner database with the specified DDL statement.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SpannerUpdateDatabaseInstanceOperator`\n\n:param instance_id: The Cloud Spanner instance ID.\n:param database_id: The Cloud Spanner database ID.\n:param ddl_statements: The string list containing DDL to apply to the database.\n:param project_id: Optional, the ID of the project that owns the Cloud Spanner\n    Database.",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.548503",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "instance",
        "bigquery",
        "spannerupdatedatabaseinstanceoperator",
        "cloud",
        "database",
        "spanner"
      ]
    },
    {
      "id": "official_spannerdeletedatabaseinstanceoperator_20260128_160330",
      "component_name": "SpannerDeleteDatabaseInstanceOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SpannerDeleteDatabaseInstanceOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "database_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "database_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.spanner import SpannerHook\nfrom airflow.providers.google.cloud.links.spanner import SpannerDatabaseLink, SpannerInstanceLink\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a Cloud Spanner database.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:SpannerDeleteDatabaseInstanceOperator`\n\n:param instance_id: Cloud Spanner instance ID.\n:param database_id: Cloud Spanner database ID.\n:param project_id: Optional, the ID of the project that owns the Cloud Spanner\n    Database.  If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param gcp_conn_id: The con",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.548722",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "instance",
        "bigquery",
        "spannerdeletedatabaseinstanceoperator",
        "delete",
        "cloud",
        "database",
        "spanner"
      ]
    },
    {
      "id": "official_cloudspeechtotextrecognizespeechoperator_20260128_160330",
      "component_name": "CloudSpeechToTextRecognizeSpeechOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSpeechToTextRecognizeSpeechOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "audio",
          "config",
          "project_id",
          "gcp_conn_id",
          "timeout",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "audio",
          "type": "RecognitionAudio",
          "required": false
        },
        {
          "name": "config",
          "type": "RecognitionConfig",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.speech_to_text import CloudSpeechToTextHook, RecognitionAudio\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Recognizes speech from audio file and returns it as text.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudSpeechToTextRecognizeSpeechOperator`\n\n:param config: information to the recognizer that specifies how to process the request. See more:\n    https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig\n:param audio: audio data to be recognized. See m",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.572958",
      "relevance_keywords": [
        "text",
        "gcs",
        "operator",
        "google",
        "gcp",
        "cloudspeechtotextrecognizespeechoperator",
        "bigquery",
        "to",
        "recognize",
        "speech",
        "cloud"
      ]
    },
    {
      "id": "official_stackdriverlistalertpoliciesoperator_20260128_160330",
      "component_name": "StackdriverListAlertPoliciesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverListAlertPoliciesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "filter_",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "format_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel\nfrom airflow.providers.google.cloud.hooks.stackdriver import StackdriverHook\nfrom airflow.providers.google.cloud.links.stackdriver import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Fetches all the Alert Policies identified by the filter passed as filter parameter.\n\nThe desired return type can be specified by the format parameter, the supported\nformats are \"dict\", \"json\" and None which returns python dictionary, stringified\nJSON and protobuf respectively.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StackdriverListAlertPoliciesOperator`\n\n:param format_: (Optional) Desired output format of the result. ",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.608273",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "stackdriverlistalertpoliciesoperator",
        "bigquery",
        "policies",
        "alert",
        "list",
        "cloud",
        "stackdriver"
      ]
    },
    {
      "id": "official_stackdriverenablealertpoliciesoperator_20260128_160330",
      "component_name": "StackdriverEnableAlertPoliciesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverEnableAlertPoliciesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "filter_",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "filter_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel\nfrom airflow.providers.google.cloud.hooks.stackdriver import StackdriverHook\nfrom airflow.providers.google.cloud.links.stackdriver import (",
        "class_attributes": [
          "ui_color",
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Enables one or more disabled alerting policies identified by filter parameter.\n\nInoperative in case the policy is already enabled.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StackdriverEnableAlertPoliciesOperator`\n\n:param filter_:  If provided, this field specifies the criteria that\n    must be met by alert policies to be enabled.\n    For more details, see https://cloud.google.com/monitoring/api/v3/sorting-and-filtering.",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.608854",
      "relevance_keywords": [
        "gcs",
        "stackdriverenablealertpoliciesoperator",
        "operator",
        "google",
        "gcp",
        "enable",
        "bigquery",
        "policies",
        "alert",
        "cloud",
        "stackdriver"
      ]
    },
    {
      "id": "official_stackdriverdisablealertpoliciesoperator_20260128_160330",
      "component_name": "StackdriverDisableAlertPoliciesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverDisableAlertPoliciesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "filter_",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "filter_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel\nfrom airflow.providers.google.cloud.hooks.stackdriver import StackdriverHook\nfrom airflow.providers.google.cloud.links.stackdriver import (",
        "class_attributes": [
          "ui_color",
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Disables one or more enabled alerting policies identified by filter parameter.\n\nInoperative in case the policy is already disabled.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StackdriverDisableAlertPoliciesOperator`\n\n:param filter_:  If provided, this field specifies the criteria that\n    must be met by alert policies to be disabled.\n    For more details, see https://cloud.google.com/monitoring/api/v3/sorting-and-filteri",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.609430",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "policies",
        "alert",
        "cloud",
        "stackdriver",
        "disable",
        "stackdriverdisablealertpoliciesoperator"
      ]
    },
    {
      "id": "official_stackdriverupsertalertoperator_20260128_160330",
      "component_name": "StackdriverUpsertAlertOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverUpsertAlertOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "alerts",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "alerts",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel\nfrom airflow.providers.google.cloud.hooks.stackdriver import StackdriverHook\nfrom airflow.providers.google.cloud.links.stackdriver import (",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "operator_extra_links",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new alert or updates an existing policy identified the name field in the alerts parameter.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StackdriverUpsertAlertOperator`\n\n:param alerts: A JSON string or file that specifies all the alerts that needs\n    to be either created or updated. For more details, see\n    https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.alertPolicies#AlertPolicy.\n    (templated)",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.609942",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "upsert",
        "gcp",
        "bigquery",
        "stackdriverupsertalertoperator",
        "alert",
        "cloud",
        "stackdriver"
      ]
    },
    {
      "id": "official_stackdriverdeletealertoperator_20260128_160330",
      "component_name": "StackdriverDeleteAlertOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverDeleteAlertOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "name",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel\nfrom airflow.providers.google.cloud.hooks.stackdriver import StackdriverHook\nfrom airflow.providers.google.cloud.links.stackdriver import (",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes an alerting policy.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StackdriverDeleteAlertOperator`\n\n:param name: The alerting policy to delete. The format is:\n                 ``projects/[PROJECT_ID]/alertPolicies/[ALERT_POLICY_ID]``.\n:param retry: A retry object used to retry requests. If ``None`` is\n    specified, requests will be retried using a default configuration.\n:param timeout: The amount of time, in seconds",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.610478",
      "relevance_keywords": [
        "gcs",
        "operator",
        "stackdriverdeletealertoperator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "alert",
        "cloud",
        "stackdriver"
      ]
    },
    {
      "id": "official_stackdriverlistnotificationchannelsoperator_20260128_160330",
      "component_name": "StackdriverListNotificationChannelsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverListNotificationChannelsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "filter_",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "format_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel\nfrom airflow.providers.google.cloud.hooks.stackdriver import StackdriverHook\nfrom airflow.providers.google.cloud.links.stackdriver import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Fetches all the Notification Channels identified by the filter passed as filter parameter.\n\nThe desired return type can be specified by the format parameter, the\nsupported formats are \"dict\", \"json\" and None which returns python\ndictionary, stringified JSON and protobuf respectively.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StackdriverListNotificationChannelsOperator`\n\n:param format_: (Optional) Desired output format o",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.610991",
      "relevance_keywords": [
        "stackdriverlistnotificationchannelsoperator",
        "gcs",
        "operator",
        "google",
        "channels",
        "gcp",
        "bigquery",
        "list",
        "cloud",
        "stackdriver",
        "notification"
      ]
    },
    {
      "id": "official_stackdriverenablenotificationchannelsoperator_20260128_160330",
      "component_name": "StackdriverEnableNotificationChannelsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverEnableNotificationChannelsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "filter_",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "filter_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel\nfrom airflow.providers.google.cloud.hooks.stackdriver import StackdriverHook\nfrom airflow.providers.google.cloud.links.stackdriver import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Enables one or more disabled alerting policies identified by filter parameter.\n\nInoperative in case the policy is already enabled.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StackdriverEnableNotificationChannelsOperator`\n\n:param filter_:  If provided, this field specifies the criteria that\n    must be met by notification channels to be enabled.\n    For more details, see https://cloud.google.com/monitoring/api/v3/sorting-",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.611500",
      "relevance_keywords": [
        "gcs",
        "operator",
        "stackdriverenablenotificationchannelsoperator",
        "google",
        "channels",
        "gcp",
        "enable",
        "bigquery",
        "cloud",
        "stackdriver",
        "notification"
      ]
    },
    {
      "id": "official_stackdriverdisablenotificationchannelsoperator_20260128_160330",
      "component_name": "StackdriverDisableNotificationChannelsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverDisableNotificationChannelsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "filter_",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "filter_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel\nfrom airflow.providers.google.cloud.hooks.stackdriver import StackdriverHook\nfrom airflow.providers.google.cloud.links.stackdriver import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Disables one or more enabled notification channels identified by filter parameter.\n\nInoperative in case the policy is already disabled.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StackdriverDisableNotificationChannelsOperator`\n\n:param filter_:  If provided, this field specifies the criteria that\n    must be met by alert policies to be disabled.\n    For more details, see https://cloud.google.com/monitoring/api/v3/sorting-",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.611978",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "channels",
        "gcp",
        "bigquery",
        "cloud",
        "stackdriver",
        "stackdriverdisablenotificationchannelsoperator",
        "disable",
        "notification"
      ]
    },
    {
      "id": "official_stackdriverupsertnotificationchanneloperator_20260128_160330",
      "component_name": "StackdriverUpsertNotificationChannelOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverUpsertNotificationChannelOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "channels",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "channels",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel\nfrom airflow.providers.google.cloud.hooks.stackdriver import StackdriverHook\nfrom airflow.providers.google.cloud.links.stackdriver import (",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "operator_extra_links",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a new notification or updates an existing notification channel.\n\nChannel is identified by the name field in the alerts parameter.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StackdriverUpsertNotificationChannelOperator`\n\n:param channels: A JSON string or file that specifies all the alerts that needs\n    to be either created or updated. For more details, see\n    https://cloud.google.com/monitoring/api/ref_v3/rest/v3",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.612511",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "upsert",
        "channel",
        "gcp",
        "bigquery",
        "cloud",
        "stackdriver",
        "stackdriverupsertnotificationchanneloperator",
        "notification"
      ]
    },
    {
      "id": "official_stackdriverdeletenotificationchanneloperator_20260128_160330",
      "component_name": "StackdriverDeleteNotificationChannelOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverDeleteNotificationChannelOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "name",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel\nfrom airflow.providers.google.cloud.hooks.stackdriver import StackdriverHook\nfrom airflow.providers.google.cloud.links.stackdriver import (",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a notification channel.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:StackdriverDeleteNotificationChannelOperator`\n\n:param name: The alerting policy to delete. The format is:\n                 ``projects/[PROJECT_ID]/notificationChannels/[CHANNEL_ID]``.\n:param retry: A retry object used to retry requests. If ``None`` is\n    specified, requests will be retried using a default configuration.\n:param timeout: The amount",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.613023",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "channel",
        "gcp",
        "bigquery",
        "stackdriverdeletenotificationchanneloperator",
        "delete",
        "cloud",
        "stackdriver",
        "notification"
      ]
    },
    {
      "id": "official_cloudtasksqueuecreateoperator_20260128_160330",
      "component_name": "CloudTasksQueueCreateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksQueueCreateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "task_queue",
          "project_id",
          "location",
          "queue_name",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "task_queue",
          "type": "Queue",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "queue_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a queue in Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksQueueCreateOperator`\n\n:param location: The location name in which the queue will be created.\n:param task_queue: The task queue to create.\n    Queue's name cannot be the same as an existing queue.\n    If a dict is provided, it must be of the same form as the protobuf message Queue.\n:param project_id: (Optional) The ID of the Google Cloud ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.654749",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "operator",
        "google",
        "gcp",
        "queue",
        "cloudtasksqueuecreateoperator",
        "create",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtasksqueueupdateoperator_20260128_160330",
      "component_name": "CloudTasksQueueUpdateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksQueueUpdateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "task_queue",
          "project_id",
          "location",
          "queue_name",
          "update_mask",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "task_queue",
          "type": "Queue",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "queue_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates a queue in Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksQueueUpdateOperator`\n\n:param task_queue: The task queue to update.\n    This method creates the queue if it does not exist and updates the queue if\n    it does exist. The queue's name must be specified.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default pr",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.655298",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "operator",
        "google",
        "update",
        "queue",
        "gcp",
        "bigquery",
        "cloudtasksqueueupdateoperator",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtasksqueuegetoperator_20260128_160330",
      "component_name": "CloudTasksQueueGetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksQueueGetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "queue_name",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "queue_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets a queue from Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksQueueGetOperator`\n\n:param location: The location name in which the queue was created.\n:param queue_name: The queue's name.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param retry: (Optional) A re",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.655808",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "operator",
        "google",
        "gcp",
        "queue",
        "bigquery",
        "get",
        "cloud",
        "cloudtasksqueuegetoperator"
      ]
    },
    {
      "id": "official_cloudtasksqueueslistoperator_20260128_160330",
      "component_name": "CloudTasksQueuesListOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksQueuesListOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "results_filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists queues from Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksQueuesListOperator`\n\n:param location: The location name in which the queues were created.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param results_filter: (Optional) Filter used to specify a sub",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.656295",
      "relevance_keywords": [
        "cloudtasksqueueslistoperator",
        "gcs",
        "tasks",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "list",
        "cloud",
        "queues"
      ]
    },
    {
      "id": "official_cloudtasksqueuedeleteoperator_20260128_160330",
      "component_name": "CloudTasksQueueDeleteOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksQueueDeleteOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "queue_name",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "queue_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a queue from Cloud Tasks, even if it has tasks in it.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksQueueDeleteOperator`\n\n:param location: The location name in which the queue will be deleted.\n:param queue_name: The queue's name.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default project_id from the Google Cloud connection ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.656812",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "cloudtasksqueuedeleteoperator",
        "operator",
        "google",
        "gcp",
        "queue",
        "bigquery",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtasksqueuepurgeoperator_20260128_160330",
      "component_name": "CloudTasksQueuePurgeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksQueuePurgeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "queue_name",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "queue_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Purges a queue by deleting all of its tasks from Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksQueuePurgeOperator`\n\n:param location: The location name in which the queue will be purged.\n:param queue_name: The queue's name.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default project_id from the Google Cloud connection is",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.657319",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "operator",
        "google",
        "gcp",
        "queue",
        "cloudtasksqueuepurgeoperator",
        "bigquery",
        "purge",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtasksqueuepauseoperator_20260128_160330",
      "component_name": "CloudTasksQueuePauseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksQueuePauseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "queue_name",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "queue_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Pauses a queue in Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksQueuePauseOperator`\n\n:param location: The location name in which the queue will be paused.\n:param queue_name: The queue's name.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param retry: (Optional)",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.657860",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "cloudtasksqueuepauseoperator",
        "operator",
        "google",
        "gcp",
        "queue",
        "bigquery",
        "pause",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtasksqueueresumeoperator_20260128_160330",
      "component_name": "CloudTasksQueueResumeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksQueueResumeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "queue_name",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "queue_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Resumes a queue in Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksQueueResumeOperator`\n\n:param location: The location name in which the queue will be resumed.\n:param queue_name: The queue's name.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param retry: (Option",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.658374",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "cloudtasksqueueresumeoperator",
        "operator",
        "google",
        "queue",
        "gcp",
        "bigquery",
        "resume",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtaskstaskcreateoperator_20260128_160330",
      "component_name": "CloudTasksTaskCreateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksTaskCreateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "task",
          "project_id",
          "location",
          "queue_name",
          "task_name",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "queue_name",
          "type": "str",
          "required": false
        },
        {
          "name": "task",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "task_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "response_view",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a task in Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksTaskCreateOperator`\n\n:param location: The location name in which the task will be created.\n:param queue_name: The queue's name.\n:param task: The task to add.\n    If a dict is provided, it must be of the same form as the protobuf message Task.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    I",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.658936",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "operator",
        "google",
        "task",
        "create",
        "gcp",
        "bigquery",
        "cloudtaskstaskcreateoperator",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtaskstaskgetoperator_20260128_160330",
      "component_name": "CloudTasksTaskGetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksTaskGetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "queue_name",
          "task_name",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "queue_name",
          "type": "str",
          "required": false
        },
        {
          "name": "task_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "response_view",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets a task from Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksTaskGetOperator`\n\n:param location: The location name in which the task was created.\n:param queue_name: The queue's name.\n:param task_name: The task's name.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default project_id from the Google Cloud connection is use",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.659460",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "operator",
        "google",
        "task",
        "gcp",
        "bigquery",
        "cloudtaskstaskgetoperator",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtaskstaskslistoperator_20260128_160330",
      "component_name": "CloudTasksTasksListOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksTasksListOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "queue_name",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "queue_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "response_view",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists the tasks in Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksTasksListOperator`\n\n:param location: The location name in which the tasks were created.\n:param queue_name: The queue's name.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param response_view: (Opt",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.659943",
      "relevance_keywords": [
        "cloudtaskstaskslistoperator",
        "tasks",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtaskstaskdeleteoperator_20260128_160330",
      "component_name": "CloudTasksTaskDeleteOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksTaskDeleteOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "queue_name",
          "task_name",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "queue_name",
          "type": "str",
          "required": false
        },
        {
          "name": "task_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a task from Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksTaskDeleteOperator`\n\n:param location: The location name in which the task will be deleted.\n:param queue_name: The queue's name.\n:param task_name: The task's name.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default project_id from the Google Cloud connect",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.660469",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "cloudtaskstaskdeleteoperator",
        "operator",
        "google",
        "task",
        "gcp",
        "bigquery",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtaskstaskrunoperator_20260128_160330",
      "component_name": "CloudTasksTaskRunOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksTaskRunOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "queue_name",
          "task_name",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "queue_name",
          "type": "str",
          "required": false
        },
        {
          "name": "task_name",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "response_view",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.cloud.links.cloud_tasks import CloudTasksLink, CloudTasksQueueLink",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Forces to run a task in Cloud Tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTasksTaskRunOperator`\n\n:param location: The location name in which the task was created.\n:param queue_name: The queue's name.\n:param task_name: The task's name.\n:param project_id: (Optional) The ID of the Google Cloud project that owns the Cloud Tasks.\n    If set to None or missing, the default project_id from the Google Cloud connection",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.661041",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "operator",
        "google",
        "task",
        "gcp",
        "bigquery",
        "cloudtaskstaskrunoperator",
        "run",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtexttospeechsynthesizeoperator_20260128_160330",
      "component_name": "CloudTextToSpeechSynthesizeOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTextToSpeechSynthesizeOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "input_data",
          "voice",
          "audio_config",
          "project_id",
          "gcp_conn_id",
          "target_bucket_name",
          "target_filename",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "input_data",
          "type": "Union",
          "required": false
        },
        {
          "name": "voice",
          "type": "Union",
          "required": false
        },
        {
          "name": "audio_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "target_bucket_name",
          "type": "str",
          "required": false
        },
        {
          "name": "target_filename",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom tempfile import NamedTemporaryFile\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.cloud.hooks.text_to_speech import CloudTextToSpeechHook",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Synthesizes text to speech and stores it in Google Cloud Storage.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTextToSpeechSynthesizeOperator`\n\n:param input_data: text input to be synthesized. See more:\n    https://googleapis.github.io/google-cloud-python/latest/texttospeech/gapic/v1/types.html#google.cloud.texttospeech_v1.types.SynthesisInput\n:param voice: configuration of voice to be used in synthesis. See more:\n   ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.697037",
      "relevance_keywords": [
        "cloudtexttospeechsynthesizeoperator",
        "text",
        "gcs",
        "synthesize",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "to",
        "speech",
        "cloud"
      ]
    },
    {
      "id": "official_cloudtranslatetextoperator_20260128_160330",
      "component_name": "CloudTranslateTextOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTranslateTextOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "values",
          "target_language",
          "format_",
          "source_language",
          "model",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "values",
          "type": "Union",
          "required": false
        },
        {
          "name": "target_language",
          "type": "str",
          "required": false
        },
        {
          "name": "format_",
          "type": "str",
          "required": false
        },
        {
          "name": "source_language",
          "type": "Union",
          "required": false
        },
        {
          "name": "model",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Translate a string or list of strings.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTranslateTextOperator`\n\nSee https://cloud.google.com/translate/docs/translating-text\n\nExecute method returns str or list.\n\nThis is a list of dictionaries for each queried value. Each\ndictionary typically contains three keys (though not all will be present in all cases):\n\n* ``detectedSourceLanguage``: The detected language (as an ISO 63",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.739392",
      "relevance_keywords": [
        "text",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "cloudtranslatetextoperator",
        "cloud"
      ]
    },
    {
      "id": "official_translatetextoperator_20260128_160330",
      "component_name": "TranslateTextOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateTextOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "contents",
          "target_language_code",
          "mime_type",
          "source_language_code",
          "model",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "contents",
          "type": "Sequence[...]",
          "required": false
        },
        {
          "name": "source_language_code",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "target_language_code",
          "type": "str",
          "required": false
        },
        {
          "name": "mime_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "model",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "transliteration_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "glossary_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Translate text content of moderate amount, for larger volumes of text please use the TranslateTextBatchOperator.\n\nWraps the Google cloud Translate Text (Advanced) functionality.\nSee https://cloud.google.com/translate/docs/advanced/translating-text-v3\n\nFor more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:TranslateTextOperator`.\n\n:param project_id: Optional. The ID of the Google Cloud project that the\n    service belongs to.\n    If not provided defau",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.740109",
      "relevance_keywords": [
        "text",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "cloud",
        "translatetextoperator"
      ]
    },
    {
      "id": "official_translatetextbatchoperator_20260128_160330",
      "component_name": "TranslateTextBatchOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateTextBatchOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "input_configs",
          "target_language_codes",
          "source_language_code",
          "models",
          "glossaries",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "target_language_codes",
          "type": "MutableSequence[...]",
          "required": false
        },
        {
          "name": "source_language_code",
          "type": "str",
          "required": false
        },
        {
          "name": "input_configs",
          "type": "MutableSequence[...]",
          "required": false
        },
        {
          "name": "output_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "models",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "glossaries",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Translate large volumes of text content, by the inputs provided.\n\nWraps the Google cloud Translate Text (Advanced) functionality.\nSee https://cloud.google.com/translate/docs/advanced/batch-translation\n\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateTextBatchOperator`.\n\n:param project_id: Optional. The ID of the Google Cloud project that the\n    service belongs to. If not specified the hook project_id will be used.\n:param location: requir",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.740745",
      "relevance_keywords": [
        "text",
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "translatetextbatchoperator",
        "translate",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_translatecreatedatasetoperator_20260128_160330",
      "component_name": "TranslateCreateDatasetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateCreateDatasetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset",
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "dataset",
          "type": "Union",
          "required": false
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a Google Cloud Translate dataset.\n\nCreates a `native` translation dataset, using API V3.\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateCreateDatasetOperator`.\n\n:param dataset: The dataset to create. If a dict is provided, it must correspond to\n    the automl_translation.Dataset type.\n:param project_id: ID of the Google Cloud project where dataset is located.\n    If not provided default project_id is used.\n:param location: The loc",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.741324",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "translate",
        "translatecreatedatasetoperator",
        "cloud",
        "dataset"
      ]
    },
    {
      "id": "official_translatedatasetslistoperator_20260128_160330",
      "component_name": "TranslateDatasetsListOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateDatasetsListOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get a list of native Google Cloud Translation datasets in a project.\n\nGet project's list of `native` translation datasets, using API V3.\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateDatasetsListOperator`.\n\n:param project_id: ID of the Google Cloud project where dataset is located.\n    If not provided default project_id is used.\n:param location: The location of the project.\n:param retry: Designation of what errors, if any, should be ret",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.741892",
      "relevance_keywords": [
        "datasets",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "translatedatasetslistoperator",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_translateimportdataoperator_20260128_160330",
      "component_name": "TranslateImportDataOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateImportDataOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator",
          "DatasetImportDataResultsCheckHelper"
        ],
        "template_fields": [
          "dataset_id",
          "input_config",
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "input_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "raise_for_empty_result",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator, DatasetImportDataResultsCheckHelper",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Import data to the translation dataset.\n\nLoads data to the translation dataset, using API V3.\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateImportDataOperator`.\n\n:param dataset_id: The dataset_id of target native dataset to import data to.\n:param input_config: The desired input location of translations language pairs file. If a dict provided,\n    must follow the structure of DatasetInputConfig.\n    If a dict is provided, it must be of t",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.742475",
      "relevance_keywords": [
        "gcs",
        "translateimportdataoperator",
        "operator",
        "google",
        "import",
        "gcp",
        "bigquery",
        "translate",
        "data",
        "cloud"
      ]
    },
    {
      "id": "official_translatedeletedatasetoperator_20260128_160330",
      "component_name": "TranslateDeleteDatasetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateDeleteDatasetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset_id",
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete translation dataset and all of its contents.\n\nDeletes the translation dataset and it's data, using API V3.\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateDeleteDatasetOperator`.\n\n:param dataset_id: The dataset_id of target native dataset to be deleted.\n:param location: The location of the project.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata:  St",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.743020",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "delete",
        "cloud",
        "dataset",
        "translatedeletedatasetoperator"
      ]
    },
    {
      "id": "official_translatecreatemodeloperator_20260128_160330",
      "component_name": "TranslateCreateModelOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateCreateModelOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "dataset_id",
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "display_name",
          "type": "str",
          "required": false
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a Google Cloud Translate model.\n\nCreates a `native` translation model, using API V3.\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateCreateModelOperator`.\n\n:param dataset_id: The dataset id used for model training.\n:param project_id: ID of the Google Cloud project where dataset is located.\n    If not provided default project_id is used.\n:param location: The location of the project.\n:param retry: Designation of what errors, if any,",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.743575",
      "relevance_keywords": [
        "gcs",
        "model",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "translate",
        "cloud",
        "translatecreatemodeloperator"
      ]
    },
    {
      "id": "official_translatemodelslistoperator_20260128_160330",
      "component_name": "TranslateModelsListOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateModelsListOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get a list of native Google Cloud Translation models in a project.\n\nGet project's list of `native` translation models, using API V3.\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateModelsListOperator`.\n\n:param project_id: ID of the Google Cloud project where dataset is located.\n    If not provided default project_id is used.\n:param location: The location of the project.\n:param retry: Designation of what errors, if any, should be retried.\n",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.744111",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "translatemodelslistoperator",
        "translate",
        "bigquery",
        "models",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_translatedeletemodeloperator_20260128_160330",
      "component_name": "TranslateDeleteModelOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateDeleteModelOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "model_id",
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "model_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete translation model and all of its contents.\n\nDeletes the translation model and it's data, using API V3.\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateDeleteModelOperator`.\n\n:param model_id: The model_id of target native model to be deleted.\n:param location: The location of the project.\n:param retry: Designation of what errors, if any, should be retried.\n:param timeout: The timeout for this request.\n:param metadata:  Strings which ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.744678",
      "relevance_keywords": [
        "gcs",
        "translatedeletemodeloperator",
        "model",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_translatedocumentoperator_20260128_160330",
      "component_name": "TranslateDocumentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateDocumentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "source_language_code",
          "target_language_code",
          "document_input_config",
          "document_output_config",
          "model",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "source_language_code",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "target_language_code",
          "type": "str",
          "required": false
        },
        {
          "name": "document_input_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "document_output_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "customized_attribution",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "is_translate_native_pdf_only",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "enable_shadow_removal_native_pdf",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "enable_rotation_correction",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "model",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "glossary_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Translate document provided.\n\nWraps the Google cloud Translate Text (Advanced) functionality.\nSupports wide range of input/output file types, please visit the\nhttps://cloud.google.com/translate/docs/advanced/translate-documents for more details.\n\nFor more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:TranslateDocumentOperator`.\n\n:param project_id: Optional. The ID of the Google Cloud project that the\n    service belongs to. If not specified the hook ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.745312",
      "relevance_keywords": [
        "translatedocumentoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "cloud",
        "document"
      ]
    },
    {
      "id": "official_translatedocumentbatchoperator_20260128_160330",
      "component_name": "TranslateDocumentBatchOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateDocumentBatchOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "input_configs",
          "output_config",
          "target_language_codes",
          "source_language_code",
          "models",
          "glossaries",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "source_language_code",
          "type": "str",
          "required": false
        },
        {
          "name": "target_language_codes",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "input_configs",
          "type": "MutableSequence[...]",
          "required": false
        },
        {
          "name": "output_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "customized_attribution",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "format_conversions",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "enable_shadow_removal_native_pdf",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "enable_rotation_correction",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "models",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "glossaries",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Translate documents provided via input and output configurations.\n\nUp to 10 target languages per operation supported.\nWraps the Google cloud Translate Text (Advanced) functionality.\nSee https://cloud.google.com/translate/docs/advanced/batch-translation.\n\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateDocumentBatchOperator`.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param source_languag",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.745904",
      "relevance_keywords": [
        "batch",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "translatedocumentbatchoperator",
        "cloud",
        "document"
      ]
    },
    {
      "id": "official_translatecreateglossaryoperator_20260128_160330",
      "component_name": "TranslateCreateGlossaryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateCreateGlossaryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "glossary_id",
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "glossary_id",
          "type": "str",
          "required": false
        },
        {
          "name": "input_config",
          "type": "Union",
          "required": false
        },
        {
          "name": "language_pair",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "language_codes_set",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a Google Cloud Translation Glossary.\n\nCreates a translation glossary, using API V3.\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateCreateGlossaryOperator`.\n\n:param glossary_id: User-specified id to built glossary resource name.\n:param input_config: The input configuration of examples to built glossary from.\n    Total glossary must not exceed 10M Unicode codepoints.\n    The headers should not be included into the input file table,",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.746475",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "translate",
        "translatecreateglossaryoperator",
        "glossary",
        "cloud"
      ]
    },
    {
      "id": "official_translateupdateglossaryoperator_20260128_160330",
      "component_name": "TranslateUpdateGlossaryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateUpdateGlossaryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "glossary_id",
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "glossary_id",
          "type": "str",
          "required": false
        },
        {
          "name": "new_display_name",
          "type": "str",
          "required": false
        },
        {
          "name": "new_input_config",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Update glossary item with values provided.\n\nUpdates the translation glossary, using translation API V3.\nOnly ``display_name`` and ``input_config`` fields are allowed for update.\n\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateUpdateGlossaryOperator`.\n\n:param glossary_id: User-specified id to built glossary resource name.\n:param input_config: The input configuration of examples to built glossary from.\n    Total glossary must not exceed 10",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.747003",
      "relevance_keywords": [
        "gcs",
        "translateupdateglossaryoperator",
        "operator",
        "google",
        "update",
        "gcp",
        "bigquery",
        "translate",
        "glossary",
        "cloud"
      ]
    },
    {
      "id": "official_translatelistglossariesoperator_20260128_160330",
      "component_name": "TranslateListGlossariesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateListGlossariesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filter_str",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "operator_extra_links",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get a list of translation glossaries in a project.\n\nList the translation glossaries, using translation API V3.\n\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateListGlossariesOperator`.\n\n:param project_id: ID of the Google Cloud project where glossary is located.\n    If not provided default project_id is used.\n:param page_size: Page size requested, if not set server use appropriate default.\n:param page_token: A token identifying a page of ",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.747543",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "translatelistglossariesoperator",
        "bigquery",
        "translate",
        "list",
        "cloud",
        "glossaries"
      ]
    },
    {
      "id": "official_translatedeleteglossaryoperator_20260128_160330",
      "component_name": "TranslateDeleteGlossaryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateDeleteGlossaryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "glossary_id",
          "location",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "glossary_id",
          "type": "str",
          "required": false
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, cast\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook, TranslateHook\nfrom airflow.providers.google.cloud.links.translate import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete a Google Cloud Translation Glossary.\n\nDeletes a translation glossary, using API V3.\nFor more information on how to use this operator, take a look at the guide:\n:ref:`howto/operator:TranslateDeleteGlossaryOperator`.\n\n:param glossary_id: User-specified id to delete glossary resource item.\n:param project_id: ID of the Google Cloud project where glossary is located.\n    If not provided default project_id is used.\n:param location: The location of the project.\n:param retry: Designation of what ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.748071",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "delete",
        "glossary",
        "cloud",
        "translatedeleteglossaryoperator"
      ]
    },
    {
      "id": "official_cloudtranslatespeechoperator_20260128_160330",
      "component_name": "CloudTranslateSpeechOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTranslateSpeechOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "target_language",
          "format_",
          "source_language",
          "model",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "audio",
          "type": "RecognitionAudio",
          "required": false
        },
        {
          "name": "config",
          "type": "RecognitionConfig",
          "required": false
        },
        {
          "name": "target_language",
          "type": "str",
          "required": false
        },
        {
          "name": "format_",
          "type": "str",
          "required": false
        },
        {
          "name": "source_language",
          "type": "Union",
          "required": false
        },
        {
          "name": "model",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.cloud.hooks.speech_to_text import CloudSpeechToTextHook\nfrom airflow.providers.google.cloud.hooks.translate import CloudTranslateHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Recognizes speech in audio input and translates it.\n\nNote that it uses the first result from the recognition api response - the one with the highest confidence\nIn order to see other possible results please use\n:ref:`howto/operator:CloudSpeechToTextRecognizeSpeechOperator`\nand\n:ref:`howto/operator:CloudTranslateTextOperator`\nseparately\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudTranslateSpeechOperator`\n\nSee https://c",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.784634",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "speech",
        "cloud",
        "cloudtranslatespeechoperator"
      ]
    },
    {
      "id": "official_cloudvideointelligencedetectvideolabelsoperator_20260128_160330",
      "component_name": "CloudVideoIntelligenceDetectVideoLabelsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVideoIntelligenceDetectVideoLabelsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "input_uri",
          "output_uri",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "input_uri",
          "type": "str",
          "required": false
        },
        {
          "name": "input_content",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "output_uri",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "video_context",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.videointelligence_v1 import Feature, VideoContext\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.google.cloud.hooks.video_intelligence import CloudVideoIntelligenceHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Performs video annotation, annotating video labels.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVideoIntelligenceDetectVideoLabelsOperator`.\n\n:param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,\n    which must be specified in the following format: ``gs://bucket-id/object-id``.\n:param input_content: The video data bytes.\n    If unset, the input video(s) should be specified v",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.809724",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "intelligence",
        "gcp",
        "video",
        "cloudvideointelligencedetectvideolabelsoperator",
        "bigquery",
        "labels",
        "cloud",
        "detect"
      ]
    },
    {
      "id": "official_cloudvideointelligencedetectvideoexplicitcontentoperator_20260128_160330",
      "component_name": "CloudVideoIntelligenceDetectVideoExplicitContentOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVideoIntelligenceDetectVideoExplicitContentOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "input_uri",
          "output_uri",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "input_uri",
          "type": "str",
          "required": false
        },
        {
          "name": "output_uri",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "input_content",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "video_context",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.videointelligence_v1 import Feature, VideoContext\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.google.cloud.hooks.video_intelligence import CloudVideoIntelligenceHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Performs video annotation, annotating explicit content.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVideoIntelligenceDetectVideoExplicitContentOperator`\n\n:param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,\n    which must be specified in the following format: ``gs://bucket-id/object-id``.\n:param input_content: The video data bytes.\n    If unset, the input video(s) should be",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.809971",
      "relevance_keywords": [
        "content",
        "gcs",
        "operator",
        "google",
        "intelligence",
        "explicit",
        "video",
        "gcp",
        "bigquery",
        "cloudvideointelligencedetectvideoexplicitcontentoperator",
        "cloud",
        "detect"
      ]
    },
    {
      "id": "official_cloudvideointelligencedetectvideoshotsoperator_20260128_160330",
      "component_name": "CloudVideoIntelligenceDetectVideoShotsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVideoIntelligenceDetectVideoShotsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "input_uri",
          "output_uri",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "input_uri",
          "type": "str",
          "required": false
        },
        {
          "name": "output_uri",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "input_content",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "video_context",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.videointelligence_v1 import Feature, VideoContext\nfrom google.protobuf.json_format import MessageToDict\nfrom airflow.providers.google.cloud.hooks.video_intelligence import CloudVideoIntelligenceHook\nfrom airflow.providers.google.cloud.operators.cloud_base import GoogleCloudBaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Performs video annotation, annotating video shots.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVideoIntelligenceDetectVideoShotsOperator`\n\n:param input_uri: Input video location. Currently, only Google Cloud Storage URIs are supported,\n    which must be specified in the following format: ``gs://bucket-id/object-id``.\n:param input_content: The video data bytes.\n    If unset, the input video(s) should be specified via ",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.810151",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "intelligence",
        "gcp",
        "video",
        "bigquery",
        "shots",
        "cloudvideointelligencedetectvideoshotsoperator",
        "cloud",
        "detect"
      ]
    },
    {
      "id": "official_cloudvisioncreateproductsetoperator_20260128_160330",
      "component_name": "CloudVisionCreateProductSetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionCreateProductSetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "product_set_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "product_set",
          "type": "Union",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "product_set_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a new ProductSet resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionCreateProductSetOperator`\n\n:param product_set: (Required) The ProductSet to create. If a dict is provided, it must be of the same\n    form as the protobuf message `ProductSet`.\n:param location: (Required) The region where the ProductSet should be created. Valid regions\n    (as of 2019-02-05) are: us-east1, us-west1, europe-west1, asia",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.849722",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "set",
        "create",
        "gcp",
        "bigquery",
        "cloud",
        "product",
        "cloudvisioncreateproductsetoperator",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisiongetproductsetoperator_20260128_160330",
      "component_name": "CloudVisionGetProductSetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionGetProductSetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "product_set_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "product_set_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get information associated with a ProductSet.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionGetProductSetOperator`\n\n:param location: (Required) The region where the ProductSet is located. Valid regions (as of 2019-02-05)\n    are: us-east1, us-west1, europe-west1, asia-east1\n:param product_set_id: (Required) The resource id of this ProductSet.\n:param project_id: (Optional) The project in which the ProductSet is lo",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.850271",
      "relevance_keywords": [
        "gcs",
        "cloudvisiongetproductsetoperator",
        "operator",
        "google",
        "set",
        "gcp",
        "bigquery",
        "get",
        "cloud",
        "product",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisionupdateproductsetoperator_20260128_160330",
      "component_name": "CloudVisionUpdateProductSetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionUpdateProductSetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "product_set_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "product_set",
          "type": "Union",
          "required": false
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "product_set_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Make changes to a `ProductSet` resource.\n\nOnly ``display_name`` can be updated currently.\n\n.. note:: To locate the ``ProductSet`` resource, its ``name`` in the form\n    `projects/PROJECT_ID/locations/LOC_ID/productSets/PRODUCT_SET_ID` is necessary.\n\nYou can provide the ``name` directly as an attribute of the ``product_set``\nobject. You can also leave it blank, in which case ``name`` will be created\nby the operator from ``location`` and ``product_set_id`` instead (and\noptionally ``project_id``; i",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.850810",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "set",
        "gcp",
        "bigquery",
        "cloudvisionupdateproductsetoperator",
        "cloud",
        "product",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisiondeleteproductsetoperator_20260128_160330",
      "component_name": "CloudVisionDeleteProductSetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionDeleteProductSetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "product_set_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "product_set_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Permanently deletes a ``ProductSet``.\n\n``Products`` and ``ReferenceImages`` in the ``ProductSet`` are not deleted.\nThe actual image files are not deleted from Google Cloud Storage.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionDeleteProductSetOperator`\n\n:param location: (Required) The region where the ProductSet is located.\n    Valid regions (as of 2019-02-05) are: us-east1, us-west1, europe-west1, asia-east1\n:pa",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.851308",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "set",
        "gcp",
        "bigquery",
        "delete",
        "cloudvisiondeleteproductsetoperator",
        "cloud",
        "product",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisioncreateproductoperator_20260128_160330",
      "component_name": "CloudVisionCreateProductOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionCreateProductOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "product_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "product",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "product_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create and return a new product resource.\n\nPossible errors regarding the ``Product`` object provided:\n\n- Returns ``INVALID_ARGUMENT`` if ``display_name`` is missing or longer than 4096 characters.\n- Returns ``INVALID_ARGUMENT`` if ``description`` is longer than 4096 characters.\n- Returns ``INVALID_ARGUMENT`` if ``product_category`` is missing or invalid.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionCreateProduct",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.851806",
      "relevance_keywords": [
        "gcs",
        "operator",
        "cloudvisioncreateproductoperator",
        "google",
        "create",
        "gcp",
        "bigquery",
        "cloud",
        "product",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisiongetproductoperator_20260128_160330",
      "component_name": "CloudVisionGetProductOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionGetProductOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "product_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "product_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Get information associated with a ``Product``.\n\nPossible errors:\n\n- Returns `NOT_FOUND` if the `Product` does not exist.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionGetProductOperator`\n\n:param location: (Required) The region where the Product is located. Valid regions (as of 2019-02-05) are:\n    us-east1, us-west1, europe-west1, asia-east1\n:param product_id: (Required) The resource id of this Product.\n:param pr",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.852286",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "cloudvisiongetproductoperator",
        "get",
        "cloud",
        "product",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisionupdateproductoperator_20260128_160330",
      "component_name": "CloudVisionUpdateProductOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionUpdateProductOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "product_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "product",
          "type": "Union",
          "required": false
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "product_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Make changes to a Product resource.\n\nOnly the display_name, description, and labels fields can be updated right now.\n\nIf labels are updated, the change will not be reflected in queries until the next index time.\n\n.. note:: To locate the `Product` resource, its `name` in the form\n    `projects/PROJECT_ID/locations/LOC_ID/products/PRODUCT_ID` is necessary.\n\nYou can provide the `name` directly as an attribute of the `product` object. However, you can leave it\nblank and provide `location` and `produ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.852821",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "cloudvisionupdateproductoperator",
        "bigquery",
        "cloud",
        "product",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisiondeleteproductoperator_20260128_160330",
      "component_name": "CloudVisionDeleteProductOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionDeleteProductOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "project_id",
          "product_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "product_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Permanently delete a product and its reference images.\n\nMetadata of the product and all its images will be deleted right away, but\nsearch queries against ProductSets containing the product may still work\nuntil all related caches are refreshed.\n\nPossible errors:\n\n- Returns `NOT_FOUND` if the product does not exist.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionDeleteProductOperator`\n\n:param location: (Required) Th",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.853296",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "cloudvisiondeleteproductoperator",
        "cloud",
        "product",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisionimageannotateoperator_20260128_160330",
      "component_name": "CloudVisionImageAnnotateOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionImageAnnotateOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "request",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "request",
          "type": "Union",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Run image detection and annotation for an image or a batch of images.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionImageAnnotateOperator`\n\n:param request: (Required) Annotation request for image or a batch.\n    If a dict is provided, it must be of the same form as the protobuf\n    message class:`google.cloud.vision_v1.types.AnnotateImageRequest`\n:param retry: (Optional) A retry object used to retry requests. If ",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.853758",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "image",
        "cloudvisionimageannotateoperator",
        "annotate",
        "bigquery",
        "cloud",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisioncreatereferenceimageoperator_20260128_160330",
      "component_name": "CloudVisionCreateReferenceImageOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionCreateReferenceImageOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "reference_image",
          "product_id",
          "reference_image_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "reference_image",
          "type": "Union",
          "required": false
        },
        {
          "name": "product_id",
          "type": "str",
          "required": false
        },
        {
          "name": "reference_image_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create and return a new ReferenceImage ID resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionCreateReferenceImageOperator`\n\n:param location: (Required) The region where the Product is located. Valid regions (as of 2019-02-05) are:\n    us-east1, us-west1, europe-west1, asia-east1\n:param reference_image: (Required) The reference image to create. If an image ID is specified, it is\n    ignored.\n    If a dict is ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.854235",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "reference",
        "image",
        "bigquery",
        "cloudvisioncreatereferenceimageoperator",
        "cloud",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisiondeletereferenceimageoperator_20260128_160330",
      "component_name": "CloudVisionDeleteReferenceImageOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionDeleteReferenceImageOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "product_id",
          "reference_image_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "product_id",
          "type": "str",
          "required": false
        },
        {
          "name": "reference_image_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete a ReferenceImage ID resource.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionDeleteReferenceImageOperator`\n\n:param location: (Required) The region where the Product is located. Valid regions (as of 2019-02-05) are:\n    us-east1, us-west1, europe-west1, asia-east1\n:param reference_image_id: (Optional) A user-supplied resource id for the ReferenceImage to be added.\n    If set, the server will attempt to use t",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.854725",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "reference",
        "image",
        "bigquery",
        "delete",
        "cloud",
        "vision",
        "cloudvisiondeletereferenceimageoperator"
      ]
    },
    {
      "id": "official_cloudvisionaddproducttoproductsetoperator_20260128_160330",
      "component_name": "CloudVisionAddProductToProductSetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionAddProductToProductSetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "product_set_id",
          "product_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "product_set_id",
          "type": "str",
          "required": false
        },
        {
          "name": "product_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Add a Product to the specified ProductSet.\n\nIf the Product is already present, no change is made. One Product can be\nadded to at most 100 ProductSets.\n\nPossible errors:\n\n- Returns `NOT_FOUND` if the Product or the ProductSet doesn't exist.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionAddProductToProductSetOperator`\n\n:param product_set_id: (Required) The resource id for the ProductSet to modify.\n:param product_id",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.855206",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "set",
        "gcp",
        "add",
        "bigquery",
        "to",
        "cloud",
        "product",
        "vision",
        "cloudvisionaddproducttoproductsetoperator"
      ]
    },
    {
      "id": "official_cloudvisionremoveproductfromproductsetoperator_20260128_160330",
      "component_name": "CloudVisionRemoveProductFromProductSetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionRemoveProductFromProductSetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "product_set_id",
          "product_id",
          "project_id",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "product_set_id",
          "type": "str",
          "required": false
        },
        {
          "name": "product_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "MetaData",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Remove a Product from the specified ProductSet.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionRemoveProductFromProductSetOperator`\n\n:param product_set_id: (Required) The resource id for the ProductSet to modify.\n:param product_id: (Required) The resource id of this Product.\n:param location: (Required) The region where the ProductSet is located. Valid regions (as of 2019-02-05)\n    are: us-east1, us-west1, europe-",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.855681",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "set",
        "remove",
        "gcp",
        "bigquery",
        "cloudvisionremoveproductfromproductsetoperator",
        "from",
        "cloud",
        "product",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisiondetecttextoperator_20260128_160330",
      "component_name": "CloudVisionDetectTextOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionDetectTextOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "image",
          "max_results",
          "timeout",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "image",
          "type": "Union",
          "required": true
        },
        {
          "name": "max_results",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "language_hints",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "web_detection_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "additional_properties",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Detect Text in the image.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionDetectTextOperator`\n\n:param image: (Required) The image to analyze. See more:\n    https://googleapis.github.io/google-cloud-python/latest/vision/gapic/v1/types.html#google.cloud.vision_v1.types.Image\n:param max_results: (Optional) Number of results to return.\n:param retry: (Optional) A retry object used to retry requests. If `None` is\n    spe",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.856176",
      "relevance_keywords": [
        "cloudvisiondetecttextoperator",
        "text",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "detect",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisiontextdetectoperator_20260128_160330",
      "component_name": "CloudVisionTextDetectOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionTextDetectOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "image",
          "max_results",
          "timeout",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "image",
          "type": "Union",
          "required": true
        },
        {
          "name": "max_results",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "language_hints",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "web_detection_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "additional_properties",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Detect Document Text in the image.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionTextDetectOperator`\n\n:param image: (Required) The image to analyze. See more:\n    https://googleapis.github.io/google-cloud-python/latest/vision/gapic/v1/types.html#google.cloud.vision_v1.types.Image\n:param max_results: Number of results to return.\n:param retry: (Optional) A retry object used to retry requests. If `None` is\n    speci",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.856702",
      "relevance_keywords": [
        "text",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "cloudvisiontextdetectoperator",
        "cloud",
        "detect",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisiondetectimagelabelsoperator_20260128_160330",
      "component_name": "CloudVisionDetectImageLabelsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionDetectImageLabelsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "image",
          "max_results",
          "timeout",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "image",
          "type": "Union",
          "required": true
        },
        {
          "name": "max_results",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "additional_properties",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Detect Document Text in the image.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionDetectImageLabelsOperator`\n\n:param image: (Required) The image to analyze. See more:\n    https://googleapis.github.io/google-cloud-python/latest/vision/gapic/v1/types.html#google.cloud.vision_v1.types.Image\n:param max_results: Number of results to return.\n:param retry: (Optional) A retry object used to retry requests. If `None` is\n  ",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.857603",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "image",
        "bigquery",
        "cloudvisiondetectimagelabelsoperator",
        "labels",
        "cloud",
        "detect",
        "vision"
      ]
    },
    {
      "id": "official_cloudvisiondetectimagesafesearchoperator_20260128_160330",
      "component_name": "CloudVisionDetectImageSafeSearchOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionDetectImageSafeSearchOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "image",
          "max_results",
          "timeout",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "image",
          "type": "Union",
          "required": true
        },
        {
          "name": "max_results",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "additional_properties",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Detect Document Text in the image.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudVisionDetectImageSafeSearchOperator`\n\n:param image: (Required) The image to analyze. See more:\n    https://googleapis.github.io/google-cloud-python/latest/vision/gapic/v1/types.html#google.cloud.vision_v1.types.Image\n:param max_results: Number of results to return.\n:param retry: (Optional) A retry object used to retry requests. If `None` i",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.858320",
      "relevance_keywords": [
        "safe",
        "gcs",
        "operator",
        "google",
        "gcp",
        "cloudvisiondetectimagesafesearchoperator",
        "image",
        "bigquery",
        "search",
        "cloud",
        "detect",
        "vision"
      ]
    },
    {
      "id": "official_workflowscreateworkflowoperator_20260128_160330",
      "component_name": "WorkflowsCreateWorkflowOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowsCreateWorkflowOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "workflow",
          "workflow_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "workflow",
          "type": "dict",
          "required": false
        },
        {
          "name": "workflow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "force_rerun",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport re\nimport uuid\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING\nimport pendulum\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_workflow_id",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new workflow.\n\nIf a workflow with the specified name already exists in the specified\nproject and location, the long-running operation will return\n[ALREADY_EXISTS][google.rpc.Code.ALREADY_EXISTS] error.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:WorkflowsCreateWorkflowOperator`\n\n:param workflow: Required. Workflow to be created.\n:param workflow_id: Required. The ID of the workflow to be created.\n:param project_i",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.903115",
      "relevance_keywords": [
        "gcs",
        "workflowscreateworkflowoperator",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "cloud",
        "workflow",
        "workflows"
      ]
    },
    {
      "id": "official_workflowsupdateworkflowoperator_20260128_160330",
      "component_name": "WorkflowsUpdateWorkflowOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowsUpdateWorkflowOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "workflow_id",
          "update_mask"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "workflow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport re\nimport uuid\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING\nimport pendulum\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates an existing workflow.\n\nRunning this method has no impact on already running\nexecutions of the workflow. A new revision of the\nworkflow may be created as a result of a successful\nupdate operation. In that case, such revision will be\nused in new workflow executions.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:WorkflowsUpdateWorkflowOperator`\n\n:param workflow_id: Required. The ID of the workflow to be updated.\n:param",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.903448",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "bigquery",
        "cloud",
        "workflowsupdateworkflowoperator",
        "workflow",
        "workflows"
      ]
    },
    {
      "id": "official_workflowsdeleteworkflowoperator_20260128_160330",
      "component_name": "WorkflowsDeleteWorkflowOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowsDeleteWorkflowOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "workflow_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "workflow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport re\nimport uuid\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING\nimport pendulum\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete a workflow with the specified name and all running executions of the workflow.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:WorkflowsDeleteWorkflowOperator`\n\n:param workflow_id: Required. The ID of the workflow to be created.\n:param project_id: Required. The ID of the Google Cloud project the cluster belongs to.\n:param location: Required. The GCP region in which to handle the request.\n:param retry: A retry object us",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.903722",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "workflowsdeleteworkflowoperator",
        "bigquery",
        "delete",
        "cloud",
        "workflow",
        "workflows"
      ]
    },
    {
      "id": "official_workflowslistworkflowsoperator_20260128_160330",
      "component_name": "WorkflowsListWorkflowsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowsListWorkflowsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "order_by",
          "filter_"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "filter_",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "order_by",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport re\nimport uuid\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING\nimport pendulum\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists Workflows in a given project and location; the default order is not specified.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:WorkflowsListWorkflowsOperator`\n\n:param filter_: Filter to restrict results to specific workflows.\n:param order_by: Comma-separated list of fields that\n    specifies the order of the results. Default sorting order for a field is ascending.\n    To specify descending order for a field, append a \"d",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.904044",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "list",
        "cloud",
        "workflows",
        "workflowslistworkflowsoperator"
      ]
    },
    {
      "id": "official_workflowsgetworkflowoperator_20260128_160330",
      "component_name": "WorkflowsGetWorkflowOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowsGetWorkflowOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "workflow_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "workflow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport re\nimport uuid\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING\nimport pendulum\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets details of a single Workflow.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:WorkflowsGetWorkflowOperator`\n\n:param workflow_id: Required. The ID of the workflow to be created.\n:param project_id: Required. The ID of the Google Cloud project the cluster belongs to.\n:param location: Required. The GCP region in which to handle the request.\n:param retry: A retry object used to retry requests. If ``None`` is specified, reques",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.904410",
      "relevance_keywords": [
        "workflowsgetworkflowoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "get",
        "cloud",
        "workflow",
        "workflows"
      ]
    },
    {
      "id": "official_workflowscreateexecutionoperator_20260128_160330",
      "component_name": "WorkflowsCreateExecutionOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowsCreateExecutionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "workflow_id",
          "execution"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "workflow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "execution",
          "type": "dict",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport re\nimport uuid\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING\nimport pendulum\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new execution using the latest revision of the given workflow.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:WorkflowsCreateExecutionOperator`\n\n:param execution: Required. Execution to be created.\n:param workflow_id: Required. The ID of the workflow.\n:param project_id: Required. The ID of the Google Cloud project the cluster belongs to.\n:param location: Required. The GCP region in which to handle the request.\n:par",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.905060",
      "relevance_keywords": [
        "execution",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "workflowscreateexecutionoperator",
        "cloud",
        "workflows"
      ]
    },
    {
      "id": "official_workflowscancelexecutionoperator_20260128_160330",
      "component_name": "WorkflowsCancelExecutionOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowsCancelExecutionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "workflow_id",
          "execution_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "workflow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "execution_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport re\nimport uuid\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING\nimport pendulum\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Cancels an execution using the given ``workflow_id`` and ``execution_id``.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:WorkflowsCancelExecutionOperator`\n\n:param workflow_id: Required. The ID of the workflow.\n:param execution_id: Required. The ID of the execution.\n:param project_id: Required. The ID of the Google Cloud project the cluster belongs to.\n:param location: Required. The GCP region in which to handle the request.",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.905380",
      "relevance_keywords": [
        "execution",
        "gcs",
        "operator",
        "google",
        "gcp",
        "cancel",
        "bigquery",
        "cloud",
        "workflowscancelexecutionoperator",
        "workflows"
      ]
    },
    {
      "id": "official_workflowslistexecutionsoperator_20260128_160330",
      "component_name": "WorkflowsListExecutionsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowsListExecutionsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "workflow_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "workflow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "start_date_filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport re\nimport uuid\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING\nimport pendulum\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Returns a list of executions which belong to the workflow with the given name.\n\nThe method returns executions of all workflow revisions. Returned\nexecutions are ordered by their start time (newest first).\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:WorkflowsListExecutionsOperator`\n\n:param workflow_id: Required. The ID of the workflow to be created.\n:param start_date_filter: If passed only executions older that this date w",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.905649",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "workflowslistexecutionsoperator",
        "bigquery",
        "list",
        "executions",
        "cloud",
        "workflows"
      ]
    },
    {
      "id": "official_workflowsgetexecutionoperator_20260128_160330",
      "component_name": "WorkflowsGetExecutionOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowsGetExecutionOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "location",
          "workflow_id",
          "execution_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "workflow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "execution_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport datetime\nimport json\nimport re\nimport uuid\nfrom collections.abc import Collection, Sequence\nfrom typing import TYPE_CHECKING\nimport pendulum\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Returns an execution for the given ``workflow_id`` and ``execution_id``.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:WorkflowsGetExecutionOperator`\n\n:param workflow_id: Required. The ID of the workflow.\n:param execution_id: Required. The ID of the execution.\n:param project_id: Required. The ID of the Google Cloud project the cluster belongs to.\n:param location: Required. The GCP region in which to handle the request.\n:par",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.905897",
      "relevance_keywords": [
        "execution",
        "gcs",
        "workflowsgetexecutionoperator",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "get",
        "cloud",
        "workflows"
      ]
    },
    {
      "id": "official_cloudfirestoreexportdatabaseoperator_20260128_160330",
      "component_name": "CloudFirestoreExportDatabaseOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFirestoreExportDatabaseOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "body",
          "gcp_conn_id",
          "api_version",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body",
          "type": "dict",
          "required": false
        },
        {
          "name": "database_id",
          "type": "str",
          "required": false,
          "default": "(default)"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID\nfrom airflow.providers.google.firebase.hooks.firestore import CloudFirestoreHook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_validate_inputs",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Export documents from Google Cloud Firestore to another storage system, such as Google Cloud Storage.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudFirestoreExportDatabaseOperator`\n\n:param database_id: The Database ID.\n:param body: The request body.\n    See:\n    https://firebase.google.com/docs/firestore/reference/rest/v1beta1/projects.databases/exportDocuments\n:param project_id: ID of the Google Cloud project if None ",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.936926",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "cloudfirestoreexportdatabaseoperator",
        "export",
        "gcp",
        "bigquery",
        "firestore",
        "cloud",
        "database"
      ]
    },
    {
      "id": "official_leveldboperator_20260128_160330",
      "component_name": "LevelDBOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "LevelDBOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "command",
          "type": "str",
          "required": false
        },
        {
          "name": "key",
          "type": "bytes",
          "required": false
        },
        {
          "name": "value",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "keys",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "values",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "leveldb_conn_id",
          "type": "str",
          "required": false,
          "default": "leveldb_default"
        },
        {
          "name": "name",
          "type": "str",
          "required": false,
          "default": "/tmp/testdb/"
        },
        {
          "name": "create_if_missing",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "create_db_extra_options",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.google.leveldb.hooks.leveldb import LevelDBHook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Execute command in LevelDB.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:LevelDBOperator`\n\n    :param command: command of plyvel(python wrap for leveldb) for DB object e.g.\n        ``\"put\"``, ``\"get\"``, ``\"delete\"``, ``\"write_batch\"``.\n    :param key: key for command(put,get,delete) execution(, e.g. ``b'key'``, ``b'another-key'``)\n    :param value: value for command(put) execution(bytes, e.g. ``b'value'``, ``b'another-valu",
      "success_score": 170,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.961143",
      "relevance_keywords": [
        "level",
        "gcs",
        "leveldboperator",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_googleanalyticsadminlistaccountsoperator_20260128_160330",
      "component_name": "GoogleAnalyticsAdminListAccountsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleAnalyticsAdminListAccountsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "gcp_conn_id",
          "impersonation_chain",
          "page_size",
          "page_token"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "show_deleted",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.analytics.admin_v1beta import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists all accounts to which the user has access.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleAnalyticsAdminListAccountsOperator`\n\n:param page_size: Optional, number of results to return in the list.\n:param page_token: Optional. The next_page_token value returned from a previous List request, if any.\n:param show_deleted: Optional. Whether to include soft-deleted (ie: \"trashed\") Accounts in the results.\n:param retry: ",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.984352",
      "relevance_keywords": [
        "gcs",
        "analytics",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "googleanalyticsadminlistaccountsoperator",
        "admin",
        "list",
        "cloud",
        "accounts"
      ]
    },
    {
      "id": "official_googleanalyticsadmincreatepropertyoperator_20260128_160330",
      "component_name": "GoogleAnalyticsAdminCreatePropertyOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleAnalyticsAdminCreatePropertyOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "gcp_conn_id",
          "impersonation_chain",
          "analytics_property"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "analytics_property",
          "type": "Union",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.analytics.admin_v1beta import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates property.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleAnalyticsAdminCreatePropertyOperator`\n\n:param analytics_property: The property to create. Note: the supplied property must specify its parent.\n    For more details see: https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties#Property\n:param retry: Optional, a retry object used  to retry requests. If `None` is specified, re",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.984706",
      "relevance_keywords": [
        "gcs",
        "analytics",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "property",
        "admin",
        "cloud",
        "googleanalyticsadmincreatepropertyoperator"
      ]
    },
    {
      "id": "official_googleanalyticsadmindeletepropertyoperator_20260128_160330",
      "component_name": "GoogleAnalyticsAdminDeletePropertyOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleAnalyticsAdminDeletePropertyOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "gcp_conn_id",
          "impersonation_chain",
          "property_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "property_id",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.analytics.admin_v1beta import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Soft-delete property.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleAnalyticsAdminDeletePropertyOperator`\n\n:param property_id: The id of the Property to soft-delete.\n:param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests\n    will not be retried.\n:param timeout: Optional. The timeout for this request.\n:param metadata: Optional. Strings which should be sent along with the reques",
      "success_score": 179,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.984992",
      "relevance_keywords": [
        "gcs",
        "analytics",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "property",
        "delete",
        "googleanalyticsadmindeletepropertyoperator",
        "admin",
        "cloud"
      ]
    },
    {
      "id": "official_googleanalyticsadmincreatedatastreamoperator_20260128_160330",
      "component_name": "GoogleAnalyticsAdminCreateDataStreamOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleAnalyticsAdminCreateDataStreamOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "gcp_conn_id",
          "impersonation_chain",
          "property_id",
          "data_stream"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "property_id",
          "type": "str",
          "required": false
        },
        {
          "name": "data_stream",
          "type": "Union",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.analytics.admin_v1beta import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates Data stream.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleAnalyticsAdminCreateDataStreamOperator`\n\n:param property_id: ID of the parent property for the data stream.\n:param data_stream: The data stream to create.\n    For more details see: https://developers.google.com/analytics/devguides/config/admin/v1/rest/v1beta/properties.dataStreams#DataStream\n:param retry: Optional, a retry object used  to retry request",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.985266",
      "relevance_keywords": [
        "stream",
        "gcs",
        "analytics",
        "operator",
        "google",
        "gcp",
        "create",
        "googleanalyticsadmincreatedatastreamoperator",
        "bigquery",
        "data",
        "admin",
        "cloud"
      ]
    },
    {
      "id": "official_googleanalyticsadmindeletedatastreamoperator_20260128_160330",
      "component_name": "GoogleAnalyticsAdminDeleteDataStreamOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleAnalyticsAdminDeleteDataStreamOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "gcp_conn_id",
          "impersonation_chain",
          "property_id",
          "data_stream_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "property_id",
          "type": "str",
          "required": false
        },
        {
          "name": "data_stream_id",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.analytics.admin_v1beta import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes Data stream.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleAnalyticsAdminDeleteDataStreamOperator`\n\n:param property_id: ID of the property which is parent for the data stream.\n:param data_stream_id: ID of the data stream to delete.\n:param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests\n    will not be retried.\n:param timeout: Optional. The timeout for this request.\n:pa",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.985568",
      "relevance_keywords": [
        "stream",
        "gcs",
        "analytics",
        "operator",
        "google",
        "gcp",
        "googleanalyticsadmindeletedatastreamoperator",
        "bigquery",
        "delete",
        "data",
        "admin",
        "cloud"
      ]
    },
    {
      "id": "official_googleanalyticsadminlistgoogleadslinksoperator_20260128_160330",
      "component_name": "GoogleAnalyticsAdminListGoogleAdsLinksOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleAnalyticsAdminListGoogleAdsLinksOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "gcp_conn_id",
          "impersonation_chain",
          "property_id",
          "page_size",
          "page_token"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "property_id",
          "type": "str",
          "required": false
        },
        {
          "name": "page_size",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.analytics.admin_v1beta import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Lists all Google Ads links associated with a given property.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleAnalyticsAdminListGoogleAdsLinksOperator`\n\n:param property_id: ID of the parent property.\n:param page_size: Optional, number of results to return in the list.\n:param page_token: Optional. The next_page_token value returned from a previous List request, if any.\n:param retry: Optional, a retry object used  to retry",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.985860",
      "relevance_keywords": [
        "gcs",
        "analytics",
        "operator",
        "google",
        "ads",
        "gcp",
        "googleanalyticsadminlistgoogleadslinksoperator",
        "links",
        "bigquery",
        "admin",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_googleanalyticsadmingetgoogleadslinkoperator_20260128_160330",
      "component_name": "GoogleAnalyticsAdminGetGoogleAdsLinkOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleAnalyticsAdminGetGoogleAdsLinkOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "gcp_conn_id",
          "impersonation_chain",
          "google_ads_link_id",
          "property_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "property_id",
          "type": "str",
          "required": false
        },
        {
          "name": "google_ads_link_id",
          "type": "str",
          "required": false
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.analytics.admin_v1beta import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Gets a Google Ads link associated with a given property.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleAnalyticsAdminGetGoogleAdsLinkOperator`\n\n:param property_id: Parent property id.\n:param google_ads_link_id: Google Ads link id.\n:param retry: Optional, a retry object used  to retry requests. If `None` is specified, requests\n    will not be retried.\n:param timeout: Optional. The timeout for this request.\n:param metad",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:24.986133",
      "relevance_keywords": [
        "googleanalyticsadmingetgoogleadslinkoperator",
        "gcs",
        "analytics",
        "operator",
        "google",
        "ads",
        "link",
        "gcp",
        "bigquery",
        "admin",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_googlecampaignmanagerdeletereportoperator_20260128_160330",
      "component_name": "GoogleCampaignManagerDeleteReportOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCampaignManagerDeleteReportOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "profile_id",
          "report_id",
          "report_name",
          "api_version",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "profile_id",
          "type": "str",
          "required": false
        },
        {
          "name": "report_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "report_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport tempfile\nimport uuid\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient import http\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes a report by its ID.\n\n.. seealso::\n    Check official API docs:\n    `https://developers.google.com/doubleclick-advertisers/rest/v4/reports/delete`\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleCampaignManagerDeleteReportOperator`\n\n:param profile_id: The DFA user profile ID.\n:param report_name: The name of the report to delete.\n:param report_id: The ID of the report.\n:param api_version: The version of the api th",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.009747",
      "relevance_keywords": [
        "manager",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "googlecampaignmanagerdeletereportoperator",
        "cloud",
        "report",
        "campaign"
      ]
    },
    {
      "id": "official_googlecampaignmanagerdownloadreportoperator_20260128_160330",
      "component_name": "GoogleCampaignManagerDownloadReportOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCampaignManagerDownloadReportOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "profile_id",
          "report_id",
          "file_id",
          "bucket_name",
          "report_name",
          "chunk_size",
          "api_version",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "profile_id",
          "type": "str",
          "required": false
        },
        {
          "name": "report_id",
          "type": "str",
          "required": false
        },
        {
          "name": "file_id",
          "type": "str",
          "required": false
        },
        {
          "name": "bucket_name",
          "type": "str",
          "required": false
        },
        {
          "name": "report_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gzip",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "chunk_size",
          "type": "int",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport tempfile\nimport uuid\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient import http\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_resolve_file_name",
          "_set_bucket_name",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Retrieves a report and uploads it to GCS bucket.\n\n.. seealso::\n    Check official API docs:\n    `https://developers.google.com/doubleclick-advertisers/rest/v4/reports/get`\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleCampaignManagerDownloadReportOperator`\n\n:param profile_id: The DFA user profile ID.\n:param report_id: The ID of the report.\n:param file_id: The ID of the report file.\n:param bucket_name: The bucket to up",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.010031",
      "relevance_keywords": [
        "manager",
        "gcs",
        "download",
        "googlecampaignmanagerdownloadreportoperator",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "report",
        "campaign"
      ]
    },
    {
      "id": "official_googlecampaignmanagerinsertreportoperator_20260128_160330",
      "component_name": "GoogleCampaignManagerInsertReportOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCampaignManagerInsertReportOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "profile_id",
          "report",
          "api_version",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "profile_id",
          "type": "str",
          "required": false
        },
        {
          "name": "report",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport tempfile\nimport uuid\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient import http\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "template_ext"
        ],
        "methods": [
          "__init__",
          "prepare_template",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a report.\n\n.. seealso::\n    Check official API docs:\n    `https://developers.google.com/doubleclick-advertisers/rest/v4/reports/insert`\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleCampaignManagerInsertReportOperator`\n\n:param profile_id: The DFA user profile ID.\n:param report: Report to be created.\n:param api_version: The version of the api that will be requested, for example 'v4'.\n:param gcp_conn_id: The con",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.010291",
      "relevance_keywords": [
        "manager",
        "gcs",
        "operator",
        "google",
        "insert",
        "gcp",
        "bigquery",
        "cloud",
        "report",
        "googlecampaignmanagerinsertreportoperator",
        "campaign"
      ]
    },
    {
      "id": "official_googlecampaignmanagerrunreportoperator_20260128_160330",
      "component_name": "GoogleCampaignManagerRunReportOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCampaignManagerRunReportOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "profile_id",
          "report_id",
          "synchronous",
          "api_version",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "profile_id",
          "type": "str",
          "required": false
        },
        {
          "name": "report_id",
          "type": "str",
          "required": false
        },
        {
          "name": "synchronous",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport tempfile\nimport uuid\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient import http\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Runs a report.\n\n.. seealso::\n    Check official API docs:\n    `https://developers.google.com/doubleclick-advertisers/rest/v4/reports/run`\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleCampaignManagerRunReportOperator`\n\n:param profile_id: The DFA profile ID.\n:param report_id: The ID of the report.\n:param synchronous: If set and true, tries to run the report synchronously.\n:param api_version: The version of the api that",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.010519",
      "relevance_keywords": [
        "manager",
        "gcs",
        "operator",
        "google",
        "gcp",
        "googlecampaignmanagerrunreportoperator",
        "bigquery",
        "run",
        "cloud",
        "report",
        "campaign"
      ]
    },
    {
      "id": "official_googlecampaignmanagerbatchinsertconversionsoperator_20260128_160330",
      "component_name": "GoogleCampaignManagerBatchInsertConversionsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCampaignManagerBatchInsertConversionsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "profile_id",
          "conversions",
          "encryption_entity_type",
          "encryption_entity_id",
          "encryption_source",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "profile_id",
          "type": "str",
          "required": false
        },
        {
          "name": "conversions",
          "type": "list[...]",
          "required": false
        },
        {
          "name": "encryption_entity_type",
          "type": "str",
          "required": false
        },
        {
          "name": "encryption_entity_id",
          "type": "int",
          "required": false
        },
        {
          "name": "encryption_source",
          "type": "str",
          "required": false
        },
        {
          "name": "max_failed_inserts",
          "type": "int",
          "required": false,
          "default": 0
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport tempfile\nimport uuid\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient import http\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Inserts conversions.\n\n.. seealso::\n    Check official API docs:\n    `https://developers.google.com/doubleclick-advertisers/rest/v4/conversions/batchinsert`\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleCampaignManagerBatchInsertConversionsOperator`\n\n:param profile_id: User profile ID associated with this request.\n:param conversions: Conversions to insert, should be type of Conversion:\n    https://developers.google.com",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.010742",
      "relevance_keywords": [
        "manager",
        "batch",
        "gcs",
        "operator",
        "google",
        "insert",
        "gcp",
        "googlecampaignmanagerbatchinsertconversionsoperator",
        "bigquery",
        "cloud",
        "conversions",
        "campaign"
      ]
    },
    {
      "id": "official_googlecampaignmanagerbatchupdateconversionsoperator_20260128_160330",
      "component_name": "GoogleCampaignManagerBatchUpdateConversionsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCampaignManagerBatchUpdateConversionsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "profile_id",
          "conversions",
          "encryption_entity_type",
          "encryption_entity_id",
          "encryption_source",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "profile_id",
          "type": "str",
          "required": false
        },
        {
          "name": "conversions",
          "type": "list[...]",
          "required": false
        },
        {
          "name": "encryption_entity_type",
          "type": "str",
          "required": false
        },
        {
          "name": "encryption_entity_id",
          "type": "int",
          "required": false
        },
        {
          "name": "encryption_source",
          "type": "str",
          "required": false
        },
        {
          "name": "max_failed_updates",
          "type": "int",
          "required": false,
          "default": 0
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport tempfile\nimport uuid\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient import http\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates existing conversions.\n\n.. seealso::\n    Check official API docs:\n    `https://developers.google.com/doubleclick-advertisers/rest/v4/conversions/batchupdate`\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleCampaignManagerBatchUpdateConversionsOperator`\n\n:param profile_id: User profile ID associated with this request.\n:param conversions: Conversations to update, should be type of Conversion:\n    https://developers",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.010957",
      "relevance_keywords": [
        "manager",
        "batch",
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "bigquery",
        "googlecampaignmanagerbatchupdateconversionsoperator",
        "cloud",
        "conversions",
        "campaign"
      ]
    },
    {
      "id": "official_googledisplayvideo360createsdfdownloadtaskoperator_20260128_160330",
      "component_name": "GoogleDisplayVideo360CreateSDFDownloadTaskOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleDisplayVideo360CreateSDFDownloadTaskOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "body_request",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "body_request",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport tempfile\nimport zipfile\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.marketing_platform.hooks.display_video import GoogleDisplayVideo360Hook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates an SDF operation task.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleDisplayVideo360CreateSDFDownloadTaskOperator`\n\n.. seealso::\n    Check also the official API docs:\n    `https://developers.google.com/display-video/api/reference/rest`\n\n:param version: The SDF version of the downloaded file.\n:param partner_id: The ID of the partner to download SDF for.\n:param advertiser_id: The ID of the advertiser to download",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.046253",
      "relevance_keywords": [
        "googledisplayvideo360createsdfdownloadtaskoperator",
        "display",
        "download",
        "gcs",
        "operator",
        "google",
        "task",
        "create",
        "video",
        "gcp",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_googledisplayvideo360sdftogcsoperator_20260128_160330",
      "component_name": "GoogleDisplayVideo360SDFtoGCSOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleDisplayVideo360SDFtoGCSOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "operation_name",
          "bucket_name",
          "object_name",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "operation_name",
          "type": "str",
          "required": false
        },
        {
          "name": "bucket_name",
          "type": "str",
          "required": false
        },
        {
          "name": "object_name",
          "type": "str",
          "required": false
        },
        {
          "name": "gzip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport tempfile\nimport zipfile\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.google.cloud.hooks.gcs import GCSHook\nfrom airflow.providers.google.marketing_platform.hooks.display_video import GoogleDisplayVideo360Hook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Download SDF media and save it in the Google Cloud Storage.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleDisplayVideo360SDFtoGCSOperator`\n\n.. seealso::\n    Check also the official API docs:\n    `https://developers.google.com/display-video/api/reference/rest`\n\n:param version: The SDF version of the downloaded file.\n:param partner_id: The ID of the partner to download SDF for.\n:param advertiser_id: The ID of the advert",
      "success_score": 182,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.046386",
      "relevance_keywords": [
        "display",
        "fto",
        "gcs",
        "operator",
        "google",
        "gcp",
        "video",
        "bigquery",
        "googledisplayvideo360sdftogcsoperator",
        "cloud"
      ]
    },
    {
      "id": "official_googlesearchadssearchoperator_20260128_160330",
      "component_name": "GoogleSearchAdsSearchOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleSearchAdsSearchOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_GoogleSearchAdsBaseOperator"
        ],
        "template_fields": [
          "page_token",
          "page_size"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "customer_id",
          "type": "str",
          "required": false
        },
        {
          "name": "query",
          "type": "str",
          "required": false
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_size",
          "type": "int",
          "required": false,
          "default": 10000
        },
        {
          "name": "return_total_results_count",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "summary_row_setting",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "validate_only",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from _GoogleSearchAdsBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.google.marketing_platform.hooks.search_ads import GoogleSearchAdsReportingHook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Search a report by query.\n\n.. seealso:\n    For API documentation check:\n    https://developers.google.com/search-ads/reporting/api/reference/rest/v0/customers.searchAds360/search\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleSearchAdsSearchOperator`\n\n:param customer_id: The ID of the customer being queried.\n:param query: The query to execute.\n:param page_token: Token of the page to retrieve. If not specified, the firs",
      "success_score": 181,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.078663",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "ads",
        "gcp",
        "bigquery",
        "googlesearchadssearchoperator",
        "search",
        "cloud"
      ]
    },
    {
      "id": "official_googlesearchadsgetfieldoperator_20260128_160330",
      "component_name": "GoogleSearchAdsGetFieldOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleSearchAdsGetFieldOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_GoogleSearchAdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "field_name",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from _GoogleSearchAdsBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.google.marketing_platform.hooks.search_ads import GoogleSearchAdsReportingHook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Retrieve metadata for a resource or a field.\n\n.. seealso:\n    For API documentation check:\n    https://developers.google.com/search-ads/reporting/api/reference/rest/v0/searchAds360Fields/get\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleSearchAdsGetFieldOperator`\n\n:param field_name: The name of the field.\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param api_version: The version of the",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.078780",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "ads",
        "gcp",
        "bigquery",
        "search",
        "get",
        "cloud",
        "googlesearchadsgetfieldoperator",
        "field"
      ]
    },
    {
      "id": "official_googlesearchadssearchfieldsoperator_20260128_160330",
      "component_name": "GoogleSearchAdsSearchFieldsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleSearchAdsSearchFieldsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_GoogleSearchAdsBaseOperator"
        ],
        "template_fields": [
          "page_token",
          "page_size"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "query",
          "type": "str",
          "required": false
        },
        {
          "name": "page_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "page_size",
          "type": "int",
          "required": false,
          "default": 10000
        }
      ],
      "success_factors": [
        "Inherits from _GoogleSearchAdsBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.google.marketing_platform.hooks.search_ads import GoogleSearchAdsReportingHook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Retrieve metadata for resource(s) or field(s) by the query syntax.\n\n.. seealso:\n    For API documentation check:\n    https://developers.google.com/search-ads/reporting/api/reference/rest/v0/searchAds360Fields/search\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleSearchAdsSearchFieldsOperator`\n\n:param query: The query string to execute.\n:param page_token: Token of the page to retrieve. If not specified, the first page o",
      "success_score": 181,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.078872",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "ads",
        "gcp",
        "bigquery",
        "fields",
        "search",
        "cloud",
        "googlesearchadssearchfieldsoperator"
      ]
    },
    {
      "id": "official_googlesearchadsgetcustomcolumnoperator_20260128_160330",
      "component_name": "GoogleSearchAdsGetCustomColumnOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleSearchAdsGetCustomColumnOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_GoogleSearchAdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "customer_id",
          "type": "str",
          "required": false
        },
        {
          "name": "custom_column_id",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from _GoogleSearchAdsBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.google.marketing_platform.hooks.search_ads import GoogleSearchAdsReportingHook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Retrieve details of a custom column for the given customer_id and campaign_id.\n\n.. seealso:\n    For API documentation check:\n    https://developers.google.com/search-ads/reporting/api/reference/rest/v0/customers.customColumns/get\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleSearchAdsGetCustomColumnOperator`\n\n:param customer_id: The customer ID for the custom column.\n:param custom_column_id: The ID for the custom colu",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.078954",
      "relevance_keywords": [
        "googlesearchadsgetcustomcolumnoperator",
        "gcs",
        "operator",
        "google",
        "ads",
        "custom",
        "gcp",
        "bigquery",
        "column",
        "search",
        "get",
        "cloud"
      ]
    },
    {
      "id": "official_googlesearchadslistcustomcolumnsoperator_20260128_160330",
      "component_name": "GoogleSearchAdsListCustomColumnsOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleSearchAdsListCustomColumnsOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "_GoogleSearchAdsBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "customer_id",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from _GoogleSearchAdsBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.google.marketing_platform.hooks.search_ads import GoogleSearchAdsReportingHook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "List all custom columns.\n\n.. seealso:\n    For API documentation check:\n    https://developers.google.com/search-ads/reporting/api/reference/rest/v0/customers.customColumns/list\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleSearchAdsListCustomColumnsOperator`\n\n:param customer_id: The customer ID for the custom column.\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param api_version: The ve",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.079037",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "ads",
        "custom",
        "columns",
        "gcp",
        "bigquery",
        "search",
        "googlesearchadslistcustomcolumnsoperator",
        "list",
        "cloud"
      ]
    },
    {
      "id": "official_googlesheetscreatespreadsheetoperator_20260128_160330",
      "component_name": "GoogleSheetsCreateSpreadsheetOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleSheetsCreateSpreadsheetOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "spreadsheet",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "spreadsheet",
          "type": "dict[...]",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_endpoint",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom airflow.providers.google.suite.hooks.sheets import GSheetsHook\nfrom airflow.providers.google.version_compat import BaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a new spreadsheet.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleSheetsCreateSpreadsheetOperator`\n\n:param spreadsheet: an instance of Spreadsheet\n    https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets#Spreadsheet\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, o",
      "success_score": 176,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:25.107546",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "cloud",
        "googlesheetscreatespreadsheetoperator",
        "sheets",
        "spreadsheet"
      ]
    },
    {
      "id": "official_bigquerytableexistencesensor_20260128_160330",
      "component_name": "BigQueryTableExistenceSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryTableExistenceSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "project_id",
          "dataset_id",
          "table_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "table_id",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, conf\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.triggers.bigquery import (",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Checks for the existence of a table in Google Bigquery.\n\n:param project_id: The Google cloud project in which to look for the table.\n    The connection supplied to the hook must provide\n    access to the specified project.\n:param dataset_id: The name of the dataset in which to look for the table.\n    storage bucket.\n:param table_id: The name of the table to check the existence of.\n:param gcp_conn_id: (Optional) The connection ID used to connect to Google Cloud.\n:param impersonation_chain: Option",
      "success_score": 197,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.123963",
      "relevance_keywords": [
        "existence",
        "gcs",
        "google",
        "bigquerytableexistencesensor",
        "gcp",
        "big",
        "bigquery",
        "table",
        "cloud",
        "sensor",
        "query"
      ]
    },
    {
      "id": "official_bigquerytablepartitionexistencesensor_20260128_160330",
      "component_name": "BigQueryTablePartitionExistenceSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryTablePartitionExistenceSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "project_id",
          "dataset_id",
          "table_id",
          "partition_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "dataset_id",
          "type": "str",
          "required": false
        },
        {
          "name": "table_id",
          "type": "str",
          "required": false
        },
        {
          "name": "partition_id",
          "type": "str",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, conf\nfrom airflow.providers.google.cloud.hooks.bigquery import BigQueryHook\nfrom airflow.providers.google.cloud.triggers.bigquery import (",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Checks for the existence of a partition within a table in Google Bigquery.\n\n:param project_id: The Google cloud project in which to look for the table.\n    The connection supplied to the hook must provide\n    access to the specified project.\n:param dataset_id: The name of the dataset in which to look for the table.\n    storage bucket.\n:param table_id: The name of the table to check the existence of.\n:param partition_id: The name of the partition to check the existence of.\n:param gcp_conn_id: (Op",
      "success_score": 200,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.124076",
      "relevance_keywords": [
        "existence",
        "gcs",
        "google",
        "gcp",
        "big",
        "partition",
        "bigquery",
        "table",
        "cloud",
        "bigquerytablepartitionexistencesensor",
        "sensor",
        "query"
      ]
    },
    {
      "id": "official_bigquerydatatransferservicetransferrunsensor_20260128_160330",
      "component_name": "BigQueryDataTransferServiceTransferRunSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryDataTransferServiceTransferRunSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "run_id",
          "transfer_config_id",
          "expected_statuses",
          "project_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "run_id",
          "type": "str",
          "required": false
        },
        {
          "name": "transfer_config_id",
          "type": "str",
          "required": false
        },
        {
          "name": "expected_statuses",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "request_timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.bigquery_datatransfer_v1 import TransferState\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.bigquery_dts import BiqQueryDataTransferServiceHook\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_normalize_state_list",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Waits for Data Transfer Service run to complete.\n\n.. seealso::\n    For more information on how to use this sensor, take a look at the guide:\n    :ref:`howto/operator:BigQueryDataTransferServiceTransferRunSensor`\n\n:param expected_statuses: The expected state of the operation.\n    See:\n    https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n:param run_id: ID of the transfer run.\n:param transfer_config_id: ID of transfer config to be used.\n:param project_id: T",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.136752",
      "relevance_keywords": [
        "gcs",
        "service",
        "google",
        "gcp",
        "big",
        "bigquery",
        "data",
        "run",
        "bigquerydatatransferservicetransferrunsensor",
        "transfer",
        "cloud",
        "sensor",
        "query"
      ]
    },
    {
      "id": "official_bigtabletablereplicationcompletedsensor_20260128_160330",
      "component_name": "BigtableTableReplicationCompletedSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigtableTableReplicationCompletedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator",
          "BigtableValidationMixin"
        ],
        "template_fields": [
          "project_id",
          "instance_id",
          "table_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "instance_id",
          "type": "str",
          "required": false
        },
        {
          "name": "table_id",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator, BigtableValidationMixin",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nimport google.api_core.exceptions\nfrom google.cloud.bigtable import enums\nfrom google.cloud.bigtable.table import ClusterState\nfrom airflow.providers.common.compat.sdk import BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.bigtable import BigtableHook",
        "class_attributes": [
          "REQUIRED_ATTRIBUTES",
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Sensor that waits for Cloud Bigtable table to be fully replicated to its clusters.\n\nNo exception will be raised if the instance or the table does not exist.\n\nFor more details about cluster states for a table, have a look at the reference:\nhttps://googleapis.github.io/google-cloud-python/latest/bigtable/table.html#google.cloud.bigtable.table.Table.get_cluster_states\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:BigtableTable",
      "success_score": 182,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.148936",
      "relevance_keywords": [
        "gcs",
        "completed",
        "google",
        "gcp",
        "replication",
        "bigquery",
        "bigtable",
        "bigtabletablereplicationcompletedsensor",
        "table",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_cloudcomposerdagrunsensor_20260128_160330",
      "component_name": "CloudComposerDAGRunSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerDAGRunSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "environment_id",
          "composer_dag_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "environment_id",
          "type": "str",
          "required": false
        },
        {
          "name": "composer_dag_id",
          "type": "str",
          "required": false
        },
        {
          "name": "allowed_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "execution_range",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "composer_dag_run_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "use_rest_api",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Collection, Iterable, Sequence\nfrom datetime import datetime, timedelta\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom dateutil import parser\nfrom google.api_core.exceptions import NotFound\nfrom google.cloud.orchestration.airflow.service_v1.types import Environment, ExecuteAirflowCommandResponse",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_get_logical_dates",
          "poke",
          "_pull_dag_runs",
          "_check_dag_runs_states",
          "_get_composer_airflow_version",
          "_check_composer_dag_run_id_states",
          "execute",
          "execute_complete",
          "hook"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Check if a DAG run has completed.\n\n:param project_id: Required. The ID of the Google Cloud project that the service belongs to.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param environment_id: The name of the Composer environment.\n:param composer_dag_id: The ID of executable DAG.\n:param allowed_states: Iterable of allowed states, default is ``['success']``.\n:param execution_range: execution DAGs time range. Sensor checks DAGs states only for DAGs whi",
      "success_score": 205,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.168527",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "run",
        "cloud",
        "cloudcomposerdagrunsensor",
        "sensor",
        "composer"
      ]
    },
    {
      "id": "official_cloudcomposerexternaltasksensor_20260128_160330",
      "component_name": "CloudComposerExternalTaskSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerExternalTaskSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "environment_id",
          "composer_external_dag_id",
          "composer_external_task_id",
          "composer_external_task_ids",
          "composer_external_task_group_id",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "environment_id",
          "type": "str",
          "required": false
        },
        {
          "name": "composer_external_dag_id",
          "type": "str",
          "required": false
        },
        {
          "name": "composer_external_task_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "composer_external_task_ids",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "composer_external_task_group_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "allowed_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "skipped_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "failed_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "execution_range",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Collection, Iterable, Sequence\nfrom datetime import datetime, timedelta\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom dateutil import parser\nfrom google.api_core.exceptions import NotFound\nfrom google.cloud.orchestration.airflow.service_v1.types import Environment, ExecuteAirflowCommandResponse",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_get_logical_dates",
          "poke",
          "_get_task_instances",
          "_check_task_instances_states",
          "_get_composer_airflow_version",
          "_handle_failed_states",
          "_handle_skipped_states",
          "execute",
          "execute_complete",
          "hook"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for a different DAG, task group, or task to complete for a specific composer environment.\n\nIf both `composer_external_task_group_id` and `composer_external_task_id` are ``None`` (default), the sensor\nwaits for the DAG.\nValues for `composer_external_task_group_id` and `composer_external_task_id` can't be set at the same time.\n\nBy default, the CloudComposerExternalTaskSensor will wait for the external task to\nsucceed, at which point it will also succeed. However, by default it will\n*not* fai",
      "success_score": 205,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.168725",
      "relevance_keywords": [
        "gcs",
        "task",
        "google",
        "gcp",
        "external",
        "cloudcomposerexternaltasksensor",
        "bigquery",
        "cloud",
        "sensor",
        "composer"
      ]
    },
    {
      "id": "official_clouddatatransferservicejobstatussensor_20260128_160330",
      "component_name": "CloudDataTransferServiceJobStatusSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceJobStatusSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "job_name",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_name",
          "type": "str",
          "required": false
        },
        {
          "name": "expected_statuses",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, conf\nfrom airflow.providers.google.cloud.hooks.cloud_storage_transfer_service import (",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for at least one operation belonging to the job to have the expected status.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:CloudDataTransferServiceJobStatusSensor`\n\n:param job_name: The name of the transfer job\n:param expected_statuses: The expected state of the operation.\n    See:\n    https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n:param project_id: (Optional) the ID of the",
      "success_score": 191,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.183695",
      "relevance_keywords": [
        "gcs",
        "clouddatatransferservicejobstatussensor",
        "service",
        "google",
        "status",
        "gcp",
        "bigquery",
        "data",
        "transfer",
        "job",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_dataflowjobstatussensor_20260128_160330",
      "component_name": "DataflowJobStatusSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowJobStatusSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "job_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "expected_statuses",
          "type": "Union",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_DATAFLOW_LOCATION>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, PokeReturnValue, conf\nfrom airflow.providers.google.cloud.hooks.dataflow import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete",
          "hook"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Checks for the status of a job in Google Cloud Dataflow.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowJobStatusSensor`\n\n:param job_id: ID of the job to be checked.\n:param expected_statuses: The expected state(s) of the operation.\n    See:\n    https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState\n:param project_id: Optional, the Google Cloud project ID in which to start a job.\n    If ",
      "success_score": 193,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.198485",
      "relevance_keywords": [
        "gcs",
        "google",
        "status",
        "gcp",
        "bigquery",
        "dataflow",
        "job",
        "cloud",
        "sensor",
        "dataflowjobstatussensor"
      ]
    },
    {
      "id": "official_dataflowjobmetricssensor_20260128_160330",
      "component_name": "DataflowJobMetricsSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowJobMetricsSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "job_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "callback",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "fail_on_terminal_state",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_DATAFLOW_LOCATION>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, PokeReturnValue, conf\nfrom airflow.providers.google.cloud.hooks.dataflow import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete",
          "hook"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Checks for metrics associated with a single job in Google Cloud Dataflow.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowJobMetricsSensor`\n\n:param job_id: ID of the job to be checked.\n:param callback: callback which is called with list of read job metrics\n    See:\n    https://cloud.google.com/dataflow/docs/reference/rest/v1b3/MetricUpdate\n:param fail_on_terminal_state: If set to true sensor will raise Exception when\n",
      "success_score": 193,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.198641",
      "relevance_keywords": [
        "dataflowjobmetricssensor",
        "metrics",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "dataflow",
        "job",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_dataflowjobmessagessensor_20260128_160330",
      "component_name": "DataflowJobMessagesSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowJobMessagesSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "job_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "callback",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "fail_on_terminal_state",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_DATAFLOW_LOCATION>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 10
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, PokeReturnValue, conf\nfrom airflow.providers.google.cloud.hooks.dataflow import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete",
          "hook"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Checks for job messages associated with a single job in Google Cloud Dataflow.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowJobMessagesSensor`\n\n:param job_id: ID of the Dataflow job to be checked.\n:param callback: a function that can accept a list of serialized job messages.\n    It can do whatever you want it to do. If the callback function is not provided,\n    then on successful completion the task will exit with ",
      "success_score": 193,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.198758",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "messages",
        "bigquery",
        "dataflow",
        "job",
        "cloud",
        "sensor",
        "dataflowjobmessagessensor"
      ]
    },
    {
      "id": "official_dataflowjobautoscalingeventssensor_20260128_160330",
      "component_name": "DataflowJobAutoScalingEventsSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowJobAutoScalingEventsSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "job_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "callback",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "fail_on_terminal_state",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "location",
          "type": "str",
          "required": false,
          "default": "<DEFAULT_DATAFLOW_LOCATION>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 60
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, PokeReturnValue, conf\nfrom airflow.providers.google.cloud.hooks.dataflow import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete",
          "hook"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Checks for autoscaling events associated with a single job in Google Cloud Dataflow.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DataflowJobAutoScalingEventsSensor`\n\n:param job_id: ID of the Dataflow job to be checked.\n:param callback: a function that can accept a list of serialized autoscaling events.\n    It can do whatever you want it to do. If the callback function is not provided,\n    then on successful completion the",
      "success_score": 193,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.198895",
      "relevance_keywords": [
        "gcs",
        "auto",
        "google",
        "scaling",
        "gcp",
        "bigquery",
        "dataflow",
        "job",
        "dataflowjobautoscalingeventssensor",
        "cloud",
        "events",
        "sensor"
      ]
    },
    {
      "id": "official_dataformworkflowinvocationstatesensor_20260128_160330",
      "component_name": "DataformWorkflowInvocationStateSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformWorkflowInvocationStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "workflow_invocation_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": false
        },
        {
          "name": "workflow_invocation_id",
          "type": "str",
          "required": false
        },
        {
          "name": "expected_statuses",
          "type": "Union",
          "required": false
        },
        {
          "name": "failure_statuses",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.dataform import DataformHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Checks for the status of a Workflow Invocation in Google Cloud Dataform.\n\n:param project_id: Required, the Google Cloud project ID in which to start a job.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param region: Required, The location of the Dataform workflow invocation (for example europe-west1).\n:param repository_id: Required. The ID of the Dataform repository that the task belongs to.\n:param workflow_invocation_id: Required, ID of the wor",
      "success_score": 173,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.214390",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "dataform",
        "invocation",
        "bigquery",
        "dataformworkflowinvocationstatesensor",
        "cloud",
        "sensor",
        "workflow",
        "state"
      ]
    },
    {
      "id": "official_dataformworkflowinvocationactionstatesensor_20260128_160330",
      "component_name": "DataformWorkflowInvocationActionStateSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformWorkflowInvocationActionStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "workflow_invocation_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": false
        },
        {
          "name": "workflow_invocation_id",
          "type": "str",
          "required": false
        },
        {
          "name": "target_name",
          "type": "str",
          "required": false
        },
        {
          "name": "expected_statuses",
          "type": "Iterable[...]",
          "required": false
        },
        {
          "name": "failure_statuses",
          "type": "Iterable[...]",
          "required": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.dataform import DataformHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Checks for the status of a Workflow Invocation Action in Google Cloud Dataform.\n\n:param project_id: Required, the Google Cloud project ID in which to start a job.\n    If set to None or missing, the default project_id from the Google Cloud connection is used.\n:param region: Required, The location of the Dataform workflow invocation (for example europe-west1).\n:param repository_id: Required. The ID of the Dataform repository that the task belongs to.\n:param workflow_invocation_id: Required, ID of ",
      "success_score": 173,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.214513",
      "relevance_keywords": [
        "gcs",
        "action",
        "dataformworkflowinvocationactionstatesensor",
        "google",
        "dataform",
        "gcp",
        "invocation",
        "bigquery",
        "cloud",
        "sensor",
        "workflow",
        "state"
      ]
    },
    {
      "id": "official_clouddatafusionpipelinestatesensor_20260128_160330",
      "component_name": "CloudDataFusionPipelineStateSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataFusionPipelineStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "pipeline_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "pipeline_name",
          "type": "str",
          "required": true
        },
        {
          "name": "pipeline_id",
          "type": "str",
          "required": true
        },
        {
          "name": "expected_statuses",
          "type": "Iterable[...]",
          "required": true
        },
        {
          "name": "instance_name",
          "type": "str",
          "required": true
        },
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "failure_statuses",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "namespace",
          "type": "str",
          "required": false,
          "default": "default"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowNotFoundException, BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.datafusion import DataFusionHook\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check the status of the pipeline in the Google Cloud Data Fusion.\n\n:param pipeline_name: Your pipeline name.\n:param pipeline_id: Your pipeline ID.\n:param expected_statuses: State that is expected\n:param failure_statuses: State that will terminate the sensor with an exception\n:param instance_name: The name of the instance.\n:param location: The Cloud Data Fusion location in which to handle the request.\n:param project_id: The ID of the Google Cloud project that the instance belongs to.\n:param names",
      "success_score": 173,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.230123",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "data",
        "pipeline",
        "fusion",
        "cloud",
        "sensor",
        "clouddatafusionpipelinestatesensor",
        "state"
      ]
    },
    {
      "id": "official_dataplextaskstatesensor_20260128_160330",
      "component_name": "DataplexTaskStateSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexTaskStateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "dataplex_task_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "lake_id",
          "type": "str",
          "required": true
        },
        {
          "name": "dataplex_task_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import GoogleAPICallError",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check the status of the Dataplex task.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param lake_id: Required. The ID of the Google Cloud lake that the task belongs to.\n:param dataplex_task_id: Required. Task identifier.\n:param api_version: The version of the api that will be requested for example 'v3'.\n:param retry: A retry object used  to retry requests. If `None` i",
      "success_score": 173,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.243658",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "task",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "sensor",
        "dataplextaskstatesensor",
        "state"
      ]
    },
    {
      "id": "official_dataplexdataqualityjobstatussensor_20260128_160330",
      "component_name": "DataplexDataQualityJobStatusSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexDataQualityJobStatusSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "job_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "job_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "fail_on_dq_failure",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "result_timeout",
          "type": "float",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "start_sensor_time",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import GoogleAPICallError",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_duration",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check the status of the Dataplex DataQuality job.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param data_scan_id: Required. Data Quality scan identifier.\n:param job_id: Required. Job ID.\n:param api_version: The version of the api that will be requested for example 'v3'.\n:param retry: A retry object used  to retry requests. If `None` is specified, requests\n    will ",
      "success_score": 173,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.243830",
      "relevance_keywords": [
        "quality",
        "dataplex",
        "gcs",
        "google",
        "status",
        "gcp",
        "bigquery",
        "data",
        "dataplexdataqualityjobstatussensor",
        "job",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_dataplexdataprofilejobstatussensor_20260128_160330",
      "component_name": "DataplexDataProfileJobStatusSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexDataProfileJobStatusSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "job_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "data_scan_id",
          "type": "str",
          "required": true
        },
        {
          "name": "job_id",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "result_timeout",
          "type": "float",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "start_sensor_time",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import GoogleAPICallError",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_duration",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check the status of the Dataplex DataProfile job.\n\n:param project_id: Required. The ID of the Google Cloud project that the task belongs to.\n:param region: Required. The ID of the Google Cloud region that the task belongs to.\n:param data_scan_id: Required. Data Quality scan identifier.\n:param job_id: Required. Job ID.\n:param api_version: The version of the api that will be requested for example 'v3'.\n:param retry: A retry object used  to retry requests. If `None` is specified, requests\n    will ",
      "success_score": 173,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.243967",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "dataplexdataprofilejobstatussensor",
        "google",
        "status",
        "gcp",
        "bigquery",
        "data",
        "job",
        "cloud",
        "profile",
        "sensor"
      ]
    },
    {
      "id": "official_dataprepjobgroupisfinishedsensor_20260128_160330",
      "component_name": "DataprepJobGroupIsFinishedSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprepJobGroupIsFinishedSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "job_group_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_group_id",
          "type": "Union",
          "required": false
        },
        {
          "name": "dataprep_conn_id",
          "type": "str",
          "required": false,
          "default": "dataprep_default"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.dataprep import GoogleDataprepHook, JobGroupStatuses",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check the status of the Dataprep task to be finished.\n\n:param job_group_id: ID of the job group to check",
      "success_score": 163,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.258162",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "finished",
        "bigquery",
        "is",
        "dataprepjobgroupisfinishedsensor",
        "job",
        "dataprep",
        "group",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_dataprocjobsensor_20260128_160330",
      "component_name": "DataprocJobSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocJobSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "dataproc_job_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataproc_job_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "wait_timeout",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import ServerError\nfrom google.cloud.dataproc_v1.types import Batch, JobStatus\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.dataproc import DataprocHook",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute",
          "_duration",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check for the state of a previously submitted Dataproc job.\n\n:param dataproc_job_id: The Dataproc job ID to poll. (templated)\n:param region: Required. The Cloud Dataproc region in which to handle the request. (templated)\n:param project_id: The ID of the google cloud project in which\n    to create the cluster. (templated)\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud Platform.\n:param wait_timeout: How many seconds wait for job to be ready.",
      "success_score": 179,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.274942",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "job",
        "cloud",
        "dataprocjobsensor",
        "sensor"
      ]
    },
    {
      "id": "official_dataprocbatchsensor_20260128_160330",
      "component_name": "DataprocBatchSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocBatchSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "batch_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "batch_id",
          "type": "str",
          "required": false
        },
        {
          "name": "region",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "wait_timeout",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.exceptions import ServerError\nfrom google.cloud.dataproc_v1.types import Batch, JobStatus\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.dataproc import DataprocHook",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute",
          "_duration",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check for the state of batch.\n\n:param batch_id: The Dataproc batch ID to poll. (templated)\n:param region: Required. The Cloud Dataproc region in which to handle the request. (templated)\n:param project_id: The ID of the google cloud project in which\n    to create the cluster. (templated)\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud Platform.\n:param wait_timeout: How many seconds wait for job to be ready.",
      "success_score": 179,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.275071",
      "relevance_keywords": [
        "dataproc",
        "batch",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "sensor",
        "dataprocbatchsensor"
      ]
    },
    {
      "id": "official_metastorehivepartitionsensor_20260128_160330",
      "component_name": "MetastoreHivePartitionSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "MetastoreHivePartitionSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "service_id",
          "region",
          "table",
          "partitions",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "service_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "table",
          "type": "str",
          "required": true
        },
        {
          "name": "partitions",
          "type": "Union",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.dataproc_metastore import DataprocMetastoreHook\nfrom airflow.providers.google.cloud.hooks.gcs import parse_json_from_gcs",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Waits for partitions to show up in Hive.\n\nThis sensor uses Google Cloud SDK and passes requests via gRPC.\n\n:param service_id: Required. Dataproc Metastore service id.\n:param region: Required. The ID of the Google Cloud region that the service belongs to.\n:param table: Required. Name of the partitioned table\n:param partitions: List of table partitions to wait for.\n    A name of a partition should look like \"ds=1\", or \"a=1/b=2\" in case of nested partitions.\n    Note that you cannot use logical or ",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.306191",
      "relevance_keywords": [
        "hive",
        "gcs",
        "google",
        "gcp",
        "partition",
        "metastorehivepartitionsensor",
        "metastore",
        "bigquery",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_gcsobjectexistencesensor_20260128_160330",
      "component_name": "GCSObjectExistenceSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSObjectExistenceSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "bucket",
          "object",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": false
        },
        {
          "name": "object",
          "type": "str",
          "required": false
        },
        {
          "name": "use_glob",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "google_cloud_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Retry",
          "required": false,
          "default": "<DEFAULT_RETRY>"
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport textwrap\nfrom collections.abc import Callable, Sequence\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, conf, poke_mode_only",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Checks for the existence of a file in Google Cloud Storage.\n\n:param bucket: The Google Cloud Storage bucket where the object is.\n:param object: The name of the object to check in the Google cloud\n    storage bucket.\n:param use_glob: When set to True the object parameter is interpreted as glob\n:param google_cloud_conn_id: The connection ID to use when\n    connecting to Google Cloud Storage.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or ch",
      "success_score": 194,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.327041",
      "relevance_keywords": [
        "existence",
        "gcs",
        "google",
        "object",
        "gcp",
        "bigquery",
        "gcsobjectexistencesensor",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_gcsobjectupdatesensor_20260128_160330",
      "component_name": "GCSObjectUpdateSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSObjectUpdateSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "bucket",
          "object",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": true
        },
        {
          "name": "object",
          "type": "str",
          "required": true
        },
        {
          "name": "ts_func",
          "type": "Callable",
          "required": false,
          "default": "<ts_function>"
        },
        {
          "name": "google_cloud_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport textwrap\nfrom collections.abc import Callable, Sequence\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, conf, poke_mode_only",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Checks if an object is updated in Google Cloud Storage.\n\n:param bucket: The Google Cloud Storage bucket where the object is.\n:param object: The name of the object to download in the Google cloud\n    storage bucket.\n:param ts_func: Callback for defining the update condition. The default callback\n    returns logical_date + schedule_interval. The callback takes the context\n    as parameter.\n:param google_cloud_conn_id: The connection ID to use when\n    connecting to Google Cloud Storage.\n:param imp",
      "success_score": 194,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.327226",
      "relevance_keywords": [
        "gcs",
        "update",
        "google",
        "object",
        "gcp",
        "gcsobjectupdatesensor",
        "bigquery",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_gcsobjectswithprefixexistencesensor_20260128_160330",
      "component_name": "GCSObjectsWithPrefixExistenceSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSObjectsWithPrefixExistenceSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "bucket",
          "prefix",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": true
        },
        {
          "name": "prefix",
          "type": "str",
          "required": true
        },
        {
          "name": "google_cloud_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport textwrap\nfrom collections.abc import Callable, Sequence\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, conf, poke_mode_only",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Checks for the existence of GCS objects at a given prefix, passing matches via XCom.\n\nWhen files matching the given prefix are found, the poke method's criteria will be\nfulfilled and the matching objects will be returned from the operator and passed\nthrough XCom for downstream tasks.\n\n:param bucket: The Google Cloud Storage bucket where the object is.\n:param prefix: The name of the prefix to check in the Google cloud\n    storage bucket.\n:param google_cloud_conn_id: The connection ID to use when\n",
      "success_score": 194,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.327372",
      "relevance_keywords": [
        "existence",
        "gcs",
        "gcsobjectswithprefixexistencesensor",
        "prefix",
        "objects",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "with",
        "sensor"
      ]
    },
    {
      "id": "official_gcsuploadsessioncompletesensor_20260128_160330",
      "component_name": "GCSUploadSessionCompleteSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSUploadSessionCompleteSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "bucket",
          "prefix",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "bucket",
          "type": "str",
          "required": true
        },
        {
          "name": "prefix",
          "type": "str",
          "required": true
        },
        {
          "name": "inactivity_period",
          "type": "float",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "min_objects",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "previous_objects",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "allow_delete",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "google_cloud_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport textwrap\nfrom collections.abc import Callable, Sequence\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom google.cloud.storage.retry import DEFAULT_RETRY\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, conf, poke_mode_only",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "_get_gcs_hook",
          "is_bucket_updated",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Return True if the inactivity period has passed with no increase in the number of objects in the bucket.\n\nChecks for changes in the number of objects at prefix in Google Cloud Storage\nbucket and returns True if the inactivity period has passed with no\nincrease in the number of objects. Note, this sensor will not behave correctly\nin reschedule mode, as the state of the listed objects in the GCS bucket will\nbe lost between rescheduled invocations.\n\n:param bucket: The Google Cloud Storage bucket wh",
      "success_score": 194,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.327651",
      "relevance_keywords": [
        "complete",
        "gcs",
        "upload",
        "google",
        "gcp",
        "bigquery",
        "session",
        "cloud",
        "sensor",
        "gcsuploadsessioncompletesensor"
      ]
    },
    {
      "id": "official_lookercheckpdtbuildsensor_20260128_160330",
      "component_name": "LookerCheckPdtBuildSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "LookerCheckPdtBuildSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "materialization_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "materialization_id",
          "type": "str",
          "required": true
        },
        {
          "name": "looker_conn_id",
          "type": "str",
          "required": true
        },
        {
          "name": "cancel_on_kill",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.looker import JobStatus, LookerHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "on_kill"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check for the state of a previously submitted PDT materialization job.\n\n:param materialization_id: Required. The materialization job ID to poll. (templated)\n:param looker_conn_id: Required. The connection ID to use connecting to Looker.\n:param cancel_on_kill: Optional. Flag which indicates whether cancel the hook's job or not,\n    when on_kill is called.",
      "success_score": 173,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.345634",
      "relevance_keywords": [
        "gcs",
        "build",
        "google",
        "gcp",
        "lookercheckpdtbuildsensor",
        "looker",
        "check",
        "pdt",
        "bigquery",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_pubsubpullsensor_20260128_160330",
      "component_name": "PubSubPullSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "PubSubPullSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "project_id",
          "subscription",
          "impersonation_chain"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false
        },
        {
          "name": "subscription",
          "type": "str",
          "required": false
        },
        {
          "name": "max_messages",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "return_immediately",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "ack_messages",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "messages_callback",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "poke_interval",
          "type": "float",
          "required": false,
          "default": 10.0
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom google.cloud import pubsub_v1\nfrom google.cloud.pubsub_v1.types import ReceivedMessage\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, conf\nfrom airflow.providers.google.cloud.hooks.pubsub import PubSubHook",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete",
          "_convert_to_received_messages",
          "_default_message_callback"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Pulls messages from a PubSub subscription and passes them through XCom.\n\nAlways waits for at least one message to be returned from the subscription.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:PubSubPullSensor`\n\n.. seealso::\n    If you don't want to wait for at least one message to come, use Operator instead:\n    :class:`~airflow.providers.google.cloud.operators.pubsub.PubSubPullOperator`\n\nThis sensor operator will pull u",
      "success_score": 194,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.362651",
      "relevance_keywords": [
        "gcs",
        "pubsubpullsensor",
        "google",
        "gcp",
        "pub",
        "bigquery",
        "sub",
        "pull",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_taskqueueemptysensor_20260128_160330",
      "component_name": "TaskQueueEmptySensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TaskQueueEmptySensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "project_id",
          "location",
          "queue_name",
          "gcp_conn_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "queue_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.tasks import CloudTasksHook\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Pulls tasks count from a cloud task queue; waits for queue to return task count as 0.\n\n:param project_id: the Google Cloud project ID for the subscription (templated)\n:param gcp_conn_id: The connection ID to use connecting to Google Cloud.\n:param queue_name: The queue name to for which task empty sensing is required.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last ac",
      "success_score": 185,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.377435",
      "relevance_keywords": [
        "gcs",
        "empty",
        "task",
        "google",
        "gcp",
        "queue",
        "bigquery",
        "taskqueueemptysensor",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_workflowexecutionsensor_20260128_160330",
      "component_name": "WorkflowExecutionSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowExecutionSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "location",
          "workflow_id",
          "execution_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "workflow_id",
          "type": "str",
          "required": false
        },
        {
          "name": "execution_id",
          "type": "str",
          "required": false
        },
        {
          "name": "location",
          "type": "str",
          "required": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "success_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "failure_states",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "request_timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.workflows.executions_v1beta import Execution\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator\nfrom airflow.providers.google.cloud.hooks.workflows import WorkflowsHook\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Checks state of an execution for the given ``workflow_id`` and ``execution_id``.\n\n:param workflow_id: Required. The ID of the workflow.\n:param execution_id: Required. The ID of the execution.\n:param project_id: Required. The ID of the Google Cloud project the cluster belongs to.\n:param location: Required. The Cloud Dataproc region in which to handle the request.\n:param success_states: Execution states to be considered as successful, by default\n    it's only ``SUCCEEDED`` state\n:param failure_sta",
      "success_score": 179,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.392827",
      "relevance_keywords": [
        "execution",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "workflowexecutionsensor",
        "cloud",
        "sensor",
        "workflow"
      ]
    },
    {
      "id": "official_googlecampaignmanagerreportsensor_20260128_160330",
      "component_name": "GoogleCampaignManagerReportSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCampaignManagerReportSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "profile_id",
          "report_id",
          "file_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "profile_id",
          "type": "str",
          "required": false
        },
        {
          "name": "report_id",
          "type": "str",
          "required": false
        },
        {
          "name": "file_id",
          "type": "str",
          "required": false
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "mode",
          "type": "str",
          "required": false,
          "default": "reschedule"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import BaseSensorOperator\nfrom airflow.providers.google.marketing_platform.hooks.campaign_manager import GoogleCampaignManagerHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "poke",
          "__init__"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Check if report is ready.\n\n.. seealso::\n    Check official API docs:\n    https://developers.google.com/doubleclick-advertisers/rest/v4/reports/get\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleCampaignManagerReportSensor`\n\n:param profile_id: The DFA user profile ID.\n:param report_id: The ID of the report.\n:param file_id: The ID of the report file.\n:param api_version: The version of the api that will be requested, for ",
      "success_score": 182,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.423507",
      "relevance_keywords": [
        "googlecampaignmanagerreportsensor",
        "manager",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "report",
        "cloud",
        "sensor",
        "campaign"
      ]
    },
    {
      "id": "official_googledisplayvideo360getsdfdownloadoperationsensor_20260128_160330",
      "component_name": "GoogleDisplayVideo360GetSDFDownloadOperationSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleDisplayVideo360GetSDFDownloadOperationSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "operation_name",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "operation_name",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "mode",
          "type": "str",
          "required": false,
          "default": "reschedule"
        },
        {
          "name": "poke_interval",
          "type": "int",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator\nfrom airflow.providers.google.marketing_platform.hooks.display_video import GoogleDisplayVideo360Hook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Sensor for detecting the completion of SDF operation.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GoogleDisplayVideo360GetSDFDownloadOperationSensor`\n\n:param operation_name: The name of the operation resource\n:param api_version: The version of the api that will be requested for example 'v1'.\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to ",
      "success_score": 176,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.452584",
      "relevance_keywords": [
        "googledisplayvideo360getsdfdownloadoperationsensor",
        "display",
        "gcs",
        "download",
        "google",
        "operation",
        "gcp",
        "video",
        "bigquery",
        "get",
        "cloud",
        "sensor"
      ]
    },
    {
      "id": "official_googledrivefileexistencesensor_20260128_160330",
      "component_name": "GoogleDriveFileExistenceSensor",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleDriveFileExistenceSensor",
        "component_type": "sensor",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "folder_id",
          "file_name",
          "drive_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "folder_id",
          "type": "str",
          "required": false
        },
        {
          "name": "file_name",
          "type": "str",
          "required": false
        },
        {
          "name": "drive_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import BaseSensorOperator\nfrom airflow.providers.google.suite.hooks.drive import GoogleDriveHook",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Checks for the existence of a file in Google Cloud Storage.\n\n:param folder_id: The Google drive folder where the file is.\n:param file_name: The name of the file to check in Google Drive\n:param drive_id: Optional. The id of the shared Google Drive in which the file resides.\n:param gcp_conn_id: The connection ID to use when\n    connecting to Google Cloud Storage.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts requir",
      "success_score": 182,
      "pattern_type": "official_cloud_sensor",
      "indexed_at": "2026-01-28T16:03:25.472685",
      "relevance_keywords": [
        "existence",
        "gcs",
        "file",
        "google",
        "gcp",
        "bigquery",
        "googledrivefileexistencesensor",
        "cloud",
        "drive",
        "sensor"
      ]
    },
    {
      "id": "official_googleadshook_20260128_160330",
      "component_name": "GoogleAdsHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleAdsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "google_ads_conn_id",
          "type": "str",
          "required": false,
          "default": "google_ads_default"
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom functools import cached_property\nfrom tempfile import NamedTemporaryFile\nfrom typing import IO, TYPE_CHECKING, Any, Literal\nfrom google.ads.googleads.client import GoogleAdsClient\nfrom google.ads.googleads.errors import GoogleAdsException\nfrom google.auth.exceptions import GoogleAuthError\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "get_connection_form_widgets",
          "get_ui_field_behaviour",
          "__init__",
          "search",
          "search_proto_plus",
          "list_accessible_customers",
          "_get_service",
          "_get_client",
          "_get_customer_service",
          "_get_config",
          "_determine_authentication_method",
          "_update_config_with_secret",
          "_search",
          "_extract_rows"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Google Ads API.\n\nThis hook offers two flows of authentication.\n\n1. OAuth Service Account Flow (requires two connections)\n\n    - gcp_conn_id - provides service account details (like any other GCP connection)\n    - google_ads_conn_id - which contains information from Google Ads config.yaml file\n        in the ``extras``. Example of the ``extras``:\n\n        .. code-block:: json\n\n            {\n                \"google_ads_client\": {\n                    \"developer_token\": \"{{ INSERT_TOKE",
      "success_score": 175,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.507322",
      "relevance_keywords": [
        "googleadshook",
        "gcs",
        "ads",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_alloydbhook_20260128_160330",
      "component_name": "AlloyDbHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDbHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING\nimport tenacity\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_alloy_db_admin_client",
          "wait_for_operation",
          "create_cluster",
          "create_secondary_cluster",
          "get_cluster",
          "update_cluster",
          "delete_cluster",
          "create_instance",
          "create_secondary_instance",
          "get_instance",
          "update_instance",
          "delete_instance",
          "create_user",
          "get_user",
          "update_user",
          "delete_user",
          "create_backup",
          "get_backup",
          "update_backup",
          "delete_backup"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Google Alloy DB Hook.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.532093",
      "relevance_keywords": [
        "alloy",
        "db",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "alloydbhook",
        "hook"
      ]
    },
    {
      "id": "official_bigqueryhook_20260128_160330",
      "component_name": "BigQueryHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook",
          "DbApiHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "use_legacy_sql",
          "type": "Union",
          "required": false,
          "default": "<_UNSET>"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": "<_UNSET>"
        },
        {
          "name": "priority",
          "type": "Union",
          "required": false,
          "default": "<_UNSET>"
        },
        {
          "name": "api_resource_configs",
          "type": "Union",
          "required": false,
          "default": "<_UNSET>"
        },
        {
          "name": "impersonation_scopes",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "labels",
          "type": "Union",
          "required": false,
          "default": "<_UNSET>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook, DbApiHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport logging\nimport re\nimport time\nimport uuid\nimport warnings\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom copy import deepcopy",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "get_connection_form_widgets",
          "get_ui_field_behaviour",
          "__init__",
          "get_conn",
          "get_client",
          "get_uri",
          "get_sqlalchemy_engine",
          "get_records",
          "_resolve_table_reference",
          "insert_rows",
          "_get_pandas_df",
          "_get_polars_df",
          "get_df",
          "get_df",
          "get_df",
          "get_pandas_df",
          "table_exists",
          "table_partition_exists",
          "create_table",
          "create_empty_dataset",
          "get_dataset_tables",
          "delete_dataset",
          "update_table",
          "insert_all",
          "update_dataset",
          "get_datasets_list",
          "get_dataset",
          "run_grant_dataset_view_access",
          "run_table_upsert",
          "delete_table",
          "list_rows",
          "get_schema",
          "update_table_schema",
          "poll_job_complete",
          "cancel_job",
          "get_job",
          "_custom_job_id",
          "insert_job",
          "generate_job_id",
          "get_run_after_or_logical_date",
          "split_tablename",
          "get_query_results",
          "scopes"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with BigQuery.\n\nThis hook uses the Google Cloud connection.\n\n:param gcp_conn_id: The Airflow connection used for GCP credentials.\n:param use_legacy_sql: This specifies whether to use legacy SQL dialect.\n:param location: The location of the BigQuery resource.\n:param priority: Specifies a priority for the query.\n    Possible values include INTERACTIVE and BATCH.\n    The default value is INTERACTIVE.\n:param api_resource_configs: This contains params configuration applied for\n    Google Big",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.569036",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "big",
        "bigquery",
        "bigqueryhook",
        "cloud",
        "hook",
        "query"
      ]
    },
    {
      "id": "official_biqquerydatatransferservicehook_20260128_160330",
      "component_name": "BiqQueryDataTransferServiceHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BiqQueryDataTransferServiceHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import copy\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.bigquery_datatransfer_v1 import DataTransferServiceAsyncClient, DataTransferServiceClient\nfrom google.cloud.bigquery_datatransfer_v1.types import (",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "_disable_auto_scheduling",
          "get_conn",
          "create_transfer_config",
          "delete_transfer_config",
          "start_manual_transfer_runs",
          "get_transfer_run"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Bigquery Transfer API.\n\nAll the methods in the hook where ``project_id`` is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.604735",
      "relevance_keywords": [
        "biqquerydatatransferservicehook",
        "gcs",
        "service",
        "google",
        "gcp",
        "bigquery",
        "data",
        "biq",
        "transfer",
        "cloud",
        "hook",
        "query"
      ]
    },
    {
      "id": "official_asyncbiqquerydatatransferservicehook_20260128_160330",
      "component_name": "AsyncBiqQueryDataTransferServiceHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AsyncBiqQueryDataTransferServiceHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom copy import copy\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.bigquery_datatransfer_v1 import DataTransferServiceAsyncClient, DataTransferServiceClient\nfrom google.cloud.bigquery_datatransfer_v1.types import (",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook of the BigQuery service to be used with async client of the Google library.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.604894",
      "relevance_keywords": [
        "gcs",
        "service",
        "google",
        "query",
        "gcp",
        "bigquery",
        "data",
        "biq",
        "transfer",
        "cloud",
        "async",
        "hook",
        "asyncbiqquerydatatransferservicehook"
      ]
    },
    {
      "id": "official_bigtablehook_20260128_160330",
      "component_name": "BigtableHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigtableHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.cloud.bigtable import Client, enums\nfrom google.cloud.bigtable.cluster import Cluster\nfrom google.cloud.bigtable.instance import Instance\nfrom google.cloud.bigtable.table import ClusterState, Table\nfrom airflow.providers.google.common.consts import CLIENT_INFO",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_get_client",
          "get_instance",
          "delete_instance",
          "create_instance",
          "update_instance",
          "create_table",
          "delete_table",
          "update_cluster",
          "get_column_families_for_table",
          "get_cluster_states_for_table"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Bigtable APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.635861",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "bigtable",
        "cloud",
        "bigtablehook",
        "hook"
      ]
    },
    {
      "id": "official_cloudbatchhook_20260128_160330",
      "component_name": "CloudBatchHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBatchHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport itertools\nimport json\nimport time\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING\nfrom google.cloud.batch import ListJobsRequest, ListTasksRequest\nfrom google.cloud.batch_v1 import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "submit_batch_job",
          "delete_job",
          "list_jobs",
          "list_tasks",
          "wait_for_job",
          "get_job"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for the Google Cloud Batch service.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as a sequ",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.665414",
      "relevance_keywords": [
        "batch",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloudbatchhook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudbatchasynchook_20260128_160330",
      "component_name": "CloudBatchAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBatchAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport itertools\nimport json\nimport time\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING\nfrom google.cloud.batch import ListJobsRequest, ListTasksRequest\nfrom google.cloud.batch_v1 import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Async hook for the Google Cloud Batch service.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as ",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.665690",
      "relevance_keywords": [
        "batch",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloudbatchasynchook",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_cloudbuildhook_20260128_160330",
      "component_name": "CloudBuildHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook",
          "OperationHelper"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook, OperationHelper",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.client_options import ClientOptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1 import CloudBuildAsyncClient, CloudBuildClient, GetBuildRequest\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_get_build_id_from_operation",
          "get_conn",
          "cancel_build",
          "create_build_without_waiting_for_result",
          "create_build_trigger",
          "delete_build_trigger",
          "get_build",
          "get_build_trigger",
          "list_build_triggers",
          "list_builds",
          "retry_build",
          "run_build_trigger",
          "update_build_trigger"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for the Google Cloud Build Service.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as a sequ",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.682449",
      "relevance_keywords": [
        "gcs",
        "build",
        "google",
        "gcp",
        "bigquery",
        "cloudbuildhook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudbuildasynchook_20260128_160330",
      "component_name": "CloudBuildAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudBuildAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.client_options import ClientOptions\nfrom google.api_core.exceptions import AlreadyExists\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.devtools.cloudbuild_v1 import CloudBuildAsyncClient, CloudBuildClient, GetBuildRequest\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [],
        "methods": [],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Asynchronous Hook for the Google Cloud Build Service.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.682619",
      "relevance_keywords": [
        "gcs",
        "build",
        "cloudbuildasynchook",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_cloudcomposerhook_20260128_160330",
      "component_name": "CloudComposerHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook",
          "OperationHelper"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook, OperationHelper"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport json\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import urlencode, urljoin\nfrom aiohttp import ClientSession\nfrom google.api_core.client_options import ClientOptions\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "client_options"
        ],
        "methods": [
          "get_environment_client",
          "get_image_versions_client",
          "make_composer_airflow_api_request",
          "get_operation",
          "get_environment_name",
          "get_parent",
          "create_environment",
          "delete_environment",
          "get_environment",
          "list_environments",
          "update_environment",
          "list_image_versions",
          "execute_airflow_command",
          "poll_airflow_command",
          "wait_command_execution_result",
          "trigger_dag_run",
          "get_dag_runs",
          "get_task_instances"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Composer APIs.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.707136",
      "relevance_keywords": [
        "gcs",
        "cloudcomposerhook",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "composer",
        "hook"
      ]
    },
    {
      "id": "official_cloudcomposerasynchook_20260128_160330",
      "component_name": "CloudComposerAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudComposerAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport json\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom urllib.parse import urlencode, urljoin\nfrom aiohttp import ClientSession\nfrom google.api_core.client_options import ClientOptions\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [
          "sync_hook_class",
          "client_options"
        ],
        "methods": [
          "get_environment_name",
          "get_parent"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Composer async APIs.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.707406",
      "relevance_keywords": [
        "cloudcomposerasynchook",
        "gcs",
        "google",
        "gcp",
        "async",
        "bigquery",
        "cloud",
        "composer",
        "hook"
      ]
    },
    {
      "id": "official_cloudlogginghook_20260128_160330",
      "component_name": "CloudLoggingHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudLoggingHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.cloud.logging_v2.services.config_service_v2 import ConfigServiceV2Client\nfrom google.cloud.logging_v2.types import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "get_parent",
          "create_sink",
          "get_sink",
          "list_sinks",
          "delete_sink",
          "update_sink"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Logging Log Sinks API.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate.",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.742610",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "logging",
        "bigquery",
        "cloudlogginghook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudmemorystorehook_20260128_160330",
      "component_name": "CloudMemorystoreHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "_append_label",
          "create_instance",
          "delete_instance",
          "export_instance",
          "failover_instance",
          "get_instance",
          "import_instance",
          "list_instances",
          "update_instance"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Memorystore APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, ",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.786108",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "memorystore",
        "cloud",
        "cloudmemorystorehook",
        "hook"
      ]
    },
    {
      "id": "official_cloudmemorystorememcachedhook_20260128_160330",
      "component_name": "CloudMemorystoreMemcachedHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudMemorystoreMemcachedHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "_append_label",
          "apply_parameters",
          "create_instance",
          "delete_instance",
          "get_instance",
          "list_instances",
          "update_instance",
          "update_parameters"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Memorystore for Memcached service APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n  ",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.786566",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "cloudmemorystorememcachedhook",
        "bigquery",
        "memorystore",
        "memcached",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudrunhook_20260128_160330",
      "component_name": "CloudRunHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport itertools\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.cloud.run_v2 import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "delete_job",
          "create_job",
          "update_job",
          "execute_job",
          "get_job",
          "list_jobs"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for the Google Cloud Run service.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as a sequen",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.822622",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "run",
        "cloudrunhook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudrunasynchook_20260128_160330",
      "component_name": "CloudRunAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport itertools\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.cloud.run_v2 import (",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Async hook for the Google Cloud Run service.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as a ",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.822779",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "run",
        "cloudrunasynchook",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_cloudrunservicehook_20260128_160330",
      "component_name": "CloudRunServiceHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunServiceHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport itertools\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.cloud.run_v2 import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "get_service",
          "create_service",
          "delete_service"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for the Google Cloud Run services.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as a seque",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.823074",
      "relevance_keywords": [
        "gcs",
        "service",
        "google",
        "gcp",
        "bigquery",
        "cloudrunservicehook",
        "run",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudrunserviceasynchook_20260128_160330",
      "component_name": "CloudRunServiceAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudRunServiceAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport itertools\nfrom collections.abc import Iterable, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.cloud.run_v2 import (",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__",
          "get_conn"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Async hook for the Google Cloud Run services.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as a",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.823185",
      "relevance_keywords": [
        "gcs",
        "service",
        "google",
        "gcp",
        "bigquery",
        "run",
        "cloudrunserviceasynchook",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_cloudsqlhook_20260128_160330",
      "component_name": "CloudSQLHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport base64\nimport errno\nimport json\nimport os\nimport platform\nimport random\nimport re\nimport shlex\nimport shutil",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "get_conn",
          "get_instance",
          "create_instance",
          "patch_instance",
          "delete_instance",
          "get_database",
          "create_database",
          "patch_database",
          "delete_database",
          "export_instance",
          "import_instance",
          "clone_instance",
          "create_ssl_certificate",
          "_wait_for_operation_to_complete"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud SQL APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.\n\n:param api_version: This is the version of the api.\n:param gcp_conn_id: The Airflow connection used for GCP credentials.\n:param impersonation_chain: This is the optional service account to impersonate using short term\n    credentials.",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.857672",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "cloudsqlhook",
        "bigquery",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudsqlasynchook_20260128_160330",
      "component_name": "CloudSQLAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport base64\nimport errno\nimport json\nimport os\nimport platform\nimport random\nimport re\nimport shlex\nimport shutil",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Class to get asynchronous hook for Google Cloud SQL.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.858307",
      "relevance_keywords": [
        "cloudsqlasynchook",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_cloudsqldatabasehook_20260128_160330",
      "component_name": "CloudSQLDatabaseHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSQLDatabaseHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_cloudsql_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_sql_default"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "default_gcp_project_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sql_proxy_binary_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ssl_cert",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ssl_key",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ssl_root_cert",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ssl_secret_id",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport base64\nimport errno\nimport json\nimport os\nimport platform\nimport random\nimport re\nimport shlex\nimport shutil",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "sslcert",
          "sslkey",
          "sslrootcert",
          "_get_ssl_temporary_file_path",
          "_get_cert_from_secret",
          "_set_temporary_ssl_file",
          "_get_bool",
          "_check_ssl_file",
          "_validate_inputs",
          "validate_ssl_certs",
          "validate_socket_path_length",
          "_generate_unique_path",
          "_quote",
          "_reserve_port",
          "_generate_connection_uri",
          "_get_instance_socket_name",
          "_get_sqlproxy_instance_specification",
          "_generate_connection_parameters",
          "create_connection",
          "get_sqlproxy_runner",
          "get_database_hook",
          "cleanup_database_hook",
          "reserve_free_tcp_port",
          "free_reserved_port",
          "_get_iam_db_login",
          "_generate_login_token"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Serves DB connection configuration for Google Cloud SQL (Connections of *gcpcloudsqldb://* type).\n\nThe hook is a \"meta\" one. It does not perform an actual connection.\n\nIt is there to retrieve all the parameters configured in gcpcloudsql:// connection,\nstart/stop Cloud SQL Proxy if needed, dynamically generate Postgres or MySQL\nconnection in the database and return an actual Postgres or MySQL hook.\nThe returned Postgres/MySQL hooks are using direct connection or Cloud SQL\nProxy socket/TCP as conf",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.858906",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloudsqldatabasehook",
        "cloud",
        "database",
        "hook"
      ]
    },
    {
      "id": "official_clouddatatransferservicehook_20260128_160330",
      "component_name": "CloudDataTransferServiceHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "create_transfer_job",
          "get_transfer_job",
          "list_transfer_job",
          "enable_transfer_job",
          "update_transfer_job",
          "delete_transfer_job",
          "run_transfer_job",
          "cancel_transfer_operation",
          "get_transfer_operation",
          "list_transfer_operations",
          "pause_transfer_operation",
          "resume_transfer_operation",
          "wait_for_transfer_job",
          "_inject_project_id",
          "operations_contain_expected_statuses"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Google Storage Transfer Service functionalities.\n\nAll methods in the hook with *project_id* in the signature must be called\nwith keyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.898297",
      "relevance_keywords": [
        "gcs",
        "service",
        "google",
        "gcp",
        "bigquery",
        "data",
        "transfer",
        "cloud",
        "clouddatatransferservicehook",
        "hook"
      ]
    },
    {
      "id": "official_clouddatatransferserviceasynchook_20260128_160330",
      "component_name": "CloudDataTransferServiceAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataTransferServiceAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Asynchronous hook for Google Storage Transfer Service.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.898520",
      "relevance_keywords": [
        "gcs",
        "service",
        "google",
        "gcp",
        "bigquery",
        "data",
        "transfer",
        "cloud",
        "async",
        "hook",
        "clouddatatransferserviceasynchook"
      ]
    },
    {
      "id": "official_computeenginehook_20260128_160330",
      "component_name": "ComputeEngineHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.cloud.compute_v1.services.instance_group_managers import InstanceGroupManagersClient\nfrom google.cloud.compute_v1.services.instance_templates import InstanceTemplatesClient\nfrom google.cloud.compute_v1.services.instances import InstancesClient\nfrom googleapiclient.discovery import build",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "get_conn",
          "get_compute_instance_template_client",
          "get_compute_instance_client",
          "get_compute_instance_group_managers_client",
          "insert_instance_template",
          "delete_instance_template",
          "get_instance_template",
          "insert_instance",
          "get_instance",
          "delete_instance",
          "start_instance",
          "stop_instance",
          "set_machine_type",
          "_execute_set_machine_type",
          "insert_instance_group_manager",
          "get_instance_group_manager",
          "delete_instance_group_manager",
          "patch_instance_group_manager",
          "_wait_for_operation_to_complete",
          "_check_zone_operation_status",
          "_check_global_operation_status",
          "get_instance_info",
          "get_instance_address",
          "set_instance_metadata"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Compute Engine APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.935869",
      "relevance_keywords": [
        "gcs",
        "compute",
        "google",
        "gcp",
        "bigquery",
        "engine",
        "computeenginehook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_computeenginesshhook_20260128_160330",
      "component_name": "ComputeEngineSSHHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ComputeEngineSSHHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "SSHHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "instance_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "zone",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "user",
          "type": "Union",
          "required": false,
          "default": "root"
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "hostname",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_iap_tunnel",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_oslogin",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "expire_time",
          "type": "int",
          "required": false,
          "default": 300
        },
        {
          "name": "cmd_timeout",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "max_retries",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from SSHHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport random\nimport shlex\nimport time\nfrom functools import cached_property\nfrom io import StringIO\nfrom typing import Any\nfrom googleapiclient.errors import HttpError\nfrom paramiko.ssh_exception import SSHException\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "get_ui_field_behaviour",
          "__init__",
          "_oslogin_hook",
          "_compute_hook",
          "_load_connection_config",
          "get_conn",
          "_connect_to_instance",
          "_authorize_compute_engine_instance_metadata",
          "_authorize_os_login",
          "_generate_ssh_key"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook to connect to a remote instance in compute engine.\n\n:param instance_name: The name of the Compute Engine instance\n:param zone: The zone of the Compute Engine instance\n:param user: The name of the user on which the login attempt will be made\n:param project_id: The project ID of the remote instance\n:param gcp_conn_id: The connection id to use when fetching connection info\n:param hostname: The hostname of the target instance. If it is not passed, it will be detected\n    automatically.\n:param u",
      "success_score": 175,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.971162",
      "relevance_keywords": [
        "gcs",
        "compute",
        "google",
        "gcp",
        "bigquery",
        "engine",
        "computeenginesshhook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_clouddatacataloghook_20260128_160330",
      "component_name": "CloudDataCatalogHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDataCatalogHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import datacatalog\nfrom google.cloud.datacatalog import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "create_entry",
          "create_entry_group",
          "create_tag",
          "create_tag_template",
          "create_tag_template_field",
          "delete_entry",
          "delete_entry_group",
          "delete_tag",
          "delete_tag_template",
          "delete_tag_template_field",
          "get_entry",
          "get_entry_group",
          "get_tag_template",
          "list_tags",
          "get_tag_for_template_name",
          "lookup_entry",
          "rename_tag_template_field",
          "search_catalog",
          "update_entry",
          "update_tag",
          "update_tag_template",
          "update_tag_template_field"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Data Catalog Service.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as a s",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:25.999088",
      "relevance_keywords": [
        "gcs",
        "clouddatacataloghook",
        "google",
        "gcp",
        "bigquery",
        "data",
        "cloud",
        "hook",
        "catalog"
      ]
    },
    {
      "id": "official_dataflowhook_20260128_160330",
      "component_name": "DataflowHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataflowHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "poll_sleep",
          "type": "int",
          "required": false,
          "default": 10
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "drain_pipeline",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "cancel_timeout",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "wait_until_finished",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "expected_terminal_state",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport functools\nimport json\nimport re\nimport shlex\nimport subprocess\nimport time\nimport uuid\nimport warnings\nfrom collections.abc import Callable, Generator, Sequence",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "get_pipelines_conn",
          "start_template_dataflow",
          "launch_job_with_template",
          "_update_environment",
          "send_launch_template_request",
          "start_flex_template",
          "launch_job_with_flex_template",
          "launch_beam_yaml_job",
          "_build_gcloud_command",
          "_create_dataflow_job_with_gcloud",
          "extract_job_id",
          "build_dataflow_job_name",
          "is_job_dataflow_running",
          "cancel_job",
          "get_job",
          "fetch_job_metrics_by_id",
          "fetch_job_messages_by_id",
          "fetch_job_autoscaling_events_by_id",
          "wait_for_done",
          "is_job_done",
          "create_data_pipeline",
          "get_data_pipeline",
          "run_data_pipeline",
          "delete_data_pipeline",
          "build_parent_name"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Dataflow.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.065794",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "dataflowhook",
        "bigquery",
        "dataflow",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_asyncdataflowhook_20260128_160330",
      "component_name": "AsyncDataflowHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AsyncDataflowHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook",
          "DataflowJobTerminalStateHelper"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook, DataflowJobTerminalStateHelper"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport functools\nimport json\nimport re\nimport shlex\nimport subprocess\nimport time\nimport uuid\nimport warnings\nfrom collections.abc import Callable, Generator, Sequence",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Async hook class for dataflow service.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.066554",
      "relevance_keywords": [
        "asyncdataflowhook",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "dataflow",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_dataformhook_20260128_160330",
      "component_name": "DataformHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dataform_v1beta1 import DataformClient\nfrom google.cloud.dataform_v1beta1.types import (",
        "class_attributes": [],
        "methods": [
          "get_dataform_client",
          "wait_for_workflow_invocation",
          "create_compilation_result",
          "get_compilation_result",
          "create_workflow_invocation",
          "get_workflow_invocation",
          "query_workflow_invocation_actions",
          "cancel_workflow_invocation",
          "create_repository",
          "delete_repository",
          "create_workspace",
          "delete_workspace",
          "write_file",
          "make_directory",
          "remove_directory",
          "remove_file",
          "install_npm_packages"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud DataForm APIs.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.106482",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "dataform",
        "bigquery",
        "dataformhook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_datafusionhook_20260128_160330",
      "component_name": "DataFusionHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataFusionHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport json\nimport os\nimport time\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom urllib.parse import quote, urlencode, urljoin\nimport google.auth\nfrom aiohttp import ClientSession",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "wait_for_operation",
          "wait_for_pipeline_state",
          "_name",
          "_parent",
          "_base_url",
          "_cdap_request",
          "_check_response_status_and_data",
          "get_conn",
          "restart_instance",
          "delete_instance",
          "create_instance",
          "get_instance",
          "get_instance_artifacts",
          "patch_instance",
          "create_pipeline",
          "delete_pipeline",
          "list_pipelines",
          "get_pipeline_workflow",
          "start_pipeline",
          "stop_pipeline",
          "cdap_program_type",
          "cdap_program_id"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google DataFusion.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.136301",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "data",
        "datafusionhook",
        "fusion",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_datafusionasynchook_20260128_160330",
      "component_name": "DataFusionAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataFusionAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport json\nimport os\nimport time\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom urllib.parse import quote, urlencode, urljoin\nimport google.auth\nfrom aiohttp import ClientSession",
        "class_attributes": [
          "sync_hook_class",
          "scopes"
        ],
        "methods": [
          "_base_url"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Class to get asynchronous hook for DataFusion.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.136526",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "data",
        "fusion",
        "datafusionasynchook",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_dataplexhook_20260128_160330",
      "component_name": "DataplexHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook",
          "OperationHelper"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook, OperationHelper"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.client_options import ClientOptions\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dataplex_v1 import (",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "get_dataplex_client",
          "get_dataplex_data_scan_client",
          "get_dataplex_catalog_client",
          "wait_for_operation",
          "create_entry",
          "get_entry",
          "delete_entry",
          "list_entries",
          "search_entries",
          "lookup_entry",
          "update_entry",
          "create_aspect_type",
          "get_aspect_type",
          "create_entry_type",
          "get_entry_type",
          "delete_aspect_type",
          "list_aspect_types",
          "update_aspect_type",
          "delete_entry_type",
          "list_entry_types",
          "update_entry_type",
          "create_entry_group",
          "get_entry_group",
          "delete_entry_group",
          "list_entry_groups",
          "update_entry_group",
          "create_task",
          "delete_task",
          "list_tasks",
          "get_task",
          "delete_lake",
          "create_lake",
          "get_lake",
          "create_zone",
          "delete_zone",
          "create_asset",
          "delete_asset",
          "create_data_scan",
          "run_data_scan",
          "get_data_scan_job",
          "wait_for_data_scan_job",
          "get_data_scan",
          "update_data_scan",
          "delete_data_scan",
          "list_data_scan_jobs"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Dataplex.\n\n:param api_version: The version of the api that will be requested for example 'v3'.\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.176070",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "dataplexhook",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_dataplexasynchook_20260128_160330",
      "component_name": "DataplexAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataplexAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import MutableSequence, Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.client_options import ClientOptions\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.dataplex_v1 import (",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Asynchronous Hook for Google Cloud Dataplex APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.176908",
      "relevance_keywords": [
        "dataplex",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "dataplexasynchook",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_googledataprephook_20260128_160330",
      "component_name": "GoogleDataprepHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleDataprepHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "dataprep_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        }
      ],
      "success_factors": [
        "Inherits from BaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom enum import Enum\nfrom typing import Any\nfrom urllib.parse import urljoin\nimport requests\nfrom requests import HTTPError\nfrom tenacity import retry, stop_after_attempt, wait_exponential",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "_headers",
          "get_jobs_for_job_group",
          "get_job_group",
          "run_job_group",
          "create_flow",
          "copy_flow",
          "delete_flow",
          "run_flow",
          "get_job_group_status",
          "_raise_for_status",
          "create_imported_dataset",
          "create_wrangled_dataset",
          "create_output_object",
          "create_write_settings",
          "delete_imported_dataset"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for connection with Dataprep API.\n\nTo get connection Dataprep with Airflow you need Dataprep token.\n\nhttps://clouddataprep.com/documentation/api#section/Authentication\n\nIt should be added to the Connection in Airflow in JSON format.",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.208897",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "googledataprephook",
        "bigquery",
        "dataprep",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_dataprochook_20260128_160330",
      "component_name": "DataprocHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport shlex\nimport subprocess\nimport time\nimport uuid\nfrom collections.abc import MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.client_options import ClientOptions\nfrom google.api_core.exceptions import ServerError",
        "class_attributes": [],
        "methods": [
          "get_cluster_client",
          "get_template_client",
          "get_job_client",
          "get_batch_client",
          "get_operations_client",
          "dataproc_options_to_args",
          "_build_gcloud_command",
          "_create_dataproc_cluster_with_gcloud",
          "wait_for_operation",
          "create_cluster",
          "delete_cluster",
          "diagnose_cluster",
          "get_cluster",
          "list_clusters",
          "update_cluster",
          "start_cluster",
          "stop_cluster",
          "create_workflow_template",
          "instantiate_workflow_template",
          "instantiate_inline_workflow_template",
          "wait_for_job",
          "get_job",
          "submit_job",
          "cancel_job",
          "create_batch",
          "delete_batch",
          "get_batch",
          "list_batches",
          "wait_for_batch",
          "check_error_for_resource_is_not_ready_msg"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Google Cloud Dataproc APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.253841",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "dataprochook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_dataprocasynchook_20260128_160330",
      "component_name": "DataprocAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport shlex\nimport subprocess\nimport time\nimport uuid\nfrom collections.abc import MutableSequence, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.client_options import ClientOptions\nfrom google.api_core.exceptions import ServerError",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Asynchronous interaction with Google Cloud Dataproc APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.254438",
      "relevance_keywords": [
        "dataprocasynchook",
        "dataproc",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_dataprocmetastorehook_20260128_160330",
      "component_name": "DataprocMetastoreHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataprocMetastoreHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.client_options import ClientOptions\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.metastore_v1 import DataprocMetastoreClient\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.consts import CLIENT_INFO",
        "class_attributes": [],
        "methods": [
          "get_dataproc_metastore_client",
          "get_dataproc_metastore_client_v1beta",
          "wait_for_operation",
          "create_backup",
          "create_metadata_import",
          "create_service",
          "delete_backup",
          "delete_service",
          "export_metadata",
          "get_service",
          "get_backup",
          "list_backups",
          "restore_service",
          "update_service",
          "list_hive_partitions"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Dataproc Metastore APIs.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.296194",
      "relevance_keywords": [
        "dataproc",
        "gcs",
        "google",
        "gcp",
        "dataprocmetastorehook",
        "metastore",
        "bigquery",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_datastorehook_20260128_160330",
      "component_name": "DatastoreHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatastoreHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom googleapiclient.discovery import Resource, build\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "allocate_ids",
          "begin_transaction",
          "commit",
          "lookup",
          "rollback",
          "run_query",
          "get_operation",
          "delete_operation",
          "poll_operation_until_done",
          "export_to_storage_bucket",
          "import_from_storage_bucket"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Google Cloud Datastore. This hook uses the Google Cloud connection.\n\nThis object is not threads safe. If you want to make multiple requests\nsimultaneously, you will need to create a hook per thread.\n\n:param api_version: The version of the API it is going to connect to.",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.315546",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "datastorehook",
        "datastore",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_clouddlphook_20260128_160330",
      "component_name": "CloudDLPHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudDLPHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nimport time",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "_project_deidentify_template_path",
          "_project_stored_info_type_path",
          "_project_inspect_template_path",
          "cancel_dlp_job",
          "create_deidentify_template",
          "create_dlp_job",
          "create_inspect_template",
          "create_job_trigger",
          "create_stored_info_type",
          "deidentify_content",
          "delete_deidentify_template",
          "delete_dlp_job",
          "delete_inspect_template",
          "delete_job_trigger",
          "delete_stored_info_type",
          "get_deidentify_template",
          "get_dlp_job",
          "get_inspect_template",
          "get_job_trigger",
          "get_stored_info_type",
          "inspect_content",
          "list_deidentify_templates",
          "list_dlp_jobs",
          "list_info_types",
          "list_inspect_templates",
          "list_job_triggers",
          "list_stored_info_types",
          "redact_image",
          "reidentify_content",
          "update_deidentify_template",
          "update_inspect_template",
          "update_job_trigger",
          "update_stored_info_type"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Data Loss Prevention (DLP) APIs.\n\nCloud DLP allows clients to detect the presence of Personally Identifiable\nInformation (PII) and other privacy-sensitive data in user-supplied,\nunstructured data streams, like text blocks or images. The service also\nincludes methods for sensitive data redaction and scheduling of data scans\non Google Cloud based data sets.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional servic",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.337977",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "hook",
        "clouddlphook"
      ]
    },
    {
      "id": "official_cloudfunctionshook_20260128_160330",
      "component_name": "CloudFunctionsHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFunctionsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nimport requests\nfrom googleapiclient.discovery import build\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID, GoogleBaseHook",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "_full_location",
          "get_conn",
          "get_function",
          "create_new_function",
          "update_function",
          "upload_function_zip",
          "delete_function",
          "call_function",
          "_wait_for_operation_to_complete"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Google Cloud Functions APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.378933",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "functions",
        "bigquery",
        "cloudfunctionshook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_gcshook_20260128_160330",
      "component_name": "GCSHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport functools\nimport gzip as gz\nimport json\nimport os\nimport shutil\nimport time\nimport warnings\nfrom collections.abc import Callable, Generator, Sequence\nfrom contextlib import contextmanager",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "get_conn",
          "copy",
          "rewrite",
          "download",
          "download",
          "download",
          "download_as_byte_array",
          "provide_file",
          "provide_file_and_upload",
          "upload",
          "exists",
          "get_blob_update_time",
          "is_updated_after",
          "is_updated_between",
          "is_updated_before",
          "is_older_than",
          "delete",
          "get_bucket",
          "delete_bucket",
          "list",
          "_list",
          "_list_blobs_with_match_glob",
          "list_by_timespan",
          "_get_blob",
          "get_size",
          "get_crc32c",
          "get_md5hash",
          "get_metadata",
          "create_bucket",
          "insert_bucket_acl",
          "insert_object_acl",
          "compose",
          "_sync_to_local_dir_delete_stale_local_files",
          "_sync_to_local_dir_if_changed",
          "sync_to_local_dir",
          "sync",
          "_calculate_sync_destination_path",
          "_prepare_sync_plan"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Use the Google Cloud connection to interact with Google Cloud Storage.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.425209",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "gcshook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_gcsasynchook_20260128_160330",
      "component_name": "GCSAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GCSAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport functools\nimport gzip as gz\nimport json\nimport os\nimport shutil\nimport time\nimport warnings\nfrom collections.abc import Callable, Generator, Sequence\nfrom contextlib import contextmanager",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "GCSAsyncHook run on the trigger worker, inherits from GoogleBaseAsyncHook.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.425686",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "gcsasynchook",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_googledeploymentmanagerhook_20260128_160330",
      "component_name": "GoogleDeploymentManagerHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleDeploymentManagerHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import Any\nfrom googleapiclient.discovery import Resource, build\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID, GoogleBaseHook",
        "class_attributes": [],
        "methods": [
          "get_conn",
          "list_deployments",
          "delete_deployment"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Google Cloud Deployment Manager using the Google Cloud connection.\n\nThis allows for scheduled and programmatic inspection and deletion of resources managed by GDM.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.443960",
      "relevance_keywords": [
        "manager",
        "gcs",
        "google",
        "gcp",
        "googledeploymentmanagerhook",
        "bigquery",
        "deployment",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_genaigenerativemodelhook_20260128_160330",
      "component_name": "GenAIGenerativeModelHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGenerativeModelHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom typing import TYPE_CHECKING, Any\nfrom google import genai\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID, GoogleBaseHook",
        "class_attributes": [],
        "methods": [
          "get_genai_client",
          "embed_content",
          "generate_content",
          "supervised_fine_tuning_train",
          "count_tokens",
          "create_cached_content"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Class for Google Cloud Generative AI Vertex AI hook.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.459031",
      "relevance_keywords": [
        "generative",
        "gcs",
        "model",
        "google",
        "gcp",
        "bigquery",
        "genaigenerativemodelhook",
        "cloud",
        "hook",
        "gen"
      ]
    },
    {
      "id": "official_genaigeminiapihook_20260128_160330",
      "component_name": "GenAIGeminiAPIHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GenAIGeminiAPIHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gemini_api_key",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom typing import TYPE_CHECKING, Any\nfrom google import genai\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID, GoogleBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_genai_client",
          "get_batch_job",
          "list_batch_jobs",
          "create_batch_job",
          "delete_batch_job",
          "cancel_batch_job",
          "create_embeddings",
          "upload_file",
          "get_file",
          "download_file",
          "list_files",
          "delete_file"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Class for Google Cloud Generative AI Gemini Developer API hook.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.459179",
      "relevance_keywords": [
        "gemini",
        "gcs",
        "google",
        "gcp",
        "genaigeminiapihook",
        "bigquery",
        "cloud",
        "hook",
        "gen"
      ]
    },
    {
      "id": "official_cloudkmshook_20260128_160330",
      "component_name": "CloudKMSHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudKMSHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport base64\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.kms_v1 import KeyManagementServiceClient\nfrom airflow.providers.google.common.consts import CLIENT_INFO\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "encrypt",
          "decrypt"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Key Management service.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as a",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.502535",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "cloudkmshook",
        "bigquery",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_gkehook_20260128_160330",
      "component_name": "GKEHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport contextlib\nimport json\nimport time\nfrom collections.abc import AsyncGenerator, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.auth.transport import requests as google_requests",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_cluster_manager_client",
          "wait_for_operation",
          "get_operation",
          "_append_label",
          "delete_cluster",
          "create_cluster",
          "get_cluster",
          "check_cluster_autoscaling_ability"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Google Kubernetes Engine cluster APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.529946",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "gkehook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_gkeasynchook_20260128_160330",
      "component_name": "GKEAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport contextlib\nimport json\nimport time\nfrom collections.abc import AsyncGenerator, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.auth.transport import requests as google_requests",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Asynchronous client of GKE.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.530258",
      "relevance_keywords": [
        "gkeasynchook",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_gkekuberneteshook_20260128_160330",
      "component_name": "GKEKubernetesHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEKubernetesHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook",
          "KubernetesHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_url",
          "type": "str",
          "required": true
        },
        {
          "name": "ssl_ca_cert",
          "type": "str",
          "required": true
        },
        {
          "name": "enable_tcp_keepalive",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook, KubernetesHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport contextlib\nimport json\nimport time\nfrom collections.abc import AsyncGenerator, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.auth.transport import requests as google_requests",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "apply_from_yaml_file"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "GKE authenticated hook for standard Kubernetes API.\n\nThis hook provides full set of the standard Kubernetes API provided by the KubernetesHook,\nand at the same time it provides a GKE authentication, so it makes it possible to KubernetesHook\nfunctionality against GKE clusters.",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.530538",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "gkekuberneteshook",
        "hook",
        "bigquery",
        "cloud",
        "kubernetes"
      ]
    },
    {
      "id": "official_gkekubernetesasynchook_20260128_160330",
      "component_name": "GKEKubernetesAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEKubernetesAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook",
          "AsyncKubernetesHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_url",
          "type": "str",
          "required": true
        },
        {
          "name": "ssl_ca_cert",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "enable_tcp_keepalive",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook, AsyncKubernetesHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport contextlib\nimport json\nimport time\nfrom collections.abc import AsyncGenerator, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.auth.transport import requests as google_requests",
        "class_attributes": [
          "sync_hook_class",
          "scopes"
        ],
        "methods": [
          "__init__",
          "_get_config"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Async GKE authenticated hook for standard Kubernetes API.\n\nThis hook provides full set of the standard Kubernetes API provided by the AsyncKubernetesHook,\nand at the same time it provides a GKE authentication, so it makes it possible to KubernetesHook\nfunctionality against GKE clusters.",
      "success_score": 160,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.530836",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "hook",
        "bigquery",
        "gkekubernetesasynchook",
        "cloud",
        "async",
        "kubernetes"
      ]
    },
    {
      "id": "official_lookerhook_20260128_160330",
      "component_name": "LookerHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "LookerHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "looker_conn_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from BaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport time\nfrom enum import Enum\nfrom typing import TYPE_CHECKING\nfrom looker_sdk.rtl import api_settings, auth_session, requests_transport, serialize\nfrom looker_sdk.sdk.api40 import methods as methods40\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "start_pdt_build",
          "check_pdt_build",
          "pdt_build_status",
          "stop_pdt_build",
          "wait_for_job",
          "get_looker_sdk"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Looker APIs.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.546761",
      "relevance_keywords": [
        "gcs",
        "lookerhook",
        "google",
        "gcp",
        "looker",
        "bigquery",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_managedkafkahook_20260128_160330",
      "component_name": "ManagedKafkaHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "ManagedKafkaHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport base64\nimport datetime\nimport json\nimport time\nfrom collections.abc import Sequence\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_managed_kafka_client",
          "wait_for_operation",
          "get_confluent_token",
          "create_cluster",
          "list_clusters",
          "get_cluster",
          "update_cluster",
          "delete_cluster",
          "create_topic",
          "list_topics",
          "get_topic",
          "update_topic",
          "delete_topic",
          "list_consumer_groups",
          "get_consumer_group",
          "update_consumer_group",
          "delete_consumer_group"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Managed Service for Apache Kafka APIs.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.562796",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "kafka",
        "bigquery",
        "managedkafkahook",
        "cloud",
        "managed",
        "hook"
      ]
    },
    {
      "id": "official_mlenginehook_20260128_160330",
      "component_name": "MLEngineHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "MLEngineHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport contextlib\nimport logging\nimport random\nimport time\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING\nfrom aiohttp import ClientSession\nfrom gcloud.aio.auth import AioSession, Token",
        "class_attributes": [],
        "methods": [
          "get_conn",
          "create_job",
          "create_job_without_waiting_result",
          "cancel_job",
          "get_job",
          "_wait_for_job_done",
          "create_version",
          "set_default_version",
          "list_versions",
          "delete_version",
          "create_model",
          "get_model",
          "delete_model",
          "_delete_all_versions",
          "_append_label"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google ML Engine APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.586318",
      "relevance_keywords": [
        "gcs",
        "mlenginehook",
        "google",
        "gcp",
        "bigquery",
        "engine",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_mlengineasynchook_20260128_160330",
      "component_name": "MLEngineAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "MLEngineAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport contextlib\nimport logging\nimport random\nimport time\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING\nfrom aiohttp import ClientSession\nfrom gcloud.aio.auth import AioSession, Token",
        "class_attributes": [
          "sync_hook_class",
          "scopes"
        ],
        "methods": [
          "_check_fileds"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Class to get asynchronous hook for MLEngine.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.586515",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "engine",
        "cloud",
        "mlengineasynchook",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_cloudnaturallanguagehook_20260128_160330",
      "component_name": "CloudNaturalLanguageHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudNaturalLanguageHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.language_v1 import EncodingType, LanguageServiceClient\nfrom google.cloud.language_v1.types import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "analyze_entities",
          "analyze_entity_sentiment",
          "analyze_sentiment",
          "analyze_syntax",
          "annotate_text",
          "classify_text"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Natural Language Service.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.601505",
      "relevance_keywords": [
        "gcs",
        "google",
        "language",
        "gcp",
        "bigquery",
        "cloudnaturallanguagehook",
        "cloud",
        "natural",
        "hook"
      ]
    },
    {
      "id": "official_osloginhook_20260128_160330",
      "component_name": "OSLoginHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "OSLoginHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "import_ssh_public_key"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google OS login APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.612968",
      "relevance_keywords": [
        "osloginhook",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "login",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_pubsubhook_20260128_160330",
      "component_name": "PubSubHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "PubSubHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "enable_message_ordering",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "enable_open_telemetry_tracing",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom base64 import b64decode",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "subscriber_client",
          "publish",
          "_validate_messages",
          "create_topic",
          "delete_topic",
          "create_subscription",
          "delete_subscription",
          "pull",
          "acknowledge"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for accessing Google Pub/Sub.\n\nThe Google Cloud project against which actions are applied is determined by\nthe project embedded in the Connection referenced by gcp_conn_id.",
      "success_score": 155,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.629213",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "pub",
        "bigquery",
        "sub",
        "cloud",
        "hook",
        "pubsubhook"
      ]
    },
    {
      "id": "official_pubsubasynchook_20260128_160330",
      "component_name": "PubSubAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "PubSubAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom base64 import b64decode",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Class to get asynchronous hook for Google Cloud PubSub.",
      "success_score": 155,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.629419",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "pub",
        "bigquery",
        "sub",
        "cloud",
        "pubsubasynchook",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_googlecloudsecretmanagerhook_20260128_160330",
      "component_name": "GoogleCloudSecretManagerHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCloudSecretManagerHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "location",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.secretmanager_v1 import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "client",
          "get_conn",
          "_get_parent",
          "_get_secret_path",
          "_get_secret_version_path",
          "create_secret",
          "add_secret_version",
          "list_secrets",
          "secret_exists",
          "access_secret",
          "delete_secret"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for the Google Cloud Secret Manager API.\n\nSee https://cloud.google.com/secret-manager",
      "success_score": 155,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.662737",
      "relevance_keywords": [
        "manager",
        "gcs",
        "googlecloudsecretmanagerhook",
        "google",
        "gcp",
        "secret",
        "bigquery",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_spannerhook_20260128_160330",
      "component_name": "SpannerHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SpannerHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook",
          "DbApiHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook, DbApiHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections import OrderedDict\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, NamedTuple\nfrom google.api_core.exceptions import AlreadyExists, GoogleAPICallError\nfrom google.cloud.spanner_v1.client import Client\nfrom sqlalchemy import create_engine\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "_get_client",
          "_get_conn_params",
          "get_uri",
          "get_sqlalchemy_engine",
          "get_instance",
          "_apply_to_instance",
          "create_instance",
          "update_instance",
          "delete_instance",
          "get_database",
          "create_database",
          "update_database",
          "delete_database",
          "execute_dml",
          "_execute_sql_in_transaction",
          "_get_openlineage_authority_part",
          "get_openlineage_database_dialect",
          "get_openlineage_database_info",
          "get_openlineage_default_schema"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Spanner APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.695368",
      "relevance_keywords": [
        "gcs",
        "spannerhook",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "hook",
        "spanner"
      ]
    },
    {
      "id": "official_cloudspeechtotexthook_20260128_160330",
      "component_name": "CloudSpeechToTextHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudSpeechToTextHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.speech_v1 import SpeechClient\nfrom google.cloud.speech_v1.types import RecognitionAudio, RecognitionConfig\nfrom airflow.providers.google.common.consts import CLIENT_INFO\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "recognize_speech"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Speech API.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Account Token Creator IAM role.\n    If set as a sequence, t",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.710203",
      "relevance_keywords": [
        "text",
        "gcs",
        "google",
        "gcp",
        "cloudspeechtotexthook",
        "bigquery",
        "to",
        "speech",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_stackdriverhook_20260128_160330",
      "component_name": "StackdriverHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "StackdriverHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport contextlib\nimport json\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import InvalidArgument\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import monitoring_v3\nfrom google.cloud.monitoring_v3 import AlertPolicy, NotificationChannel",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_get_policy_client",
          "_get_channel_client",
          "list_alert_policies",
          "_toggle_policy_status",
          "enable_alert_policies",
          "disable_alert_policies",
          "upsert_alert",
          "delete_alert_policy",
          "list_notification_channels",
          "_toggle_channel_status",
          "enable_notification_channels",
          "disable_notification_channels",
          "upsert_channel",
          "delete_notification_channel"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Stackdriver Hook for connecting with Google Cloud Stackdriver.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.725027",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "stackdriver",
        "hook",
        "stackdriverhook"
      ]
    },
    {
      "id": "official_cloudtaskshook_20260128_160330",
      "component_name": "CloudTasksHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTasksHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.tasks_v2 import CloudTasksClient\nfrom google.cloud.tasks_v2.types import Queue, Task\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.consts import CLIENT_INFO",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "create_queue",
          "update_queue",
          "get_queue",
          "list_queues",
          "delete_queue",
          "purge_queue",
          "pause_queue",
          "resume_queue",
          "create_task",
          "get_task",
          "list_tasks",
          "delete_task",
          "run_task"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Tasks APIs.\n\nCloud Tasks allows developers to manage the execution of background work in their applications.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the la",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.742043",
      "relevance_keywords": [
        "gcs",
        "tasks",
        "google",
        "gcp",
        "bigquery",
        "cloudtaskshook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudtexttospeechhook_20260128_160330",
      "component_name": "CloudTextToSpeechHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTextToSpeechHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.texttospeech_v1 import TextToSpeechClient\nfrom google.cloud.texttospeech_v1.types import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "synthesize_speech"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Text to Speech API.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.755935",
      "relevance_keywords": [
        "text",
        "gcs",
        "cloudtexttospeechhook",
        "google",
        "gcp",
        "bigquery",
        "to",
        "speech",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudtranslatehook_20260128_160330",
      "component_name": "CloudTranslateHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudTranslateHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import (\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "translate"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud translate APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.774892",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "cloudtranslatehook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_translatehook_20260128_160330",
      "component_name": "TranslateHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "TranslateHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook",
          "OperationHelper"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook, OperationHelper"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import MutableMapping, MutableSequence, Sequence\nfrom typing import (\nfrom google.api_core.exceptions import GoogleAPICallError\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.api_core.retry import Retry",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_client",
          "wait_for_operation_done",
          "extract_object_id",
          "translate_text",
          "batch_translate_text",
          "create_dataset",
          "get_dataset",
          "import_dataset_data",
          "list_datasets",
          "delete_dataset",
          "create_model",
          "get_model",
          "list_models",
          "delete_model",
          "translate_document",
          "batch_translate_document",
          "create_glossary",
          "get_glossary",
          "update_glossary",
          "list_glossaries",
          "delete_glossary"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud translation (Advanced) using client version V3.\n\nSee related docs https://cloud.google.com/translate/docs/editions#advanced.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.775218",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "translate",
        "cloud",
        "hook",
        "translatehook"
      ]
    },
    {
      "id": "official_cloudvideointelligencehook_20260128_160330",
      "component_name": "CloudVideoIntelligenceHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVideoIntelligenceHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.videointelligence_v1 import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "annotate_video"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Video Intelligence APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a s",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.801836",
      "relevance_keywords": [
        "gcs",
        "cloudvideointelligencehook",
        "intelligence",
        "google",
        "gcp",
        "video",
        "bigquery",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudvisionhook_20260128_160330",
      "component_name": "CloudVisionHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudVisionHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom copy import deepcopy\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.vision_v1 import (",
        "class_attributes": [
          "_client",
          "product_name_determiner",
          "product_set_name_determiner"
        ],
        "methods": [
          "__init__",
          "get_conn",
          "annotator_client",
          "_check_for_error",
          "create_product_set",
          "get_product_set",
          "update_product_set",
          "delete_product_set",
          "create_product",
          "get_product",
          "update_product",
          "delete_product",
          "create_reference_image",
          "delete_reference_image",
          "add_product_to_product_set",
          "remove_product_from_product_set",
          "annotate_image",
          "batch_annotate_images",
          "text_detection",
          "document_text_detection",
          "label_detection",
          "safe_search_detection",
          "_get_autogenerated_id"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Cloud Vision APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 155,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.832020",
      "relevance_keywords": [
        "cloudvisionhook",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "vision",
        "hook"
      ]
    },
    {
      "id": "official_workflowshook_20260128_160330",
      "component_name": "WorkflowsHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "WorkflowsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud.workflows.executions_v1beta import Execution, ExecutionsClient\nfrom google.cloud.workflows_v1beta import Workflow, WorkflowsClient\nfrom airflow.providers.google.common.consts import CLIENT_INFO\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID, GoogleBaseHook",
        "class_attributes": [],
        "methods": [
          "get_workflows_client",
          "get_executions_client",
          "create_workflow",
          "get_workflow",
          "update_workflow",
          "delete_workflow",
          "list_workflows",
          "create_execution",
          "get_execution",
          "cancel_execution",
          "list_executions"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google GCP APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.862194",
      "relevance_keywords": [
        "workflowshook",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "workflows",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_googlebasehook_20260128_160330",
      "component_name": "GoogleBaseHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleBaseHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport datetime\nimport functools\nimport json\nimport logging\nimport os\nimport tempfile\nfrom collections.abc import Callable, Generator, Sequence\nfrom contextlib import ExitStack, contextmanager",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "get_connection_form_widgets",
          "get_ui_field_behaviour",
          "__init__",
          "get_credentials_and_project_id",
          "get_credentials",
          "_get_access_token",
          "_get_credentials_email",
          "_authorize",
          "_get_field",
          "project_id",
          "num_retries",
          "scopes",
          "quota_retry",
          "operation_in_progress_retry",
          "refresh_credentials_retry",
          "fallback_to_default_project_id",
          "provide_gcp_credential_file",
          "provide_gcp_credential_file_as_context",
          "provide_authorized_gcloud",
          "download_content_from_request",
          "test_connection"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "A base hook for Google cloud-related hooks.\n\nGoogle cloud has a shared REST API client that is built in the same way no matter\nwhich service you use.  This class helps construct and authorize the credentials\nneeded to then call googleapiclient.discovery.build() to actually discover and\nbuild a client for a Google cloud service.\n\nThe class also contains some miscellaneous helper functions.\n\nAll hook derived from this base hook use the 'Google Cloud' connection\ntype. Three ways of authentication a",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.897048",
      "relevance_keywords": [
        "gcs",
        "google",
        "googlebasehook",
        "gcp",
        "bigquery",
        "base",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_googlebaseasynchook_20260128_160330",
      "component_name": "GoogleBaseAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleBaseAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from BaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport datetime\nimport functools\nimport json\nimport logging\nimport os\nimport tempfile\nfrom collections.abc import Callable, Generator, Sequence\nfrom contextlib import ExitStack, contextmanager",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "GoogleBaseAsyncHook inherits from BaseHook class, run on the trigger worker.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.897273",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "base",
        "googlebaseasynchook",
        "cloud",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_googlediscoveryapihook_20260128_160330",
      "component_name": "GoogleDiscoveryApiHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleDiscoveryApiHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_service_name",
          "type": "str",
          "required": true
        },
        {
          "name": "api_version",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom googleapiclient.discovery import Resource, build\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "get_conn",
          "query",
          "_call_api_request",
          "_build_api_request",
          "_paginate_api",
          "_build_next_api_request"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "A hook to use the Google API Discovery Service.\n\n:param api_service_name: The name of the api service that is needed to get the data\n    for example 'youtube'.\n:param api_version: The version of the api that will be requested for example 'v3'.\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last a",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.932021",
      "relevance_keywords": [
        "googlediscoveryapihook",
        "discovery",
        "api",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_cloudfirestorehook_20260128_160330",
      "component_name": "CloudFirestoreHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "CloudFirestoreHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport time\nfrom collections.abc import Sequence\nfrom googleapiclient.discovery import build, build_from_document\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.hooks.base_google import PROVIDE_PROJECT_ID, GoogleBaseHook",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "get_conn",
          "export_documents",
          "_wait_for_operation_to_complete"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for the Google Firestore APIs.\n\nAll the methods in the hook where project_id is used must be called with\nkeyword arguments rather than positional.\n\n:param api_version: API version used (for example v1 or v1beta1).\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:26.982872",
      "relevance_keywords": [
        "gcs",
        "cloudfirestorehook",
        "google",
        "gcp",
        "bigquery",
        "firestore",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_leveldbhook_20260128_160330",
      "component_name": "LevelDBHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "LevelDBHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "leveldb_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import Any\nfrom airflow.providers.common.compat.sdk import (",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "get_connection_form_widgets",
          "get_ui_field_behaviour",
          "__init__",
          "get_conn",
          "close_conn",
          "run",
          "put",
          "get",
          "delete",
          "write_batch"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Plyvel Wrapper to Interact With LevelDB Database.\n\n`LevelDB Connection Documentation <https://plyvel.readthedocs.io/en/latest/>`__",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.005888",
      "relevance_keywords": [
        "level",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "leveldbhook",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_googleanalyticsadminhook_20260128_160330",
      "component_name": "GoogleAnalyticsAdminHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleAnalyticsAdminHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "list_accounts",
          "create_property",
          "delete_property",
          "create_data_stream",
          "delete_data_stream",
          "list_google_ads_links"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Analytics 4 (GA4) Admin API.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.028434",
      "relevance_keywords": [
        "gcs",
        "analytics",
        "google",
        "gcp",
        "googleanalyticsadminhook",
        "bigquery",
        "admin",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_googlecampaignmanagerhook_20260128_160330",
      "component_name": "GoogleCampaignManagerHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCampaignManagerHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.discovery import Resource, build\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "get_conn",
          "delete_report",
          "insert_report",
          "list_reports",
          "patch_report",
          "run_report",
          "update_report",
          "get_report",
          "get_report_file",
          "_conversions_batch_request",
          "conversions_batch_insert",
          "conversions_batch_update"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Campaign Manager.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.055667",
      "relevance_keywords": [
        "manager",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "googlecampaignmanagerhook",
        "campaign",
        "hook"
      ]
    },
    {
      "id": "official_googledisplayvideo360hook_20260128_160330",
      "component_name": "GoogleDisplayVideo360Hook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleDisplayVideo360Hook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom googleapiclient.discovery import Resource, build\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "get_conn_to_display_video",
          "erf_uri",
          "create_sdf_download_operation",
          "get_sdf_download_operation",
          "download_media"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Display & Video 360.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.082013",
      "relevance_keywords": [
        "display",
        "gcs",
        "googledisplayvideo360hook",
        "google",
        "gcp",
        "video",
        "bigquery",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_googlesearchadsreportinghook_20260128_160330",
      "component_name": "GoogleSearchAdsReportingHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleSearchAdsReportingHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_search_ads_default"
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [
          "_conn",
          "default_api_version"
        ],
        "methods": [
          "__init__",
          "_get_config",
          "get_credentials",
          "get_conn",
          "customer_service",
          "fields_service",
          "search",
          "get_custom_column",
          "list_custom_columns",
          "get_field",
          "search_fields"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for the Google Search Ads 360 Reporting API.",
      "success_score": 155,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.110338",
      "relevance_keywords": [
        "googlesearchadsreportinghook",
        "gcs",
        "google",
        "ads",
        "gcp",
        "bigquery",
        "search",
        "cloud",
        "reporting",
        "hook"
      ]
    },
    {
      "id": "official_googlesearchadshook_20260128_160330",
      "component_name": "GoogleSearchAdsHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleSearchAdsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v2"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "get_conn"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for Google Search Ads 360.",
      "success_score": 155,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.110529",
      "relevance_keywords": [
        "gcs",
        "google",
        "ads",
        "gcp",
        "googlesearchadshook",
        "bigquery",
        "search",
        "cloud",
        "hook"
      ]
    },
    {
      "id": "official_googlecalendarhook_20260128_160330",
      "component_name": "GoogleCalendarHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleCalendarHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": true
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom googleapiclient.discovery import build\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "get_events",
          "create_event"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Google Calendar via Google Cloud connection.\n\nReading and writing cells in Google Sheet: https://developers.google.com/calendar/api/v3/reference\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param api_version: API Version. For example v3\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be im",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.135074",
      "relevance_keywords": [
        "googlecalendarhook",
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "calendar",
        "hook"
      ]
    },
    {
      "id": "official_googledrivehook_20260128_160330",
      "component_name": "GoogleDriveHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GoogleDriveHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v3"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import IO, Any\nfrom googleapiclient.discovery import Resource, build\nfrom googleapiclient.errors import Error as GoogleApiClientError\nfrom googleapiclient.http import HttpRequest, MediaFileUpload\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [
          "_conn"
        ],
        "methods": [
          "__init__",
          "get_conn",
          "_ensure_folders_exists",
          "get_media_request",
          "exists",
          "_get_file_info",
          "_resolve_file_path",
          "get_file_id",
          "upload_file",
          "download_file"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook for the Google Drive APIs.\n\n:param api_version: API version used (for example v3).\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the request.\n    If set as a string, the account must grant the originating account\n    the Service Accoun",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.162735",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "cloud",
        "drive",
        "googledrivehook",
        "hook"
      ]
    },
    {
      "id": "official_gsheetshook_20260128_160330",
      "component_name": "GSheetsHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GSheetsHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v4"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "api_endpoint",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import Any\nfrom google.api_core.client_options import ClientOptions\nfrom googleapiclient.discovery import build\nfrom airflow.providers.common.compat.sdk import AirflowException\nfrom airflow.providers.google.common.hooks.base_google import GoogleBaseHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "get_conn",
          "get_values",
          "batch_get_values",
          "update_values",
          "batch_update_values",
          "append_values",
          "clear",
          "batch_clear",
          "get_spreadsheet",
          "get_sheet_titles",
          "create_spreadsheet"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Google Sheets via Google Cloud connection.\n\nReading and writing cells in Google Sheet: https://developers.google.com/sheets/api/guides/values\n\n:param gcp_conn_id: The connection ID to use when fetching connection info.\n:param api_version: API Version\n:param impersonation_chain: Optional service account to impersonate using short-term\n    credentials, or chained list of accounts required to get the access_token\n    of the last account in the list, which will be impersonated in the r",
      "success_score": 170,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.193731",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "bigquery",
        "gsheetshook",
        "cloud",
        "sheets",
        "hook"
      ]
    },
    {
      "id": "official_alloydbcreateuseroperator_20260128_160330",
      "component_name": "AlloyDBCreateUserOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBCreateUserOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "user_id",
          "type": "str",
          "required": true
        },
        {
          "name": "user_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_get_user",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a User in an Alloy DB cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBCreateUserOperator`\n\n:param user_id: Required. ID of the user to create.\n:param user_configuration: Required. The user to create. For more details please see API documentation:\n    https://cloud.google.com/python/docs/reference/alloydb/latest/google.cloud.alloydb_v1.types.User\n:param cluster_id: Required. ID of the cluster for creatin",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:27.219828",
      "relevance_keywords": [
        "alloy",
        "user",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "bigquery",
        "alloydbcreateuseroperator",
        "cloud"
      ]
    },
    {
      "id": "official_alloydbupdateuseroperator_20260128_160330",
      "component_name": "AlloyDBUpdateUserOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBUpdateUserOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        },
        {
          "name": "user_id",
          "type": "str",
          "required": true
        },
        {
          "name": "user_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "allow_missing",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Update an Alloy DB user.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBUpdateUserOperator`\n\n:param user_id: Required. The ID of the user to update.\n:param cluster_id: Required. ID of the cluster.\n:param user_configuration: Required. User to update. For more details please see API documentation:\n        https://cloud.google.com/python/docs/reference/alloydb/latest/google.cloud.alloydb_v1.types.User\n:param update_mask:",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:27.220407",
      "relevance_keywords": [
        "alloy",
        "user",
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "bigquery",
        "cloud",
        "alloydbupdateuseroperator"
      ]
    },
    {
      "id": "official_alloydbdeleteuseroperator_20260128_160330",
      "component_name": "AlloyDBDeleteUserOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBDeleteUserOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "user_id",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete an Alloy DB user.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBDeleteUserOperator`\n\n:param user_id: Required. ID of the user to delete.\n:param cluster_id: Required. ID of the cluster.\n:param request_id: Optional. An optional request ID to identify requests. Specify a unique request ID\n    so that if you must retry your request, the server ignores the request if it has already been\n    completed. The server gu",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:27.221230",
      "relevance_keywords": [
        "alloy",
        "user",
        "gcs",
        "operator",
        "google",
        "gcp",
        "alloydbdeleteuseroperator",
        "bigquery",
        "delete",
        "cloud"
      ]
    },
    {
      "id": "official_alloydbcreatebackupoperator_20260128_160330",
      "component_name": "AlloyDBCreateBackupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBCreateBackupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "backup_id",
          "type": "str",
          "required": true
        },
        {
          "name": "backup_configuration",
          "type": "Union",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_get_backup",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a Backup in an Alloy DB cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBCreateBackupOperator`\n\n:param backup_id: Required. ID of the backup to create.\n:param backup_configuration: Required. Backup to create. For more details please see API documentation:\n        https://cloud.google.com/python/docs/reference/alloydb/latest/google.cloud.alloydb_v1.types.Backup\n:param request_id: Optional. An optional req",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:27.221842",
      "relevance_keywords": [
        "alloy",
        "backup",
        "gcs",
        "operator",
        "google",
        "gcp",
        "create",
        "alloydbcreatebackupoperator",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_alloydbupdatebackupoperator_20260128_160330",
      "component_name": "AlloyDBUpdateBackupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBUpdateBackupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "backup_id",
          "type": "str",
          "required": true
        },
        {
          "name": "backup_configuration",
          "type": "Union",
          "required": true
        },
        {
          "name": "update_mask",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "allow_missing",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "extra_links_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Update an Alloy DB backup.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBUpdateBackupOperator`\n\n:param backup_id: Required. ID of the backup to update.\n:param backup_configuration: Required. Backup to update. For more details please see API documentation:\n        https://cloud.google.com/python/docs/reference/alloydb/latest/google.cloud.alloydb_v1.types.Backup\n:param update_mask: Optional. Field mask is used to speci",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:27.222321",
      "relevance_keywords": [
        "alloy",
        "alloydbupdatebackupoperator",
        "backup",
        "gcs",
        "operator",
        "google",
        "update",
        "gcp",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_alloydbdeletebackupoperator_20260128_160330",
      "component_name": "AlloyDBDeleteBackupOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AlloyDBDeleteBackupOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "AlloyDBWriteBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "backup_id",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from AlloyDBWriteBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import NotFound\nfrom google.api_core.gapic_v1.method import DEFAULT, _MethodDefault\nfrom google.cloud import alloydb_v1\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete an Alloy DB backup.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:AlloyDBDeleteBackupOperator`\n\n:param backup_id: Required. ID of the backup to delete.\n:param request_id: Optional. An optional request ID to identify requests. Specify a unique request ID\n    so that if you must retry your request, the server ignores the request if it has already been\n    completed. The server guarantees that for at least 60 minutes si",
      "success_score": 175,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:27.222811",
      "relevance_keywords": [
        "alloy",
        "backup",
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "delete",
        "cloud",
        "alloydbdeletebackupoperator"
      ]
    },
    {
      "id": "official_dataformremovedirectoryoperator_20260128_160330",
      "component_name": "DataformRemoveDirectoryOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformRemoveDirectoryOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workspace_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workspace_id",
          "type": "str",
          "required": true
        },
        {
          "name": "directory_path",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Removes directory in specified workspace.\n\n:param project_id: Required. The ID of the Google Cloud project where workspace located.\n:param region: Required. The ID of the Google Cloud region where workspace located.\n:param repository_id: Required. The ID of the Dataform repository where workspace located.\n:param workspace_id: Required. The ID of the Dataform workspace where directory located.\n:param path: Required. The directory's full path including directory name, relative to the workspace roo",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:27.520016",
      "relevance_keywords": [
        "dataformremovedirectoryoperator",
        "gcs",
        "operator",
        "google",
        "gcp",
        "dataform",
        "remove",
        "directory",
        "bigquery",
        "cloud"
      ]
    },
    {
      "id": "official_dataforminstallnpmpackagesoperator_20260128_160330",
      "component_name": "DataformInstallNpmPackagesOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DataformInstallNpmPackagesOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [
          "project_id",
          "region",
          "repository_id",
          "workspace_id",
          "impersonation_chain"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "project_id",
          "type": "str",
          "required": true
        },
        {
          "name": "region",
          "type": "str",
          "required": true
        },
        {
          "name": "repository_id",
          "type": "str",
          "required": true
        },
        {
          "name": "workspace_id",
          "type": "str",
          "required": true
        },
        {
          "name": "retry",
          "type": "Union",
          "required": false,
          "default": "<DEFAULT>"
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "metadata",
          "type": "Sequence[...]",
          "required": false,
          "default": []
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleCloudBaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.google.cloud.links.dataform import (",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Install NPM dependencies in the provided workspace.\n\nRequires \"package.json\" to be created in the workspace.\n\n:param project_id: Required. The ID of the Google Cloud project where workspace located.\n:param region: Required. The ID of the Google Cloud region where workspace located.\n:param repository_id: Required. The ID of the Dataform repository where workspace located.\n:param workspace_id: Required. The ID of the Dataform workspace.\n:param retry: Designation of what errors, if any, should be r",
      "success_score": 185,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:27.520351",
      "relevance_keywords": [
        "gcs",
        "packages",
        "operator",
        "google",
        "gcp",
        "dataform",
        "dataforminstallnpmpackagesoperator",
        "install",
        "bigquery",
        "cloud",
        "npm"
      ]
    },
    {
      "id": "official_gkeresumejoboperator_20260128_160330",
      "component_name": "GKEResumeJobOperator",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "GKEResumeJobOperator",
        "component_type": "operator",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GKEOperatorMixin",
          "GoogleCloudBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": true
        },
        {
          "name": "namespace",
          "type": "str",
          "required": true
        },
        {
          "name": "location",
          "type": "str",
          "required": true
        },
        {
          "name": "cluster_name",
          "type": "str",
          "required": true
        },
        {
          "name": "use_internal_ip",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "use_dns_endpoint",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "project_id",
          "type": "str",
          "required": false,
          "default": "<PROVIDE_PROJECT_ID>"
        },
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GKEOperatorMixin, GoogleCloudBaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom google.api_core.exceptions import AlreadyExists\nfrom kubernetes.client import V1JobList, models as k8s\nfrom packaging.version import parse as parse_version",
        "class_attributes": [
          "template_fields",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Resume Job by given name.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:GKEResumeJobOperator`\n\n:param name: The name of the Job to resume\n:param namespace: The name of the Google Kubernetes Engine namespace.\n:param location: The name of the Google Kubernetes Engine zone or region in which the\n    cluster resides, e.g. 'us-central1-a'\n:param cluster_name: The name of the Google Kubernetes Engine cluster.\n:param use_internal_",
      "success_score": 190,
      "pattern_type": "official_cloud_operator",
      "indexed_at": "2026-01-28T16:03:27.739134",
      "relevance_keywords": [
        "gcs",
        "operator",
        "google",
        "gcp",
        "bigquery",
        "resume",
        "cloud",
        "job",
        "gkeresumejoboperator"
      ]
    },
    {
      "id": "official_bigqueryasynchook_20260128_160330",
      "component_name": "BigQueryAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport logging\nimport re\nimport time\nimport uuid\nimport warnings\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom copy import deepcopy",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__",
          "get_records",
          "value_check",
          "_get_numeric_matches",
          "_convert_to_float_if_possible",
          "interval_check"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Uses gcloud-aio library to retrieve Job details.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.913675",
      "relevance_keywords": [
        "bigqueryasynchook",
        "gcs",
        "google",
        "gcp",
        "big",
        "bigquery",
        "cloud",
        "async",
        "hook",
        "query"
      ]
    },
    {
      "id": "official_bigquerytableasynchook_20260128_160330",
      "component_name": "BigQueryTableAsyncHook",
      "category": "cloud",
      "subcategory": "gcp",
      "provider": "google",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BigQueryTableAsyncHook",
        "component_type": "hook",
        "category": "cloud",
        "provider": "google",
        "base_classes": [
          "GoogleBaseAsyncHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "gcp_conn_id",
          "type": "str",
          "required": false,
          "default": "google_cloud_default"
        },
        {
          "name": "impersonation_chain",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from GoogleBaseAsyncHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport logging\nimport re\nimport time\nimport uuid\nimport warnings\nfrom collections.abc import Iterable, Mapping, Sequence\nfrom copy import deepcopy",
        "class_attributes": [
          "sync_hook_class"
        ],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Async hook for BigQuery Table.",
      "success_score": 150,
      "pattern_type": "official_cloud_hook",
      "indexed_at": "2026-01-28T16:03:27.915018",
      "relevance_keywords": [
        "gcs",
        "google",
        "gcp",
        "big",
        "bigquerytableasynchook",
        "bigquery",
        "table",
        "cloud",
        "async",
        "hook",
        "query"
      ]
    },
    {
      "id": "official_httpoperator_20260128_160330",
      "component_name": "HttpOperator",
      "category": "integration",
      "subcategory": "http",
      "provider": "http",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "HttpOperator",
        "component_type": "operator",
        "category": "integration",
        "provider": "http",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "endpoint",
          "data",
          "headers"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "endpoint",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "method",
          "type": "str",
          "required": false,
          "default": "POST"
        },
        {
          "name": "data",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "headers",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "pagination_function",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "response_check",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "response_filter",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "extra_options",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "request_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "http_conn_id",
          "type": "str",
          "required": false,
          "default": "http_default"
        },
        {
          "name": "log_response",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "auth_type",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tcp_keep_alive",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "tcp_keep_alive_idle",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "tcp_keep_alive_count",
          "type": "int",
          "required": false,
          "default": 20
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport base64\nimport pickle\nfrom collections.abc import Callable, Sequence\nfrom typing import TYPE_CHECKING, Any\nfrom aiohttp import BasicAuth\nfrom requests import Response\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseHook, BaseOperator, conf\nfrom airflow.providers.http.triggers.http import HttpTrigger, serialize_auth_type\nfrom airflow.utils.helpers import merge_dicts",
        "class_attributes": [
          "conn_id_field",
          "template_fields",
          "template_fields_renderers",
          "template_ext",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "hook",
          "execute",
          "execute_sync",
          "paginate_sync",
          "execute_async",
          "_resolve_auth_type",
          "process_response",
          "_default_response_maker",
          "execute_complete",
          "paginate_async",
          "_merge_next_page_parameters"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Calls an endpoint on an HTTP system to execute an action.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:HttpOperator`\n\n:param http_conn_id: The :ref:`http connection<howto/connection:http>` to run\n    the operator against\n:param endpoint: The relative part of the full url. (templated)\n:param method: The HTTP method to use, default = \"POST\"\n:param data: The data to pass. POST-data in POST/PUT and params\n    in the URL for a ",
      "success_score": 194,
      "pattern_type": "official_integration_operator",
      "indexed_at": "2026-01-28T16:03:29.178098",
      "relevance_keywords": [
        "http",
        "api",
        "operator",
        "rest",
        "integration",
        "httpoperator"
      ]
    },
    {
      "id": "official_httpsensor_20260128_160330",
      "component_name": "HttpSensor",
      "category": "integration",
      "subcategory": "http",
      "provider": "http",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "HttpSensor",
        "component_type": "sensor",
        "category": "integration",
        "provider": "http",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "endpoint",
          "request_params",
          "headers"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "endpoint",
          "type": "str",
          "required": false
        },
        {
          "name": "http_conn_id",
          "type": "str",
          "required": false,
          "default": "http_default"
        },
        {
          "name": "method",
          "type": "str",
          "required": false,
          "default": "GET"
        },
        {
          "name": "request_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "request_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "headers",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "response_error_codes_allowlist",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "response_check",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "extra_options",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tcp_keep_alive",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "tcp_keep_alive_idle",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "tcp_keep_alive_count",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "tcp_keep_alive_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, conf\nfrom airflow.providers.http.hooks.http import HttpHook\nfrom airflow.providers.http.triggers.http import HttpSensorTrigger",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Execute HTTP GET statement; return False on failure 404 Not Found or `response_check` returning False.\n\nHTTP Error codes other than 404 (like 403) or Connection Refused Error\nwould raise an exception and fail the sensor itself directly (no more poking).\nTo avoid failing the task for other codes than 404, the argument ``response_error_codes_allowlist``\ncan be passed with the list containing all the allowed error status codes, like ``[\"404\", \"503\"]``\nTo skip error status code check at all, the arg",
      "success_score": 194,
      "pattern_type": "official_integration_sensor",
      "indexed_at": "2026-01-28T16:03:29.195281",
      "relevance_keywords": [
        "http",
        "api",
        "httpsensor",
        "rest",
        "sensor",
        "integration"
      ]
    },
    {
      "id": "official_httphook_20260128_160330",
      "component_name": "HttpHook",
      "category": "integration",
      "subcategory": "http",
      "provider": "http",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "HttpHook",
        "component_type": "hook",
        "category": "integration",
        "provider": "http",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "method",
          "type": "str",
          "required": false,
          "default": "POST"
        },
        {
          "name": "http_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "auth_type",
          "type": "Any",
          "required": false,
          "default": null
        },
        {
          "name": "tcp_keep_alive",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "tcp_keep_alive_idle",
          "type": "int",
          "required": false,
          "default": 120
        },
        {
          "name": "tcp_keep_alive_count",
          "type": "int",
          "required": false,
          "default": 20
        },
        {
          "name": "tcp_keep_alive_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "adapter",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport copy\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any, cast\nfrom urllib.parse import urlparse\nimport aiohttp\nimport tenacity\nfrom aiohttp import ClientResponseError\nfrom asgiref.sync import sync_to_async\nfrom requests import PreparedRequest, Request, Response, Session",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name",
          "default_host",
          "default_headers"
        ],
        "methods": [
          "__init__",
          "auth_type",
          "auth_type",
          "get_conn",
          "_set_base_url",
          "_configure_session_from_auth",
          "_extract_auth",
          "_configure_session_from_extra",
          "_configure_session_from_mount_adapters",
          "run",
          "check_response",
          "run_and_check",
          "run_with_advanced_retry",
          "url_from_endpoint",
          "test_connection"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with HTTP servers.\n\n:param method: the API method to be called\n:param http_conn_id: :ref:`http connection<howto/connection:http>` that has the base\n    API url i.e https://www.google.com/ and optional authentication credentials. Default\n    headers can also be specified in the Extra field in json format.\n:param auth_type: The auth type for the service\n:param adapter: An optional instance of `requests.adapters.HTTPAdapter` to mount for the session.\n:param tcp_keep_alive: Enable TCP Keep ",
      "success_score": 170,
      "pattern_type": "official_integration_hook",
      "indexed_at": "2026-01-28T16:03:29.213123",
      "relevance_keywords": [
        "http",
        "api",
        "hook",
        "httphook",
        "rest",
        "integration"
      ]
    },
    {
      "id": "official_httpasynchook_20260128_160330",
      "component_name": "HttpAsyncHook",
      "category": "integration",
      "subcategory": "http",
      "provider": "http",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "HttpAsyncHook",
        "component_type": "hook",
        "category": "integration",
        "provider": "http",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "method",
          "type": "str",
          "required": false,
          "default": "POST"
        },
        {
          "name": "http_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "auth_type",
          "type": "Any",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "retry_delay",
          "type": "float",
          "required": false,
          "default": 1.0
        }
      ],
      "success_factors": [
        "Inherits from BaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport copy\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any, cast\nfrom urllib.parse import urlparse\nimport aiohttp\nimport tenacity\nfrom aiohttp import ClientResponseError\nfrom asgiref.sync import sync_to_async\nfrom requests import PreparedRequest, Request, Response, Session",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "_retryable_error_async"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with HTTP servers asynchronously.\n\n:param method: the API method to be called\n:param http_conn_id: http connection id that has the base\n    API url i.e https://www.google.com/ and optional authentication credentials. Default\n    headers can also be specified in the Extra field in json format.\n:param auth_type: The auth type for the service",
      "success_score": 170,
      "pattern_type": "official_integration_hook",
      "indexed_at": "2026-01-28T16:03:29.213383",
      "relevance_keywords": [
        "http",
        "api",
        "hook",
        "rest",
        "httpasynchook",
        "async",
        "integration"
      ]
    },
    {
      "id": "official_slackapioperator_20260128_160330",
      "component_name": "SlackAPIOperator",
      "category": "notification",
      "subcategory": "messaging",
      "provider": "slack",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SlackAPIOperator",
        "component_type": "operator",
        "category": "notification",
        "provider": "slack",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "slack_conn_id",
          "type": "str",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "method",
          "type": "str",
          "required": false
        },
        {
          "name": "api_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "base_url",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "proxy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry_handlers",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, Literal\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.common.compat.sdk import BaseOperator\nfrom airflow.providers.slack.hooks.slack import SlackHook",
        "class_attributes": [],
        "methods": [
          "__init__",
          "hook",
          "construct_api_call_params",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Base Slack Operator class.\n\n:param slack_conn_id: :ref:`Slack API Connection <howto/connection:slack>`\n    which its password is Slack API token.\n:param method: The Slack API Method to Call (https://api.slack.com/methods).\n:param api_params: API Method call parameters (https://api.slack.com/methods). Optional\n:param timeout: The maximum number of seconds the client will wait to connect\n    and receive a response from Slack. Optional\n:param base_url: A string representing the Slack API base URL. ",
      "success_score": 175,
      "pattern_type": "official_notification_operator",
      "indexed_at": "2026-01-28T16:03:29.234292",
      "relevance_keywords": [
        "operator",
        "message",
        "slackapioperator",
        "slack",
        "notification"
      ]
    },
    {
      "id": "official_slackapipostoperator_20260128_160330",
      "component_name": "SlackAPIPostOperator",
      "category": "notification",
      "subcategory": "messaging",
      "provider": "slack",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SlackAPIPostOperator",
        "component_type": "operator",
        "category": "notification",
        "provider": "slack",
        "base_classes": [
          "SlackAPIOperator"
        ],
        "template_fields": [
          "username",
          "text",
          "attachments",
          "blocks",
          "channel"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "channel",
          "type": "str",
          "required": false,
          "default": "#general"
        },
        {
          "name": "username",
          "type": "str",
          "required": false,
          "default": "Airflow"
        },
        {
          "name": "text",
          "type": "str",
          "required": false,
          "default": "No message has been set.\nHere is a cat video instead\nhttps://www.youtube.com/watch?v=J---aiyznGQ"
        },
        {
          "name": "icon_url",
          "type": "str",
          "required": false,
          "default": "https://raw.githubusercontent.com/apache/airflow/main/airflow-core/src/airflow/ui/public/pin_100.png"
        },
        {
          "name": "blocks",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "attachments",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from SlackAPIOperator",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, Literal\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.common.compat.sdk import BaseOperator\nfrom airflow.providers.slack.hooks.slack import SlackHook",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "construct_api_call_params"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Post messages to a Slack channel.\n\n.. code-block:: python\n\n    slack = SlackAPIPostOperator(\n        task_id=\"post_hello\",\n        dag=dag,\n        text=\"hello there!\",\n        channel=\"#random\",\n    )\n\n:param channel: channel in which to post message on slack name (#general) or\n    ID (C12318391). (templated)\n:param username: Username that airflow will be posting to Slack as. (templated)\n:param text: message to send to slack. (templated)\n:param icon_url: URL to icon used for this message\n:param",
      "success_score": 190,
      "pattern_type": "official_notification_operator",
      "indexed_at": "2026-01-28T16:03:29.234400",
      "relevance_keywords": [
        "post",
        "operator",
        "slackapipostoperator",
        "message",
        "slack",
        "notification"
      ]
    },
    {
      "id": "official_slackapifileoperator_20260128_160330",
      "component_name": "SlackAPIFileOperator",
      "category": "notification",
      "subcategory": "messaging",
      "provider": "slack",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SlackAPIFileOperator",
        "component_type": "operator",
        "category": "notification",
        "provider": "slack",
        "base_classes": [
          "SlackAPIOperator"
        ],
        "template_fields": [
          "channels",
          "initial_comment",
          "filename",
          "filetype",
          "content",
          "title",
          "snippet_type"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "channels",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "initial_comment",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filename",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "filetype",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "content",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "title",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "method_version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "snippet_type",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from SlackAPIOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, Literal\nfrom airflow.exceptions import AirflowProviderDeprecationWarning\nfrom airflow.providers.common.compat.sdk import BaseOperator\nfrom airflow.providers.slack.hooks.slack import SlackHook",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Send a file to a Slack channel.\n\n.. code-block:: python\n\n    # Send file with filename and filetype\n    slack_operator_file = SlackAPIFileOperator(\n        task_id=\"slack_file_upload_1\",\n        dag=dag,\n        slack_conn_id=\"slack\",\n        channels=\"#general,#random\",\n        initial_comment=\"Hello World!\",\n        filename=\"/files/dags/test.txt\",\n        filetype=\"txt\",\n    )\n\n    # Send file content\n    slack_operator_file_content = SlackAPIFileOperator(\n        task_id=\"slack_file_upload_2",
      "success_score": 190,
      "pattern_type": "official_notification_operator",
      "indexed_at": "2026-01-28T16:03:29.234493",
      "relevance_keywords": [
        "slackapifileoperator",
        "file",
        "operator",
        "message",
        "slack",
        "notification"
      ]
    },
    {
      "id": "official_slackwebhookoperator_20260128_160330",
      "component_name": "SlackWebhookOperator",
      "category": "notification",
      "subcategory": "messaging",
      "provider": "slack",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SlackWebhookOperator",
        "component_type": "operator",
        "category": "notification",
        "provider": "slack",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "message",
          "attachments",
          "blocks",
          "channel",
          "username",
          "proxy"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "slack_webhook_conn_id",
          "type": "Any",
          "required": false
        },
        {
          "name": "message",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "attachments",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "blocks",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "channel",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "username",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "icon_emoji",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "icon_url",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "proxy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry_handlers",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import BaseOperator\nfrom airflow.providers.slack.hooks.slack_webhook import SlackWebhookHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "hook",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "This operator allows you to post messages to Slack using Incoming Webhooks.\n\n.. note::\n    You cannot override the default channel (chosen by the user who installed your app),\n    username, or icon when you're using Incoming Webhooks to post messages.\n    Instead, these values will always inherit from the associated Slack App configuration\n    (`link <https://api.slack.com/messaging/webhooks#advanced_message_formatting>`_).\n    It is possible to change this values only in `Legacy Slack Integrati",
      "success_score": 190,
      "pattern_type": "official_notification_operator",
      "indexed_at": "2026-01-28T16:03:29.249765",
      "relevance_keywords": [
        "webhook",
        "slackwebhookoperator",
        "operator",
        "message",
        "slack",
        "notification"
      ]
    },
    {
      "id": "official_slackhook_20260128_160330",
      "component_name": "SlackHook",
      "category": "notification",
      "subcategory": "messaging",
      "provider": "slack",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SlackHook",
        "component_type": "hook",
        "category": "notification",
        "provider": "slack",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "slack_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "base_url",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "proxy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry_handlers",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport time\nimport warnings",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "client",
          "get_conn",
          "_get_conn_params",
          "call",
          "send_file_v2",
          "send_file_v1_to_v2",
          "get_channel_id",
          "_call_conversations_list",
          "test_connection",
          "get_connection_form_widgets",
          "get_ui_field_behaviour"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates a Slack API Connection to be used for calls.\n\nThis class provide a thin wrapper around the ``slack_sdk.WebClient``.\n\n.. seealso::\n    - :ref:`Slack API connection <howto/connection:slack>`\n    - https://api.slack.com/messaging\n    - https://slack.dev/python-slack-sdk/web/index.html\n\n.. warning::\n    This hook intend to use `Slack API` connection\n    and might not work correctly with `Slack Incoming Webhook` and `HTTP` connections.\n\nExamples:\n .. code-block:: python\n\n    # Create hook\n   ",
      "success_score": 175,
      "pattern_type": "official_notification_hook",
      "indexed_at": "2026-01-28T16:03:29.267331",
      "relevance_keywords": [
        "slackhook",
        "message",
        "slack",
        "hook",
        "notification"
      ]
    },
    {
      "id": "official_slackwebhookhook_20260128_160330",
      "component_name": "SlackWebhookHook",
      "category": "notification",
      "subcategory": "messaging",
      "provider": "slack",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SlackWebhookHook",
        "component_type": "hook",
        "category": "notification",
        "provider": "slack",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "slack_webhook_conn_id",
          "type": "str",
          "required": false
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "proxy",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "retry_handlers",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport warnings\nfrom collections.abc import Callable\nfrom functools import cached_property, wraps\nfrom typing import TYPE_CHECKING, Any\nfrom slack_sdk import WebhookClient\nfrom slack_sdk.webhook.async_client import AsyncWebhookClient\nfrom airflow.providers.common.compat.connection import get_async_connection\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowNotFoundException, BaseHook",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "client",
          "get_conn",
          "_get_conn_params",
          "_build_conn_params",
          "_process_body",
          "send_dict",
          "send",
          "send_text",
          "get_connection_form_widgets",
          "get_ui_field_behaviour"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "This class provide a thin wrapper around the ``slack_sdk.WebhookClient``.\n\nThis hook allows you to post messages to Slack by using Incoming Webhooks.\n\n.. seealso::\n    - :ref:`Slack Incoming Webhook connection <howto/connection:slack-incoming-webhook>`\n    - https://api.slack.com/messaging/webhooks\n    - https://slack.dev/python-slack-sdk/webhook/index.html\n\n.. note::\n    You cannot override the default channel (chosen by the user who installed your app),\n    username, or icon when you're using ",
      "success_score": 175,
      "pattern_type": "official_notification_hook",
      "indexed_at": "2026-01-28T16:03:29.283934",
      "relevance_keywords": [
        "webhook",
        "message",
        "slackwebhookhook",
        "slack",
        "hook",
        "notification"
      ]
    },
    {
      "id": "official_databricksjobrunlink_20260128_160330",
      "component_name": "DatabricksJobRunLink",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksJobRunLink",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseOperatorLink"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from BaseOperatorLink",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport hashlib\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator, BaseOperatorLink, XCom, conf\nfrom airflow.providers.databricks.hooks.databricks import (",
        "class_attributes": [
          "name"
        ],
        "methods": [
          "get_link"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Constructs a link to monitor a Databricks Job Run.",
      "success_score": 170,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.341765",
      "relevance_keywords": [
        "data-engineering",
        "operator",
        "link",
        "databricksjobrunlink",
        "run",
        "databricks",
        "job"
      ]
    },
    {
      "id": "official_databrickscreatejobsoperator_20260128_160330",
      "component_name": "DatabricksCreateJobsOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksCreateJobsOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "json",
          "databricks_conn_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "json",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "description",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tags",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tasks",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_clusters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "email_notifications",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "webhook_notifications",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "notification_settings",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout_seconds",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "schedule",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "max_concurrent_runs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "git_source",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "access_control_list",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport hashlib\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator, BaseOperatorLink, XCom, conf\nfrom airflow.providers.databricks.hooks.databricks import (",
        "class_attributes": [
          "template_fields",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "_hook",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Creates (or resets) a Databricks job using the API endpoint.\n\n.. seealso::\n    https://docs.databricks.com/api/workspace/jobs/create\n    https://docs.databricks.com/api/workspace/jobs/reset\n\n:param json: A JSON object containing API parameters which will be passed\n    directly to the ``api/2.2/jobs/create`` endpoint. The other named parameters\n    (i.e. ``name``, ``tags``, ``tasks``, etc.) to this operator will\n    be merged with this json dictionary if they are provided.\n    If there are confli",
      "success_score": 196,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.342187",
      "relevance_keywords": [
        "data-engineering",
        "operator",
        "create",
        "jobs",
        "databrickscreatejobsoperator",
        "databricks"
      ]
    },
    {
      "id": "official_databrickssubmitrunoperator_20260128_160330",
      "component_name": "DatabricksSubmitRunOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksSubmitRunOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "json",
          "databricks_conn_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "json",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tasks",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "spark_jar_task",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "notebook_task",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "spark_python_task",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "spark_submit_task",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "pipeline_task",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "dbt_task",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "new_cluster",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "existing_cluster_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "libraries",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "run_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout_seconds",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        },
        {
          "name": "polling_period_seconds",
          "type": "int",
          "required": false,
          "default": 30
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport hashlib\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator, BaseOperatorLink, XCom, conf\nfrom airflow.providers.databricks.hooks.databricks import (",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "ui_color",
          "ui_fgcolor",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_hook",
          "_get_hook",
          "execute",
          "on_kill",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Submits a Spark job run to Databricks using the api/2.2/jobs/runs/submit API endpoint.\n\nSee: https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunsSubmit\n\nThere are three ways to instantiate this operator.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DatabricksSubmitRunOperator`\n\n:param tasks: Array of Objects(RunSubmitTaskSettings) <= 100 items.\n\n    .. seealso::\n        https://docs.databricks.com/",
      "success_score": 196,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.342491",
      "relevance_keywords": [
        "data-engineering",
        "operator",
        "submit",
        "run",
        "databricks",
        "databrickssubmitrunoperator"
      ]
    },
    {
      "id": "official_databricksrunnowoperator_20260128_160330",
      "component_name": "DatabricksRunNowOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksRunNowOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "json",
          "databricks_conn_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "job_parameters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "json",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "dbt_commands",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "notebook_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "python_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "jar_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "spark_submit_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "python_named_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "idempotency_token",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        },
        {
          "name": "polling_period_seconds",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "databricks_retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "databricks_retry_delay",
          "type": "int",
          "required": false,
          "default": 1
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport hashlib\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator, BaseOperatorLink, XCom, conf\nfrom airflow.providers.databricks.hooks.databricks import (",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "ui_color",
          "ui_fgcolor",
          "operator_extra_links"
        ],
        "methods": [
          "__init__",
          "_hook",
          "_get_hook",
          "execute",
          "execute_complete",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Runs an existing Spark job run to Databricks using the api/2.2/jobs/run-now API endpoint.\n\nSee: https://docs.databricks.com/dev-tools/api/latest/jobs.html#operation/JobsRunNow\n\nThere are two ways to instantiate this operator.\n\nIn the first way, you can take the JSON payload that you typically use\nto call the ``api/2.2/jobs/run-now`` endpoint and pass it directly\nto our ``DatabricksRunNowOperator`` through the ``json`` parameter.\nFor example ::\n\n    json = {\n        \"job_id\": 42,\n        \"job_par",
      "success_score": 196,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.343196",
      "relevance_keywords": [
        "data-engineering",
        "operator",
        "now",
        "run",
        "databricksrunnowoperator",
        "databricks"
      ]
    },
    {
      "id": "official_databrickssqlstatementsoperator_20260128_160330",
      "component_name": "DatabricksSQLStatementsOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksSQLStatementsOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "DatabricksSQLStatementsMixin",
          "BaseOperator"
        ],
        "template_fields": [
          "databricks_conn_id"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "statement",
          "type": "str",
          "required": true
        },
        {
          "name": "warehouse_id",
          "type": "str",
          "required": true
        },
        {
          "name": "catalog",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "schema",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "parameters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        },
        {
          "name": "polling_period_seconds",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "databricks_retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "databricks_retry_delay",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "databricks_retry_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "do_xcom_push",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "wait_for_termination",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "timeout",
          "type": "float",
          "required": false,
          "default": 3600
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from DatabricksSQLStatementsMixin, BaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport hashlib\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator, BaseOperatorLink, XCom, conf\nfrom airflow.providers.databricks.hooks.databricks import (",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "_hook",
          "_get_hook",
          "execute",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Submits a Databricks SQL Statement to Databricks using the api/2.0/sql/statements/ API endpoint.\n\nSee: https://docs.databricks.com/api/workspace/statementexecution\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DatabricksSQLStatementsOperator`\n\n:param statement: The SQL statement to execute. The statement can optionally be parameterized, see parameters.\n:param warehouse_id: Warehouse upon which to execute a statement.\n:param",
      "success_score": 193,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.343500",
      "relevance_keywords": [
        "data-engineering",
        "operator",
        "databrickssqlstatementsoperator",
        "statements",
        "databricks"
      ]
    },
    {
      "id": "official_databrickstaskbaseoperator_20260128_160330",
      "component_name": "DatabricksTaskBaseOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksTaskBaseOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseOperator",
          "ABC"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "caller",
          "type": "str",
          "required": false,
          "default": "DatabricksTaskBaseOperator"
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        },
        {
          "name": "databricks_task_key",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "databricks_retry_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "databricks_retry_delay",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "databricks_retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "existing_cluster_id",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "job_cluster_key",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "new_cluster",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "polling_period_seconds",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "wait_for_termination",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "workflow_run_metadata",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator, ABC",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport hashlib\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator, BaseOperatorLink, XCom, conf\nfrom airflow.providers.databricks.hooks.databricks import (",
        "class_attributes": [],
        "methods": [
          "__init__",
          "_hook",
          "_get_hook",
          "databricks_task_key",
          "_generate_databricks_task_key",
          "_databricks_workflow_task_group",
          "_get_task_base_json",
          "_get_run_json",
          "_launch_job",
          "_handle_terminal_run_state",
          "_get_current_databricks_task",
          "_convert_to_databricks_workflow_task",
          "monitor_databricks_job",
          "execute",
          "execute_complete"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Base class for operators that are run as Databricks job tasks or tasks within a Databricks workflow.\n\n:param caller: The name of the caller operator to be used in the logs.\n:param databricks_conn_id: The name of the Airflow connection to use.\n:param databricks_task_key: An optional task_key used to refer to the task by Databricks API. By\n    default this will be set to the hash of ``dag_id + task_id``.\n:param databricks_retry_args: An optional dictionary with arguments passed to ``tenacity.Retry",
      "success_score": 190,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.343746",
      "relevance_keywords": [
        "data-engineering",
        "task",
        "operator",
        "databrickstaskbaseoperator",
        "base",
        "databricks"
      ]
    },
    {
      "id": "official_databricksreposcreateoperator_20260128_160330",
      "component_name": "DatabricksReposCreateOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksReposCreateOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "repo_path",
          "tag",
          "branch",
          "databricks_conn_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "git_url",
          "type": "str",
          "required": false
        },
        {
          "name": "git_provider",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "branch",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tag",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "repo_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ignore_existing_repo",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        },
        {
          "name": "databricks_retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "databricks_retry_delay",
          "type": "int",
          "required": false,
          "default": 1
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom urllib.parse import urlsplit\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator\nfrom airflow.providers.databricks.hooks.databricks import DatabricksHook",
        "class_attributes": [
          "template_fields",
          "__git_providers__",
          "__aws_code_commit_regexp__",
          "__repos_path_regexp__"
        ],
        "methods": [
          "__init__",
          "__detect_repo_provider__",
          "_hook",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates, and optionally checks out, a Databricks Repo using the POST api/2.0/repos API endpoint.\n\n See: https://docs.databricks.com/dev-tools/api/latest/repos.html#operation/create-repo\n\n:param git_url: Required HTTPS URL of a Git repository\n:param git_provider: Optional name of Git provider. Must be provided if we can't guess its name from URL.\n:param repo_path: optional path for a repository. Must be in the format ``/Repos/{folder}/{repo-name}``.\n    If not specified, it will be created in the",
      "success_score": 187,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.370541",
      "relevance_keywords": [
        "databricksreposcreateoperator",
        "data-engineering",
        "operator",
        "create",
        "repos",
        "databricks"
      ]
    },
    {
      "id": "official_databricksreposupdateoperator_20260128_160330",
      "component_name": "DatabricksReposUpdateOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksReposUpdateOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "repo_path",
          "tag",
          "branch",
          "databricks_conn_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "branch",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tag",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "repo_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "repo_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        },
        {
          "name": "databricks_retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "databricks_retry_delay",
          "type": "int",
          "required": false,
          "default": 1
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom urllib.parse import urlsplit\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator\nfrom airflow.providers.databricks.hooks.databricks import DatabricksHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_hook",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Updates specified repository to a given branch or tag using the PATCH api/2.0/repos API endpoint.\n\nSee: https://docs.databricks.com/dev-tools/api/latest/repos.html#operation/update-repo\n\n:param branch: optional name of branch to update to. Should be specified if ``tag`` is omitted\n:param tag: optional name of tag to update to. Should be specified if ``branch`` is omitted\n:param repo_id: optional ID of existing repository. Should be specified if ``repo_path`` is omitted\n:param repo_path: optional",
      "success_score": 187,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.370667",
      "relevance_keywords": [
        "data-engineering",
        "operator",
        "update",
        "repos",
        "databricksreposupdateoperator",
        "databricks"
      ]
    },
    {
      "id": "official_databricksreposdeleteoperator_20260128_160330",
      "component_name": "DatabricksReposDeleteOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksReposDeleteOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "repo_path",
          "databricks_conn_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "repo_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "repo_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        },
        {
          "name": "databricks_retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "databricks_retry_delay",
          "type": "int",
          "required": false,
          "default": 1
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom urllib.parse import urlsplit\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator\nfrom airflow.providers.databricks.hooks.databricks import DatabricksHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_hook",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Deletes specified repository using the DELETE api/2.0/repos API endpoint.\n\nSee: https://docs.databricks.com/dev-tools/api/latest/repos.html#operation/delete-repo\n\n:param repo_id: optional ID of existing repository. Should be specified if ``repo_path`` is omitted\n:param repo_path: optional path of existing repository. Should be specified if ``repo_id`` is omitted\n:param databricks_conn_id: Reference to the :ref:`Databricks connection <howto/connection:databricks>`.\n    By default and in the commo",
      "success_score": 181,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.370777",
      "relevance_keywords": [
        "data-engineering",
        "operator",
        "repos",
        "delete",
        "databricksreposdeleteoperator",
        "databricks"
      ]
    },
    {
      "id": "official_databrickssqloperator_20260128_160330",
      "component_name": "DatabricksSqlOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksSqlOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "SQLExecuteQueryOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "http_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sql_endpoint_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "session_configuration",
          "type": "Any",
          "required": false,
          "default": null
        },
        {
          "name": "http_headers",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "catalog",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "schema",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "output_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "output_format",
          "type": "str",
          "required": false,
          "default": "csv"
        },
        {
          "name": "csv_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "client_parameters",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from SQLExecuteQueryOperator",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport csv\nimport json\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom databricks.sql.utils import ParamEscaper\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "conn_id_field"
        ],
        "methods": [
          "__init__",
          "_hook",
          "get_db_hook",
          "_should_run_output_processing",
          "_process_output"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Executes SQL code in a Databricks SQL endpoint or a Databricks cluster.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DatabricksSqlOperator`\n\n:param databricks_conn_id: Reference to\n    :ref:`Databricks connection id<howto/connection:databricks>` (templated)\n:param http_path: Optional string specifying HTTP path of Databricks SQL Endpoint or cluster.\n    If not specified, it should be either specified in the Databricks conn",
      "success_score": 175,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.409400",
      "relevance_keywords": [
        "data-engineering",
        "sql",
        "operator",
        "databricks",
        "databrickssqloperator"
      ]
    },
    {
      "id": "official_databrickscopyintooperator_20260128_160330",
      "component_name": "DatabricksCopyIntoOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksCopyIntoOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "file_location",
          "files",
          "table_name",
          "databricks_conn_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "table_name",
          "type": "str",
          "required": false
        },
        {
          "name": "file_location",
          "type": "str",
          "required": false
        },
        {
          "name": "file_format",
          "type": "str",
          "required": false
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "http_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sql_endpoint_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "session_configuration",
          "type": "Any",
          "required": false,
          "default": null
        },
        {
          "name": "http_headers",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "client_parameters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "catalog",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "schema",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "files",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "pattern",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "expression_list",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "credential",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport csv\nimport json\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, ClassVar\nfrom databricks.sql.utils import ParamEscaper\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_get_hook",
          "_hook",
          "_generate_options",
          "_create_sql_query",
          "execute",
          "on_kill",
          "_build_input_openlineage_dataset",
          "_build_output_openlineage_dataset",
          "get_openlineage_facets_on_complete"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Executes COPY INTO command in a Databricks SQL endpoint or a Databricks cluster.\n\nCOPY INTO command is constructed from individual pieces, that are described in\n`documentation <https://docs.databricks.com/sql/language-manual/delta-copy-into.html>`_.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DatabricksSqlCopyIntoOperator`\n\n:param table_name: Required name of the table. (templated)\n:param file_location: Required location ",
      "success_score": 187,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.409703",
      "relevance_keywords": [
        "data-engineering",
        "operator",
        "into",
        "databrickscopyintooperator",
        "copy",
        "databricks"
      ]
    },
    {
      "id": "official_databrickssqlstatementssensor_20260128_160330",
      "component_name": "DatabricksSQLStatementsSensor",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksSQLStatementsSensor",
        "component_type": "sensor",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "DatabricksSQLStatementsMixin",
          "BaseSensorOperator"
        ],
        "template_fields": [
          "databricks_conn_id",
          "statement",
          "statement_id"
        ],
        "has_deferrable": true,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "warehouse_id",
          "type": "str",
          "required": true
        },
        {
          "name": "statement",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "statement_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "catalog",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "schema",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "parameters",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        },
        {
          "name": "polling_period_seconds",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "databricks_retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "databricks_retry_delay",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "databricks_retry_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "do_xcom_push",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "wait_for_termination",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "timeout",
          "type": "float",
          "required": false,
          "default": 3600
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from DatabricksSQLStatementsMixin, BaseSensorOperator",
        "Implements poke() method for polling",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, conf\nfrom airflow.providers.databricks.hooks.databricks import DatabricksHook, SQLStatementState\nfrom airflow.providers.databricks.operators.databricks import DEFER_METHOD_NAME\nfrom airflow.providers.databricks.utils.mixins import DatabricksSQLStatementsMixin",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "ui_color",
          "ui_fgcolor"
        ],
        "methods": [
          "__init__",
          "_hook",
          "_get_hook",
          "execute",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "DatabricksSQLStatementsSensor.",
      "success_score": 164,
      "pattern_type": "official_data-engineering_sensor",
      "indexed_at": "2026-01-28T16:03:29.470382",
      "relevance_keywords": [
        "data-engineering",
        "databrickssqlstatementssensor",
        "statements",
        "databricks",
        "sensor"
      ]
    },
    {
      "id": "official_databrickspartitionsensor_20260128_160330",
      "component_name": "DatabricksPartitionSensor",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksPartitionSensor",
        "component_type": "sensor",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "databricks_conn_id",
          "catalog",
          "schema",
          "table_name",
          "partitions",
          "http_headers"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "http_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sql_warehouse_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "session_configuration",
          "type": "Any",
          "required": false,
          "default": null
        },
        {
          "name": "http_headers",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "catalog",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "schema",
          "type": "str",
          "required": false,
          "default": "default"
        },
        {
          "name": "table_name",
          "type": "str",
          "required": false
        },
        {
          "name": "partitions",
          "type": "dict",
          "required": false
        },
        {
          "name": "partition_operator",
          "type": "str",
          "required": false,
          "default": "="
        },
        {
          "name": "handler",
          "type": "Callable[...]",
          "required": false,
          "default": "<fetch_all_handler>"
        },
        {
          "name": "client_parameters",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Sequence\nfrom datetime import datetime\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom databricks.sql.utils import ParamEscaper\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "_sql_sensor",
          "_get_hook",
          "_check_table_partitions",
          "_generate_partition_query",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Sensor to detect the presence of table partitions in Databricks.\n\n:param databricks_conn_id: Reference to :ref:`Databricks\n    connection id<howto/connection:databricks>` (templated), defaults to\n    DatabricksSqlHook.default_conn_name.\n:param sql_warehouse_name: Optional name of Databricks SQL warehouse. If not specified, ``http_path``\n    must be provided as described below, defaults to None\n:param http_path: Optional string specifying HTTP path of Databricks SQL warehouse or All Purpose clust",
      "success_score": 190,
      "pattern_type": "official_data-engineering_sensor",
      "indexed_at": "2026-01-28T16:03:29.503633",
      "relevance_keywords": [
        "databrickspartitionsensor",
        "data-engineering",
        "partition",
        "databricks",
        "sensor"
      ]
    },
    {
      "id": "official_databrickssqlsensor_20260128_160330",
      "component_name": "DatabricksSqlSensor",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksSqlSensor",
        "component_type": "sensor",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "databricks_conn_id",
          "sql",
          "catalog",
          "schema",
          "http_headers"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "http_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sql_warehouse_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "session_configuration",
          "type": "Any",
          "required": false,
          "default": null
        },
        {
          "name": "http_headers",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "catalog",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "schema",
          "type": "str",
          "required": false,
          "default": "default"
        },
        {
          "name": "sql",
          "type": "Union",
          "required": false
        },
        {
          "name": "handler",
          "type": "Callable[...]",
          "required": false,
          "default": "<fetch_all_handler>"
        },
        {
          "name": "client_parameters",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Callable, Iterable, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator\nfrom airflow.providers.common.sql.hooks.handlers import fetch_all_handler\nfrom airflow.providers.databricks.hooks.databricks_sql import DatabricksSqlHook",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "hook",
          "_get_results",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Sensor that runs a SQL query on Databricks.\n\n:param databricks_conn_id: Reference to :ref:`Databricks\n    connection id<howto/connection:databricks>` (templated), defaults to\n    DatabricksSqlHook.default_conn_name.\n:param sql_warehouse_name: Optional name of Databricks SQL warehouse. If not specified, ``http_path``\n    must be provided as described below, defaults to None\n:param http_path: Optional string specifying HTTP path of Databricks SQL warehouse or All Purpose cluster.\n    If not specif",
      "success_score": 190,
      "pattern_type": "official_data-engineering_sensor",
      "indexed_at": "2026-01-28T16:03:29.535060",
      "relevance_keywords": [
        "data-engineering",
        "sql",
        "databrickssqlsensor",
        "databricks",
        "sensor"
      ]
    },
    {
      "id": "official_databrickshook_20260128_160330",
      "component_name": "DatabricksHook",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksHook",
        "component_type": "hook",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseDatabricksHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "timeout_seconds",
          "type": "int",
          "required": false,
          "default": 180
        },
        {
          "name": "retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "retry_delay",
          "type": "float",
          "required": false,
          "default": 1.0
        },
        {
          "name": "retry_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "caller",
          "type": "str",
          "required": false,
          "default": "DatabricksHook"
        }
      ],
      "success_factors": [
        "Inherits from BaseDatabricksHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations",
        "class_attributes": [
          "hook_name"
        ],
        "methods": [
          "__init__",
          "create_job",
          "reset_job",
          "update_job",
          "run_now",
          "submit_run",
          "list_jobs",
          "find_job_id_by_name",
          "list_pipelines",
          "find_pipeline_id_by_name",
          "get_run_page_url",
          "get_job_id",
          "get_run_state",
          "get_run",
          "get_run_state_str",
          "get_run_state_lifecycle",
          "get_run_state_result",
          "get_run_state_message",
          "get_run_output",
          "cancel_run",
          "cancel_all_runs",
          "delete_run",
          "repair_run",
          "get_latest_repair_id",
          "get_cluster_state",
          "restart_cluster",
          "start_cluster",
          "terminate_cluster",
          "install",
          "uninstall",
          "update_repo",
          "delete_repo",
          "create_repo",
          "get_repo_by_path",
          "update_job_permission",
          "post_sql_statement",
          "get_sql_statement_state",
          "cancel_sql_statement",
          "test_connection"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Databricks.\n\n:param databricks_conn_id: Reference to the :ref:`Databricks connection <howto/connection:databricks>`.\n:param timeout_seconds: The amount of time in seconds the requests library\n    will wait before timing-out.\n:param retry_limit: The number of times to retry the connection in case of\n    service outages.\n:param retry_delay: The number of seconds to wait between retries (it\n    might be a floating point number).\n:param retry_args: An optional dictionary with arguments",
      "success_score": 170,
      "pattern_type": "official_data-engineering_hook",
      "indexed_at": "2026-01-28T16:03:29.571312",
      "relevance_keywords": [
        "databrickshook",
        "data-engineering",
        "hook",
        "databricks"
      ]
    },
    {
      "id": "official_basedatabrickshook_20260128_160330",
      "component_name": "BaseDatabricksHook",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "BaseDatabricksHook",
        "component_type": "hook",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "timeout_seconds",
          "type": "int",
          "required": false,
          "default": 180
        },
        {
          "name": "retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "retry_delay",
          "type": "float",
          "required": false,
          "default": 1.0
        },
        {
          "name": "retry_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "caller",
          "type": "str",
          "required": false,
          "default": "Unknown"
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport copy\nimport platform\nimport time",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "extra_parameters"
        ],
        "methods": [
          "__init__",
          "databricks_conn",
          "get_conn",
          "user_agent_header",
          "user_agent_value",
          "host",
          "_parse_host",
          "_get_connection_attr",
          "_get_retry_object",
          "_a_get_retry_object",
          "_get_sp_token",
          "_get_aad_token",
          "_get_aad_token_for_default_az_credential",
          "_get_aad_headers",
          "_is_oauth_token_valid",
          "_check_azure_metadata_service",
          "_validate_azure_metadata_service",
          "_get_token",
          "_log_request_error",
          "_endpoint_url",
          "_do_api_call",
          "_get_error_code",
          "_retryable_error"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Base for interaction with Databricks.\n\n:param databricks_conn_id: Reference to the :ref:`Databricks connection <howto/connection:databricks>`.\n:param timeout_seconds: The amount of time in seconds the requests library\n    will wait before timing-out.\n:param retry_limit: The number of times to retry the connection in case of\n    service outages.\n:param retry_delay: The number of seconds to wait between retries (it\n    might be a floating point number).\n:param retry_args: An optional dictionary wi",
      "success_score": 175,
      "pattern_type": "official_data-engineering_hook",
      "indexed_at": "2026-01-28T16:03:29.619274",
      "relevance_keywords": [
        "data-engineering",
        "base",
        "basedatabrickshook",
        "databricks",
        "hook"
      ]
    },
    {
      "id": "official_databrickssqlhook_20260128_160330",
      "component_name": "DatabricksSqlHook",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksSqlHook",
        "component_type": "hook",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "BaseDatabricksHook",
          "DbApiHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "http_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sql_endpoint_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "session_configuration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "http_headers",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "catalog",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "schema",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "caller",
          "type": "str",
          "required": false,
          "default": "DatabricksSqlHook"
        }
      ],
      "success_factors": [
        "Inherits from BaseDatabricksHook, DbApiHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport threading\nfrom collections import namedtuple\nfrom collections.abc import Callable, Iterable, Mapping, Sequence\nfrom contextlib import closing\nfrom copy import copy\nfrom datetime import timedelta\nfrom typing import (",
        "class_attributes": [
          "hook_name",
          "_test_connection_sql"
        ],
        "methods": [
          "__init__",
          "_get_extra_config",
          "_get_sql_endpoint_by_name",
          "get_conn",
          "sqlalchemy_url",
          "get_uri",
          "run",
          "run",
          "run",
          "_make_common_data_structure",
          "bulk_dump",
          "bulk_load",
          "get_openlineage_database_info",
          "get_openlineage_database_dialect",
          "get_openlineage_default_schema",
          "get_openlineage_database_specific_lineage"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook to interact with Databricks SQL.\n\n:param databricks_conn_id: Reference to the\n    :ref:`Databricks connection <howto/connection:databricks>`.\n:param http_path: Optional string specifying HTTP path of Databricks SQL Endpoint or cluster.\n    If not specified, it should be either specified in the Databricks connection's extra parameters,\n    or ``sql_endpoint_name`` must be specified.\n:param sql_endpoint_name: Optional name of Databricks SQL Endpoint. If not specified, ``http_path``\n    must b",
      "success_score": 170,
      "pattern_type": "official_data-engineering_hook",
      "indexed_at": "2026-01-28T16:03:29.659014",
      "relevance_keywords": [
        "data-engineering",
        "sql",
        "databrickssqlhook",
        "databricks",
        "hook"
      ]
    },
    {
      "id": "official_databricksnotebookoperator_20260128_160330",
      "component_name": "DatabricksNotebookOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksNotebookOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "DatabricksTaskBaseOperator"
        ],
        "template_fields": [
          "notebook_params",
          "workflow_run_metadata"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "notebook_path",
          "type": "str",
          "required": true
        },
        {
          "name": "source",
          "type": "str",
          "required": true
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        },
        {
          "name": "databricks_retry_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "databricks_retry_delay",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "databricks_retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "existing_cluster_id",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "job_cluster_key",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "new_cluster",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "notebook_packages",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "notebook_params",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "polling_period_seconds",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "wait_for_termination",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "workflow_run_metadata",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DatabricksTaskBaseOperator",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport hashlib\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator, BaseOperatorLink, XCom, conf\nfrom airflow.providers.databricks.hooks.databricks import (",
        "class_attributes": [
          "template_fields",
          "CALLER"
        ],
        "methods": [
          "__init__",
          "_get_task_timeout_seconds",
          "_get_task_base_json",
          "_extend_workflow_notebook_packages",
          "_convert_to_databricks_workflow_task"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Runs a notebook on Databricks using an Airflow operator.\n\nThe DatabricksNotebookOperator allows users to launch and monitor notebook job runs on Databricks as\nAirflow tasks. It can be used as a part of a DatabricksWorkflowTaskGroup to take advantage of job\nclusters, which allows users to run their tasks on cheaper clusters that can be shared between tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DatabricksNotebookOper",
      "success_score": 196,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.689351",
      "relevance_keywords": [
        "databricksnotebookoperator",
        "data-engineering",
        "operator",
        "notebook",
        "databricks"
      ]
    },
    {
      "id": "official_databrickstaskoperator_20260128_160330",
      "component_name": "DatabricksTaskOperator",
      "category": "data-engineering",
      "subcategory": "databricks",
      "provider": "databricks",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DatabricksTaskOperator",
        "component_type": "operator",
        "category": "data-engineering",
        "provider": "databricks",
        "base_classes": [
          "DatabricksTaskBaseOperator"
        ],
        "template_fields": [
          "workflow_run_metadata"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "task_config",
          "type": "dict",
          "required": true
        },
        {
          "name": "databricks_conn_id",
          "type": "str",
          "required": false,
          "default": "databricks_default"
        },
        {
          "name": "databricks_retry_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "databricks_retry_delay",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "databricks_retry_limit",
          "type": "int",
          "required": false,
          "default": 3
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "existing_cluster_id",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "job_cluster_key",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "new_cluster",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "polling_period_seconds",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "wait_for_termination",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "workflow_run_metadata",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DatabricksTaskBaseOperator",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport hashlib\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator, BaseOperatorLink, XCom, conf\nfrom airflow.providers.databricks.hooks.databricks import (",
        "class_attributes": [
          "CALLER",
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_get_task_base_json"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Runs a task on Databricks using an Airflow operator.\n\nThe DatabricksTaskOperator allows users to launch and monitor task job runs on Databricks as Airflow\ntasks. It can be used as a part of a DatabricksWorkflowTaskGroup to take advantage of job clusters, which\nallows users to run their tasks on cheaper clusters that can be shared between tasks.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:DatabricksTaskOperator`\n\n:param ta",
      "success_score": 193,
      "pattern_type": "official_data-engineering_operator",
      "indexed_at": "2026-01-28T16:03:29.689933",
      "relevance_keywords": [
        "data-engineering",
        "task",
        "operator",
        "databrickstaskoperator",
        "databricks"
      ]
    },
    {
      "id": "official_kubernetesjoboperator_20260128_160330",
      "component_name": "KubernetesJobOperator",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KubernetesJobOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "KubernetesPodOperator"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "job_template_file",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "full_job_spec",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "backoff_limit",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "completion_mode",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "completions",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "manual_selector",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "parallelism",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "selector",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "suspend",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ttl_seconds_after_finished",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_until_job_complete",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "job_poll_interval",
          "type": "float",
          "required": false,
          "default": 10
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "on_kill_propagation_policy",
          "type": "Literal[...]",
          "required": false,
          "default": "Foreground"
        },
        {
          "name": "discover_pods_retry_number",
          "type": "int",
          "required": false,
          "default": 3
        }
      ],
      "success_factors": [
        "Inherits from KubernetesPodOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport copy\nimport json\nimport logging\nimport os\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, Literal\nfrom kubernetes.client import BatchV1Api, models as k8s",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "pod",
          "pod",
          "_incluster_namespace",
          "hook",
          "job_client",
          "create_job",
          "execute",
          "execute_deferrable",
          "execute_complete",
          "deserialize_job_template_file",
          "on_kill",
          "build_job_request_obj",
          "reconcile_jobs",
          "reconcile_job_specs",
          "get_pods"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Executes a Kubernetes Job.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:KubernetesJobOperator`\n\n.. note::\n    If you use `Google Kubernetes Engine <https://cloud.google.com/kubernetes-engine/>`__\n    and Airflow is not running in the same cluster, consider using\n    :class:`~airflow.providers.google.cloud.operators.kubernetes_engine.GKEStartJobOperator`, which\n    simplifies the authorization process.\n\n:param job_template_",
      "success_score": 190,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:29.914703",
      "relevance_keywords": [
        "operator",
        "cncf",
        "kubernetesjoboperator",
        "container",
        "job",
        "kubernetes"
      ]
    },
    {
      "id": "official_kubernetesdeletejoboperator_20260128_160330",
      "component_name": "KubernetesDeleteJobOperator",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KubernetesDeleteJobOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "config_file",
          "name",
          "namespace",
          "cluster_context"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "namespace",
          "type": "str",
          "required": false
        },
        {
          "name": "kubernetes_conn_id",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "config_file",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "in_cluster",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_context",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "delete_on_status",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "wait_for_completion",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "poll_interval",
          "type": "float",
          "required": false,
          "default": 10.0
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport copy\nimport json\nimport logging\nimport os\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, Literal\nfrom kubernetes.client import BatchV1Api, models as k8s",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "hook",
          "client",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Delete a Kubernetes Job.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:KubernetesDeleteJobOperator`\n\n:param name: name of the Job.\n:param namespace: the namespace to run within kubernetes.\n:param kubernetes_conn_id: The :ref:`kubernetes connection id <howto/connection:kubernetes>`\n    for the Kubernetes cluster.\n:param config_file: The path to the Kubernetes config file. (templated)\n    If not specified, default value is ``",
      "success_score": 202,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:29.915164",
      "relevance_keywords": [
        "kubernetesdeletejoboperator",
        "operator",
        "cncf",
        "delete",
        "container",
        "job",
        "kubernetes"
      ]
    },
    {
      "id": "official_kubernetespatchjoboperator_20260128_160330",
      "component_name": "KubernetesPatchJobOperator",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KubernetesPatchJobOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "config_file",
          "name",
          "namespace",
          "body",
          "cluster_context"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "name",
          "type": "str",
          "required": false
        },
        {
          "name": "namespace",
          "type": "str",
          "required": false
        },
        {
          "name": "body",
          "type": "object",
          "required": false
        },
        {
          "name": "kubernetes_conn_id",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "config_file",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "in_cluster",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_context",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport copy\nimport json\nimport logging\nimport os\nimport warnings\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, Literal\nfrom kubernetes.client import BatchV1Api, models as k8s",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "hook",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": true
      },
      "docstring_summary": "Update a Kubernetes Job.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:KubernetesPatchJobOperator`\n\n:param name: name of the Job\n:param namespace: the namespace to run within kubernetes\n:param body: Job json object with parameters for update\n    https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#job-v1-batch\n    e.g. ``{\"spec\": {\"suspend\": True}}``\n:param kubernetes_conn_id: The :ref:`kubernetes connection",
      "success_score": 205,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:29.915541",
      "relevance_keywords": [
        "operator",
        "cncf",
        "patch",
        "container",
        "job",
        "kubernetes",
        "kubernetespatchjoboperator"
      ]
    },
    {
      "id": "official_kubernetesinstallkueueoperator_20260128_160330",
      "component_name": "KubernetesInstallKueueOperator",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KubernetesInstallKueueOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "kueue_version",
          "kubernetes_conn_id"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "kueue_version",
          "type": "str",
          "required": true
        },
        {
          "name": "kubernetes_conn_id",
          "type": "str",
          "required": false,
          "default": "kubernetes_default"
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom kubernetes.utils import FailToCreateError\nfrom airflow.providers.cncf.kubernetes.hooks.kubernetes import KubernetesHook\nfrom airflow.providers.cncf.kubernetes.operators.job import KubernetesJobOperator\nfrom airflow.providers.cncf.kubernetes.version_compat import AIRFLOW_V_3_1_PLUS\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "hook",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Installs a Kubernetes Kueue.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:KubernetesInstallKueueOperator`\n\n:param kueue_version: The Kubernetes Kueue version to install.\n:param kubernetes_conn_id: The :ref:`kubernetes connection id <howto/connection:kubernetes>`\n    for the Kubernetes cluster.",
      "success_score": 181,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:29.954854",
      "relevance_keywords": [
        "operator",
        "kueue",
        "cncf",
        "install",
        "container",
        "kubernetesinstallkueueoperator",
        "kubernetes"
      ]
    },
    {
      "id": "official_kubernetesstartkueuejoboperator_20260128_160330",
      "component_name": "KubernetesStartKueueJobOperator",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KubernetesStartKueueJobOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "KubernetesJobOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "queue_name",
          "type": "str",
          "required": true
        }
      ],
      "success_factors": [
        "Inherits from KubernetesJobOperator",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom kubernetes.utils import FailToCreateError\nfrom airflow.providers.cncf.kubernetes.hooks.kubernetes import KubernetesHook\nfrom airflow.providers.cncf.kubernetes.operators.job import KubernetesJobOperator\nfrom airflow.providers.cncf.kubernetes.version_compat import AIRFLOW_V_3_1_PLUS\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Executes a Kubernetes Job in Kueue.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:KubernetesStartKueueJobOperator`\n\n:param queue_name: The name of the Queue in the cluster",
      "success_score": 175,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:29.955044",
      "relevance_keywords": [
        "operator",
        "start",
        "kueue",
        "cncf",
        "container",
        "job",
        "kubernetesstartkueuejoboperator",
        "kubernetes"
      ]
    },
    {
      "id": "official_kubernetespodoperator_20260128_160330",
      "component_name": "KubernetesPodOperator",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KubernetesPodOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "image",
          "name",
          "hostname",
          "cmds",
          "annotations",
          "arguments",
          "env_vars",
          "labels",
          "config_file",
          "pod_template_file",
          "pod_template_dict",
          "namespace",
          "container_resources",
          "volumes",
          "volume_mounts",
          "cluster_context",
          "env_from",
          "node_selector",
          "kubernetes_conn_id",
          "base_container_name",
          "trigger_kwargs"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "kubernetes_conn_id",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "namespace",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "image",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "random_name_suffix",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "cmds",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "arguments",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ports",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "volume_mounts",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "volumes",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "env_vars",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "env_from",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "secrets",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "in_cluster",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_context",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport datetime\nimport json\nimport logging\nimport math\nimport os\nimport re\nimport shlex\nimport string",
        "class_attributes": [
          "BASE_CONTAINER_NAME",
          "ISTIO_CONTAINER_NAME",
          "KILL_ISTIO_PROXY_SUCCESS_MSG",
          "POD_CHECKED_KEY",
          "POST_TERMINATION_TIMEOUT",
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "_incluster_namespace",
          "_render_nested_template_fields",
          "_get_ti_pod_labels",
          "pod_manager",
          "hook",
          "client",
          "find_pod",
          "log_matching_pod",
          "get_or_create_pod",
          "await_pod_start",
          "extract_xcom",
          "execute",
          "execute_sync",
          "await_init_containers_completion",
          "await_pod_completion",
          "_handle_api_exception",
          "_refresh_cached_properties",
          "execute_async",
          "convert_config_file_to_dict",
          "invoke_defer_method",
          "trigger_reentry",
          "_clean",
          "_write_logs",
          "post_complete_action",
          "cleanup",
          "_read_pod_events",
          "_read_pod_container_states",
          "is_istio_enabled",
          "kill_istio_sidecar",
          "process_pod_deletion",
          "_build_find_pod_label_selector",
          "_set_name",
          "patch_already_checked",
          "on_kill",
          "build_pod_request_obj",
          "dry_run",
          "process_duplicate_label_pods",
          "_get_most_recent_pod_index"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Execute a task in a Kubernetes Pod.\n\n.. seealso::\n    For more information on how to use this operator, take a look at the guide:\n    :ref:`howto/operator:KubernetesPodOperator`\n\n.. note::\n    If you use `Google Kubernetes Engine <https://cloud.google.com/kubernetes-engine/>`__\n    and Airflow is not running in the same cluster, consider using\n    :class:`~airflow.providers.google.cloud.operators.kubernetes_engine.GKEStartPodOperator`, which\n    simplifies the authorization process.\n\n:param kube",
      "success_score": 205,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:30.005827",
      "relevance_keywords": [
        "operator",
        "kubernetespodoperator",
        "pod",
        "cncf",
        "container",
        "kubernetes"
      ]
    },
    {
      "id": "official_kubernetesresourcebaseoperator_20260128_160330",
      "component_name": "KubernetesResourceBaseOperator",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KubernetesResourceBaseOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "yaml_conf",
          "yaml_conf_file"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "yaml_conf",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "yaml_conf_file",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "namespace",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "kubernetes_conn_id",
          "type": "Union",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "custom_resource_definition",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "namespaced",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "config_file",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nimport yaml\nfrom kubernetes.utils import create_from_yaml\nfrom airflow.providers.cncf.kubernetes.hooks.kubernetes import KubernetesHook\nfrom airflow.providers.cncf.kubernetes.kubernetes_helper_functions import generic_api_retry",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "client",
          "custom_object_client",
          "hook",
          "get_namespace",
          "get_crd_fields"
        ],
        "has_execute": false,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Abstract base class for all Kubernetes Resource operators.\n\n:param yaml_conf: string. Contains the kubernetes resources to Create or Delete\n:param yaml_conf_file: path to the kubernetes resources file (templated)\n:param namespace: string. Contains the namespace to create all resources inside.\n    The namespace must preexist otherwise the resource creation will fail.\n    If the API object in the yaml file already contains a namespace definition then\n    this parameter has no effect.\n:param kubern",
      "success_score": 181,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:30.051041",
      "relevance_keywords": [
        "kubernetesresourcebaseoperator",
        "operator",
        "resource",
        "cncf",
        "base",
        "container",
        "kubernetes"
      ]
    },
    {
      "id": "official_kubernetescreateresourceoperator_20260128_160330",
      "component_name": "KubernetesCreateResourceOperator",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KubernetesCreateResourceOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "KubernetesResourceBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from KubernetesResourceBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nimport yaml\nfrom kubernetes.utils import create_from_yaml\nfrom airflow.providers.cncf.kubernetes.hooks.kubernetes import KubernetesHook\nfrom airflow.providers.cncf.kubernetes.kubernetes_helper_functions import generic_api_retry",
        "class_attributes": [],
        "methods": [
          "create_custom_from_yaml_object",
          "_create_objects",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Create a resource in a kubernetes.",
      "success_score": 155,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:30.051201",
      "relevance_keywords": [
        "operator",
        "resource",
        "create",
        "cncf",
        "kubernetescreateresourceoperator",
        "container",
        "kubernetes"
      ]
    },
    {
      "id": "official_kubernetesdeleteresourceoperator_20260128_160330",
      "component_name": "KubernetesDeleteResourceOperator",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KubernetesDeleteResourceOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "KubernetesResourceBaseOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from KubernetesResourceBaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nimport yaml\nfrom kubernetes.utils import create_from_yaml\nfrom airflow.providers.cncf.kubernetes.hooks.kubernetes import KubernetesHook\nfrom airflow.providers.cncf.kubernetes.kubernetes_helper_functions import generic_api_retry",
        "class_attributes": [],
        "methods": [
          "delete_custom_from_yaml_object",
          "_delete_objects",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Delete a resource in a kubernetes.",
      "success_score": 155,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:30.051300",
      "relevance_keywords": [
        "operator",
        "resource",
        "cncf",
        "delete",
        "container",
        "kubernetesdeleteresourceoperator",
        "kubernetes"
      ]
    },
    {
      "id": "official_sparkkubernetesoperator_20260128_160330",
      "component_name": "SparkKubernetesOperator",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SparkKubernetesOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "KubernetesPodOperator"
        ],
        "template_fields": [
          "application_file",
          "namespace",
          "template_spec",
          "kubernetes_conn_id"
        ],
        "has_deferrable": true,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "image",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "code_path",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "namespace",
          "type": "str",
          "required": false,
          "default": "default"
        },
        {
          "name": "name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "application_file",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "template_spec",
          "type": "Any",
          "required": false,
          "default": null
        },
        {
          "name": "get_logs",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "do_xcom_push",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "success_run_history_limit",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "startup_timeout_seconds",
          "type": "Any",
          "required": false,
          "default": 600
        },
        {
          "name": "log_events_on_failure",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "reattach_on_restart",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "delete_on_termination",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "kubernetes_conn_id",
          "type": "str",
          "required": false,
          "default": "kubernetes_default"
        },
        {
          "name": "random_name_suffix",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from KubernetesPodOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom functools import cached_property\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, cast\nfrom kubernetes.client import CoreV1Api, CustomObjectsApi, models as k8s\nfrom airflow.providers.cncf.kubernetes import pod_generator\nfrom airflow.providers.cncf.kubernetes.hooks.kubernetes import KubernetesHook, _load_body_to_dict\nfrom airflow.providers.cncf.kubernetes.kubernetes_helper_functions import add_unique_suffix\nfrom airflow.providers.cncf.kubernetes.operators.custom_object_launcher import CustomObjectLauncher\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "template_ext",
          "ui_color",
          "BASE_CONTAINER_NAME"
        ],
        "methods": [
          "__init__",
          "_render_nested_template_fields",
          "manage_template_specs",
          "create_job_name",
          "_get_ti_pod_labels",
          "pod_manager",
          "_try_numbers_match",
          "template_body",
          "find_spark_job",
          "process_pod_deletion",
          "hook",
          "client",
          "custom_obj_api",
          "launcher",
          "get_or_create_spark_crd",
          "execute",
          "_setup_spark_configuration",
          "find_pod",
          "on_kill",
          "patch_already_checked",
          "dry_run"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates sparkApplication object in kubernetes cluster.\n\n.. seealso::\n    For more detail about Spark Application Object have a look at the reference:\n    https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/v1beta2-1.3.3-3.1.1/docs/api-docs.md#sparkapplication\n\n:param image: Docker image you wish to launch. Defaults to hub.docker.com,\n:param code_path: path to the spark code in image,\n:param namespace: kubernetes namespace to put sparkApplication\n:param name: name of the pod in whic",
      "success_score": 187,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:30.069198",
      "relevance_keywords": [
        "operator",
        "cncf",
        "container",
        "kubernetes",
        "sparkkubernetesoperator",
        "spark"
      ]
    },
    {
      "id": "official_sparkkubernetessensor_20260128_160330",
      "component_name": "SparkKubernetesSensor",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SparkKubernetesSensor",
        "component_type": "sensor",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "application_name",
          "namespace"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "application_name",
          "type": "str",
          "required": false
        },
        {
          "name": "attach_log",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "namespace",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "container_name",
          "type": "str",
          "required": false,
          "default": "spark-kubernetes-driver"
        },
        {
          "name": "kubernetes_conn_id",
          "type": "str",
          "required": false,
          "default": "kubernetes_default"
        },
        {
          "name": "api_group",
          "type": "str",
          "required": false,
          "default": "sparkoperator.k8s.io"
        },
        {
          "name": "api_version",
          "type": "str",
          "required": false,
          "default": "v1beta2"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom kubernetes import client\nfrom airflow.providers.cncf.kubernetes.hooks.kubernetes import KubernetesHook\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator",
        "class_attributes": [
          "template_fields",
          "FAILURE_STATES",
          "SUCCESS_STATES"
        ],
        "methods": [
          "__init__",
          "hook",
          "_log_driver",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Checks sparkApplication object in kubernetes cluster.\n\n.. seealso::\n    For more detail about Spark Application Object have a look at the reference:\n    https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/v1beta2-1.1.0-2.4.5/docs/api-docs.md#sparkapplication\n\n:param application_name: spark Application resource name\n:param namespace: the kubernetes namespace where the sparkApplication reside in\n:param container_name: the kubernetes container name where the sparkApplication reside in",
      "success_score": 181,
      "pattern_type": "official_container_sensor",
      "indexed_at": "2026-01-28T16:03:30.087077",
      "relevance_keywords": [
        "cncf",
        "container",
        "sparkkubernetessensor",
        "sensor",
        "kubernetes",
        "spark"
      ]
    },
    {
      "id": "official_kuberneteshook_20260128_160330",
      "component_name": "KubernetesHook",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "KubernetesHook",
        "component_type": "hook",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "BaseHook",
          "PodOperatorHookProtocol"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "conn_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "kubernetes_conn_id",
          "type": "Union",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "client_configuration",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cluster_context",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "config_file",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "config_dict",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "in_cluster",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "disable_verify_ssl",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "disable_tcp_keepalive",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseHook, PodOperatorHookProtocol",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport contextlib\nimport json\nimport tempfile\nfrom collections.abc import AsyncGenerator\nfrom functools import cached_property\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any, Protocol\nimport aiofiles",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name",
          "DEFAULT_NAMESPACE"
        ],
        "methods": [
          "get_connection_form_widgets",
          "get_ui_field_behaviour",
          "__init__",
          "_coalesce_param",
          "get_connection",
          "conn_extras",
          "_get_field",
          "get_conn",
          "_get_default_client",
          "is_in_cluster",
          "api_client",
          "core_v1_client",
          "apps_v1_client",
          "custom_object_client",
          "batch_v1_client",
          "create_custom_object",
          "get_custom_object",
          "delete_custom_object",
          "get_namespace",
          "get_xcom_sidecar_container_image",
          "get_xcom_sidecar_container_resources",
          "get_pod_log_stream",
          "get_pod_logs",
          "get_pod",
          "get_namespaced_pod_list",
          "get_deployment_status",
          "create_job",
          "get_job",
          "get_job_status",
          "wait_until_job_complete",
          "list_jobs_all_namespaces",
          "list_jobs_from_namespace",
          "is_job_complete",
          "is_job_failed",
          "is_job_successful",
          "patch_namespaced_job",
          "apply_from_yaml_file",
          "check_kueue_deployment_running",
          "get_yaml_content_from_file",
          "test_connection"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Creates Kubernetes API connection.\n\n- use in cluster configuration by using extra field ``in_cluster`` in connection\n- use custom config by providing path to the file using extra field ``kube_config_path`` in connection\n- use custom configuration by providing content of kubeconfig file via\n    extra field ``kube_config`` in connection\n- use default config by providing no extras\n\nThis hook check for configuration option in the above order. Once an option is present it will\nuse this configuration.",
      "success_score": 175,
      "pattern_type": "official_container_hook",
      "indexed_at": "2026-01-28T16:03:30.112048",
      "relevance_keywords": [
        "hook",
        "cncf",
        "container",
        "kuberneteshook",
        "kubernetes"
      ]
    },
    {
      "id": "official_asynckuberneteshook_20260128_160330",
      "component_name": "AsyncKubernetesHook",
      "category": "container",
      "subcategory": "kubernetes",
      "provider": "cncf",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "AsyncKubernetesHook",
        "component_type": "hook",
        "category": "container",
        "provider": "cncf",
        "base_classes": [
          "KubernetesHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "config_dict",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "connection_extras",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from KubernetesHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport asyncio\nimport contextlib\nimport json\nimport tempfile\nfrom collections.abc import AsyncGenerator\nfrom functools import cached_property\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Any, Protocol\nimport aiofiles",
        "class_attributes": [],
        "methods": [
          "__init__"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Hook to use Kubernetes SDK asynchronously.",
      "success_score": 155,
      "pattern_type": "official_container_hook",
      "indexed_at": "2026-01-28T16:03:30.112336",
      "relevance_keywords": [
        "cncf",
        "container",
        "kubernetes",
        "asynckuberneteshook",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_postgreshook_20260128_160330",
      "component_name": "PostgresHook",
      "category": "database",
      "subcategory": "postgresql",
      "provider": "postgres",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "PostgresHook",
        "component_type": "hook",
        "category": "database",
        "provider": "postgres",
        "base_classes": [
          "DbApiHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "options",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "enable_log_db_messages",
          "type": "bool",
          "required": false,
          "default": false
        }
      ],
      "success_factors": [
        "Inherits from DbApiHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nfrom collections.abc import Mapping\nfrom contextlib import closing\nfrom copy import deepcopy\nfrom typing import TYPE_CHECKING, Any, Literal, Protocol, TypeAlias, cast, overload\nimport psycopg2\nimport psycopg2.extras\nfrom more_itertools import chunked\nfrom psycopg2.extras import DictCursor, NamedTupleCursor, RealDictCursor, execute_batch",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "default_client_log_level",
          "default_connector_version",
          "conn_type",
          "hook_name",
          "supports_autocommit",
          "supports_executemany",
          "ignored_extra_options",
          "default_azure_oauth_scope"
        ],
        "methods": [
          "__init__",
          "__cast_nullable",
          "sqlalchemy_url",
          "dialect_name",
          "dialect",
          "_notice_handler",
          "_get_cursor",
          "_generate_cursor_name",
          "get_conn",
          "get_df",
          "get_df",
          "get_df",
          "copy_expert",
          "get_uri",
          "bulk_load",
          "bulk_dump",
          "_serialize_cell_ppg2",
          "_serialize_cell_ppg3",
          "_serialize_cell",
          "get_iam_token",
          "get_aws_iam_token",
          "get_azure_iam_token",
          "get_table_primary_key",
          "get_openlineage_database_info",
          "_get_openlineage_redshift_authority_part",
          "get_openlineage_database_dialect",
          "get_openlineage_default_schema",
          "get_ui_field_behaviour",
          "get_db_log_messages",
          "insert_rows"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with Postgres.\n\nYou can specify ssl parameters in the extra field of your connection\nas ``{\"sslmode\": \"require\", \"sslcert\": \"/path/to/cert.pem\", etc}``.\nAlso, you can choose cursor as ``{\"cursor\": \"dictcursor\"}``. Refer to the\npsycopg2.extras or psycopg.rows for more details.\n\nNote: For Redshift, use keepalives_idle in the extra connection parameters\nand set it to less than 300 seconds.\n\nNote: For AWS IAM authentication, use iam in the extra connection parameters\nand set it to true. Lea",
      "success_score": 170,
      "pattern_type": "official_database_hook",
      "indexed_at": "2026-01-28T16:03:30.241996",
      "relevance_keywords": [
        "postgreshook",
        "database",
        "postgres",
        "hook"
      ]
    },
    {
      "id": "official_mysqlhook_20260128_160330",
      "component_name": "MySqlHook",
      "category": "database",
      "subcategory": "mysql",
      "provider": "mysql",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "MySqlHook",
        "component_type": "hook",
        "category": "database",
        "provider": "mysql",
        "base_classes": [
          "DbApiHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [],
      "success_factors": [
        "Inherits from DbApiHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport logging\nfrom typing import TYPE_CHECKING, Any, Union\nfrom urllib.parse import quote_plus, urlencode\nfrom airflow.providers.common.compat.sdk import AirflowOptionalProviderFeatureException\nfrom airflow.providers.common.sql.hooks.sql import DbApiHook",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name",
          "supports_autocommit"
        ],
        "methods": [
          "__init__",
          "set_autocommit",
          "get_autocommit",
          "_get_conn_config_mysql_client",
          "_get_conn_config_mysql_connector_python",
          "get_conn",
          "bulk_load",
          "bulk_dump",
          "_serialize_cell",
          "get_iam_token",
          "bulk_load_custom",
          "get_openlineage_database_info",
          "get_openlineage_database_dialect",
          "get_openlineage_default_schema",
          "get_uri"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with MySQL.\n\nYou can specify charset in the extra field of your connection\nas ``{\"charset\": \"utf8\"}``. Also you can choose cursor as\n``{\"cursor\": \"SSCursor\"}``. Refer to the MySQLdb.cursors for more details.\n\nNote: For AWS IAM authentication, use iam in the extra connection parameters\nand set it to true. Leave the password field empty. This will use the\n\"aws_default\" connection to get the temporary token unless you override\nin extras.\nextras example: ``{\"iam\":true, \"aws_conn_id\":\"my_aws",
      "success_score": 170,
      "pattern_type": "official_database_hook",
      "indexed_at": "2026-01-28T16:03:30.272384",
      "relevance_keywords": [
        "my",
        "mysql",
        "sql",
        "mysqlhook",
        "database",
        "hook"
      ]
    },
    {
      "id": "official_mongosensor_20260128_160330",
      "component_name": "MongoSensor",
      "category": "database",
      "subcategory": "mongodb",
      "provider": "mongo",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "MongoSensor",
        "component_type": "sensor",
        "category": "database",
        "provider": "mongo",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "collection",
          "query"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "collection",
          "type": "str",
          "required": false
        },
        {
          "name": "query",
          "type": "dict",
          "required": false
        },
        {
          "name": "mongo_conn_id",
          "type": "str",
          "required": false,
          "default": "mongo_default"
        },
        {
          "name": "mongo_db",
          "type": "Any",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import BaseSensorOperator\nfrom airflow.providers.mongo.hooks.mongo import MongoHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Checks for the existence of a document which matches the given query in MongoDB.\n\n.. code-block:: python\n\n    mongo_sensor = MongoSensor(\n        collection=\"coll\",\n        query={\"key\": \"value\"},\n        mongo_conn_id=\"mongo_default\",\n        mongo_db=\"admin\",\n        task_id=\"mongo_sensor\",\n    )\n\n:param collection: Target MongoDB collection.\n:param query: The query to find the target document.\n:param mongo_conn_id: The :ref:`Mongo connection id <howto/connection:mongo>` to use\n    when connec",
      "success_score": 176,
      "pattern_type": "official_database_sensor",
      "indexed_at": "2026-01-28T16:03:30.296810",
      "relevance_keywords": [
        "mongo",
        "sensor",
        "database",
        "mongosensor"
      ]
    },
    {
      "id": "official_mongohook_20260128_160330",
      "component_name": "MongoHook",
      "category": "database",
      "subcategory": "mongodb",
      "provider": "mongo",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "MongoHook",
        "component_type": "hook",
        "category": "database",
        "provider": "mongo",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "mongo_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Implements get_conn() for connection management",
        "Implements close() for proper resource cleanup"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Iterable\nfrom typing import TYPE_CHECKING, Any, overload\nfrom urllib.parse import quote_plus, urlunsplit\nimport pymongo\nfrom pymongo import MongoClient, ReplaceOne\nfrom pymongo.errors import CollectionInvalid\nfrom airflow.exceptions import AirflowConfigException",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "get_connection_form_widgets",
          "get_ui_field_behaviour",
          "__init__",
          "_validate_connection",
          "__enter__",
          "__exit__",
          "get_conn",
          "close",
          "_create_uri",
          "get_collection",
          "create_collection",
          "aggregate",
          "find",
          "find",
          "find",
          "insert_one",
          "insert_many",
          "update_one",
          "update_many",
          "replace_one",
          "replace_many",
          "delete_one",
          "delete_many",
          "distinct"
        ],
        "has_get_conn": true,
        "has_close": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "PyMongo wrapper to interact with MongoDB.\n\nMongo Connection Documentation\nhttps://docs.mongodb.com/manual/reference/connection-string/index.html\nYou can specify connection string options in extra field of your connection\nhttps://docs.mongodb.com/manual/reference/connection-string/index.html#connection-string-options\n\nIf you want use DNS seedlist, set `srv` to True.\n\nex.\n    {\"srv\": true, \"replicaSet\": \"test\", \"ssl\": true, \"connectTimeoutMS\": 30000}\n\nFor enabling SSL, the `\"ssl\": true` option can",
      "success_score": 170,
      "pattern_type": "official_database_hook",
      "indexed_at": "2026-01-28T16:03:30.313541",
      "relevance_keywords": [
        "mongo",
        "database",
        "hook",
        "mongohook"
      ]
    },
    {
      "id": "official_redispublishoperator_20260128_160330",
      "component_name": "RedisPublishOperator",
      "category": "cache",
      "subcategory": "redis",
      "provider": "redis",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedisPublishOperator",
        "component_type": "operator",
        "category": "cache",
        "provider": "redis",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "channel",
          "message"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "channel",
          "type": "str",
          "required": false
        },
        {
          "name": "message",
          "type": "str",
          "required": false
        },
        {
          "name": "redis_conn_id",
          "type": "str",
          "required": false,
          "default": "redis_default"
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import BaseOperator\nfrom airflow.providers.redis.hooks.redis import RedisHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Publish a message to Redis.\n\n:param channel: redis channel to which the message is published (templated)\n:param message: the message to publish (templated)\n:param redis_conn_id: redis connection to use",
      "success_score": 176,
      "pattern_type": "official_cache_operator",
      "indexed_at": "2026-01-28T16:03:30.347896",
      "relevance_keywords": [
        "operator",
        "redis",
        "publish",
        "cache",
        "redispublishoperator"
      ]
    },
    {
      "id": "official_redispubsubsensor_20260128_160330",
      "component_name": "RedisPubSubSensor",
      "category": "cache",
      "subcategory": "redis",
      "provider": "redis",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedisPubSubSensor",
        "component_type": "sensor",
        "category": "cache",
        "provider": "redis",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "channels"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "channels",
          "type": "Union",
          "required": false
        },
        {
          "name": "redis_conn_id",
          "type": "str",
          "required": false
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import BaseSensorOperator\nfrom airflow.providers.redis.hooks.redis import RedisHook",
        "class_attributes": [
          "template_fields",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "pubsub",
          "poke"
        ],
        "has_poke": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Redis sensor for reading a message from pub sub channels.\n\n:param channels: The channels to be subscribed to (templated)\n:param redis_conn_id: the redis connection id",
      "success_score": 168,
      "pattern_type": "official_cache_sensor",
      "indexed_at": "2026-01-28T16:03:30.369449",
      "relevance_keywords": [
        "redispubsubsensor",
        "redis",
        "pub",
        "sub",
        "cache",
        "sensor"
      ]
    },
    {
      "id": "official_redishook_20260128_160330",
      "component_name": "RedisHook",
      "category": "cache",
      "subcategory": "redis",
      "provider": "redis",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "RedisHook",
        "component_type": "hook",
        "category": "cache",
        "provider": "redis",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "redis_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom typing import Any\nfrom redis import Redis\nfrom airflow.providers.common.compat.sdk import BaseHook",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "get_conn",
          "get_ui_field_behaviour",
          "get_connection_form_widgets"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Wrapper for connection to interact with Redis in-memory data structure store.\n\nYou can set your db in the extra field of your connection as ``{\"db\": 3}``.\nAlso you can set ssl parameters as:\n``{\"ssl\": true, \"ssl_cert_reqs\": \"require\", \"ssl_certfile\": \"/path/to/cert.pem\", etc}``.",
      "success_score": 160,
      "pattern_type": "official_cache_hook",
      "indexed_at": "2026-01-28T16:03:30.381577",
      "relevance_keywords": [
        "redishook",
        "cache",
        "hook",
        "redis"
      ]
    },
    {
      "id": "official_dockeroperator_20260128_160330",
      "component_name": "DockerOperator",
      "category": "container",
      "subcategory": "docker",
      "provider": "docker",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DockerOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "docker",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "image",
          "command",
          "environment",
          "env_file",
          "container_name",
          "mounts"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "image",
          "type": "str",
          "required": false
        },
        {
          "name": "api_version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "command",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "container_name",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cpus",
          "type": "float",
          "required": false,
          "default": 1.0
        },
        {
          "name": "docker_url",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "environment",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "private_environment",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "env_file",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "force_pull",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "mem_limit",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "host_tmp_dir",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "network_mode",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tls_ca_cert",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tls_client_cert",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport ast\nimport os\nimport pickle\nimport tarfile\nfrom collections.abc import Container, Iterable, Sequence\nfrom functools import cached_property\nfrom io import BytesIO, StringIO\nfrom tempfile import TemporaryDirectory\nfrom typing import TYPE_CHECKING, Literal",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "template_ext"
        ],
        "methods": [
          "__init__",
          "hook",
          "cli",
          "_run_image",
          "_run_image_with_mounts",
          "_attempt_to_retrieve_result",
          "_copy_from_docker",
          "execute",
          "format_command",
          "on_kill",
          "unpack_environment_variables"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Execute a command inside a docker container.\n\nBy default, a temporary directory is\ncreated on the host and mounted into a container to allow storing files\nthat together exceed the default disk size of 10GB in a container.\nIn this case The path to the mounted directory can be accessed\nvia the environment variable ``AIRFLOW_TMP_DIR``.\n\nIf the volume cannot be mounted, warning is printed and an attempt is made to execute the docker\ncommand without the temporary folder mounted. This is to make it wo",
      "success_score": 190,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:30.412104",
      "relevance_keywords": [
        "docker",
        "dockeroperator",
        "container",
        "operator"
      ]
    },
    {
      "id": "official_dockerswarmoperator_20260128_160330",
      "component_name": "DockerSwarmOperator",
      "category": "container",
      "subcategory": "docker",
      "provider": "docker",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DockerSwarmOperator",
        "component_type": "operator",
        "category": "container",
        "provider": "docker",
        "base_classes": [
          "DockerOperator"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "image",
          "type": "str",
          "required": false
        },
        {
          "name": "args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "enable_logging",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "configs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "secrets",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "mode",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "networks",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "placement",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "container_resources",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "service_prefix",
          "type": "str",
          "required": false,
          "default": "airflow"
        },
        {
          "name": "logging_driver",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "logging_driver_opts",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from DockerOperator",
        "Implements execute() method"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport re\nimport shlex\nfrom datetime import datetime\nfrom time import sleep\nfrom typing import TYPE_CHECKING, Literal\nfrom docker import types\nfrom docker.errors import APIError\nfrom airflow.providers.common.compat.sdk import AirflowException",
        "class_attributes": [],
        "methods": [
          "__init__",
          "execute",
          "_run_service",
          "_service_status",
          "_has_service_terminated",
          "_stream_logs_to_output",
          "_attempt_to_retrieve_results",
          "format_args",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Execute a command as an ephemeral docker swarm service.\n\nExample use-case - Using Docker Swarm orchestration to make one-time\nscripts highly available.\n\nA temporary directory is created on the host and\nmounted into a container to allow storing files\nthat together exceed the default disk size of 10GB in a container.\nThe path to the mounted directory can be accessed\nvia the environment variable ``AIRFLOW_TMP_DIR``.\n\nIf a login to a private registry is required prior to pulling the image, a\nDocker ",
      "success_score": 170,
      "pattern_type": "official_container_operator",
      "indexed_at": "2026-01-28T16:03:30.427034",
      "relevance_keywords": [
        "operator",
        "docker",
        "container",
        "dockerswarmoperator",
        "swarm"
      ]
    },
    {
      "id": "official_dockerhook_20260128_160330",
      "component_name": "DockerHook",
      "category": "container",
      "subcategory": "docker",
      "provider": "docker",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "DockerHook",
        "component_type": "hook",
        "category": "container",
        "provider": "docker",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "docker_conn_id",
          "type": "Union",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "base_url",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "version",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "tls",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "timeout",
          "type": "int",
          "required": false,
          "default": "<DEFAULT_TIMEOUT_SECONDS>"
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport json\nimport warnings\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any\nfrom docker import APIClient, TLSConfig\nfrom docker.constants import DEFAULT_TIMEOUT_SECONDS\nfrom docker.errors import APIError, DockerException\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowNotFoundException, BaseHook",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "construct_tls_config",
          "api_client",
          "client_created",
          "get_conn",
          "__login",
          "get_connection_form_widgets",
          "get_ui_field_behaviour",
          "_redact_tls_schema"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with a Docker Daemon and Container Registry.\n\nThis class provide a thin wrapper around the ``docker.APIClient``.\n\n.. seealso::\n    - :ref:`Docker Connection <howto/connection:docker>`\n    - `Docker SDK: Low-level API <https://docker-py.readthedocs.io/en/stable/api.html?low-level-api>`_\n\n:param docker_conn_id: :ref:`Docker connection id <howto/connection:docker>` where stored credentials\n     to Docker Registry. If set to ``None`` or empty then hook does not login to Container Registry.\n",
      "success_score": 175,
      "pattern_type": "official_container_hook",
      "indexed_at": "2026-01-28T16:03:30.442963",
      "relevance_keywords": [
        "docker",
        "dockerhook",
        "hook",
        "container"
      ]
    },
    {
      "id": "official_sshoperator_20260128_160330",
      "component_name": "SSHOperator",
      "category": "remote",
      "subcategory": "ssh",
      "provider": "ssh",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SSHOperator",
        "component_type": "operator",
        "category": "remote",
        "provider": "ssh",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "command",
          "environment",
          "remote_host"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "ssh_hook",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ssh_conn_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "remote_host",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "command",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "conn_timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cmd_timeout",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "environment",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "get_pty",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "banner_timeout",
          "type": "float",
          "required": false,
          "default": 30.0
        },
        {
          "name": "skip_on_exit_code",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom base64 import b64encode\nfrom collections.abc import Container, Sequence\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowSkipException, BaseOperator, conf\nfrom airflow.providers.ssh.hooks.ssh import SSHHook",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "template_fields_renderers"
        ],
        "methods": [
          "__init__",
          "ssh_hook",
          "hook",
          "get_ssh_client",
          "raise_for_status",
          "run_ssh_client_command",
          "execute",
          "tunnel",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "SSHOperator to execute commands on given remote host using the ssh_hook.\n\n:param ssh_hook: predefined ssh_hook to use for remote execution.\n    Either `ssh_hook` or `ssh_conn_id` needs to be provided.\n:param ssh_conn_id: :ref:`ssh connection id<howto/connection:ssh>`\n    from airflow Connections. `ssh_conn_id` will be ignored if\n    `ssh_hook` is provided.\n:param remote_host: remote host to connect (templated)\n    Nullable. If provided, it will replace the `remote_host` which was\n    defined in ",
      "success_score": 184,
      "pattern_type": "official_remote_operator",
      "indexed_at": "2026-01-28T16:03:30.488122",
      "relevance_keywords": [
        "ssh",
        "sshoperator",
        "operator",
        "remote"
      ]
    },
    {
      "id": "official_sshremotejoboperator_20260128_160330",
      "component_name": "SSHRemoteJobOperator",
      "category": "remote",
      "subcategory": "ssh",
      "provider": "ssh",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SSHRemoteJobOperator",
        "component_type": "operator",
        "category": "remote",
        "provider": "ssh",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "command",
          "environment",
          "remote_host",
          "remote_base_dir"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "ssh_conn_id",
          "type": "str",
          "required": false
        },
        {
          "name": "command",
          "type": "str",
          "required": false
        },
        {
          "name": "remote_host",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "environment",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "remote_base_dir",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "poll_interval",
          "type": "int",
          "required": false,
          "default": 5
        },
        {
          "name": "log_chunk_size",
          "type": "int",
          "required": false,
          "default": 65536
        },
        {
          "name": "timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cleanup",
          "type": "Literal[...]",
          "required": false,
          "default": "never"
        },
        {
          "name": "remote_os",
          "type": "Literal[...]",
          "required": false,
          "default": "auto"
        },
        {
          "name": "skip_on_exit_code",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "conn_timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "banner_timeout",
          "type": "float",
          "required": false,
          "default": 30.0
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Supports deferrable execution with execute_complete()",
        "Integrates with Airflow Triggers for async operations",
        "Uses @cached_property for lazy initialization",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport warnings\nfrom collections.abc import Container, Sequence\nfrom datetime import timedelta\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Any, Literal\nfrom airflow.providers.common.compat.sdk import AirflowException, AirflowSkipException, BaseOperator\nfrom airflow.providers.ssh.hooks.ssh import SSHHook\nfrom airflow.providers.ssh.triggers.ssh_remote_job import SSHRemoteJobTrigger",
        "class_attributes": [
          "template_fields",
          "template_ext",
          "template_fields_renderers",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "_validate_base_dir",
          "ssh_hook",
          "_detect_remote_os",
          "execute",
          "execute_complete",
          "_cleanup_remote_job",
          "on_kill"
        ],
        "has_execute": true,
        "has_execute_complete": true,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Execute a command on a remote host via SSH with deferrable monitoring.\n\nThis operator submits a job to run detached on the remote host, then\nuses a trigger to asynchronously monitor the job status and stream logs.\nThis approach is resilient to network interruptions as the remote job\ncontinues running independently of the SSH connection.\n\nThe remote job is wrapped to:\n- Run detached from the SSH session (via nohup on POSIX, Start-Process on Windows)\n- Redirect stdout/stderr to a log file\n- Write ",
      "success_score": 202,
      "pattern_type": "official_remote_operator",
      "indexed_at": "2026-01-28T16:03:30.525351",
      "relevance_keywords": [
        "ssh",
        "operator",
        "remote",
        "job",
        "sshremotejoboperator"
      ]
    },
    {
      "id": "official_sshhook_20260128_160330",
      "component_name": "SSHHook",
      "category": "remote",
      "subcategory": "ssh",
      "provider": "ssh",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SSHHook",
        "component_type": "hook",
        "category": "remote",
        "provider": "ssh",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "ssh_conn_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "remote_host",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "username",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "password",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "key_file",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "port",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "conn_timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cmd_timeout",
          "type": "Union",
          "required": false,
          "default": "<NOTSET>"
        },
        {
          "name": "keepalive_interval",
          "type": "int",
          "required": false,
          "default": 30
        },
        {
          "name": "banner_timeout",
          "type": "float",
          "required": false,
          "default": 30.0
        },
        {
          "name": "disabled_algorithms",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ciphers",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "auth_timeout",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "host_proxy_cmd",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport logging\nimport os\nfrom base64 import decodebytes\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom io import StringIO\nfrom select import select\nfrom typing import Any",
        "class_attributes": [
          "_pkey_loaders",
          "_host_key_mappings",
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "get_ui_field_behaviour",
          "__init__",
          "host_proxy",
          "get_conn",
          "get_tunnel",
          "_pkey_from_private_key",
          "exec_ssh_client_command",
          "test_connection"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Execute remote commands with Paramiko.\n\n.. seealso:: https://github.com/paramiko/paramiko\n\nThis hook also lets you create ssh tunnel and serve as basis for SFTP file transfer.\n\n:param ssh_conn_id: :ref:`ssh connection id<howto/connection:ssh>` from airflow\n    Connections from where all the required parameters can be fetched like\n    username, password or key_file, though priority is given to the\n    params passed during init.\n:param remote_host: remote host to connect\n:param username: username ",
      "success_score": 175,
      "pattern_type": "official_remote_hook",
      "indexed_at": "2026-01-28T16:03:30.559333",
      "relevance_keywords": [
        "ssh",
        "sshhook",
        "hook",
        "remote"
      ]
    },
    {
      "id": "official_sshhookasync_20260128_160330",
      "component_name": "SSHHookAsync",
      "category": "remote",
      "subcategory": "ssh",
      "provider": "ssh",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SSHHookAsync",
        "component_type": "hook",
        "category": "remote",
        "provider": "ssh",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": true,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "ssh_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "host",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "port",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "username",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "password",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "known_hosts",
          "type": "str",
          "required": false,
          "default": "<default_known_hosts>"
        },
        {
          "name": "key_file",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "passphrase",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "private_key",
          "type": "str",
          "required": false,
          "default": ""
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport logging\nimport os\nfrom base64 import decodebytes\nfrom collections.abc import Sequence\nfrom functools import cached_property\nfrom io import StringIO\nfrom select import select\nfrom typing import Any",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name",
          "default_known_hosts"
        ],
        "methods": [
          "__init__",
          "_parse_extras"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Asynchronous SSH hook using asyncssh for use in triggers.\n\nThis hook provides async SSH connectivity for deferrable operators\nand their triggers.\n\n:param ssh_conn_id: SSH connection ID from Airflow Connections\n:param host: hostname of the SSH server (overrides connection)\n:param port: port of the SSH server (overrides connection)\n:param username: username for authentication (overrides connection)\n:param password: password for authentication (overrides connection)\n:param known_hosts: path to know",
      "success_score": 175,
      "pattern_type": "official_remote_hook",
      "indexed_at": "2026-01-28T16:03:30.559526",
      "relevance_keywords": [
        "ssh",
        "remote",
        "async",
        "hook",
        "sshhookasync"
      ]
    },
    {
      "id": "official_sftpoperator_20260128_160330",
      "component_name": "SFTPOperator",
      "category": "remote",
      "subcategory": "sftp",
      "provider": "sftp",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SFTPOperator",
        "component_type": "operator",
        "category": "remote",
        "provider": "sftp",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "local_filepath",
          "remote_filepath",
          "remote_host"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "sftp_hook",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "ssh_conn_id",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "remote_host",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "local_filepath",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "remote_filepath",
          "type": "Union",
          "required": false
        },
        {
          "name": "operation",
          "type": "str",
          "required": false,
          "default": "<computed>"
        },
        {
          "name": "confirm",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "create_intermediate_dirs",
          "type": "bool",
          "required": false,
          "default": false
        },
        {
          "name": "concurrency",
          "type": "int",
          "required": false,
          "default": 1
        },
        {
          "name": "prefetch",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nimport socket\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom typing import Any\nimport paramiko\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "execute",
          "get_openlineage_facets_on_start",
          "_get_namespace"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "SFTPOperator for transferring files from remote host to local or vice a versa.\n\nThis operator uses sftp_hook to open sftp transport channel that serve as basis for file transfer.\n\n:param ssh_conn_id: :ref:`ssh connection id<howto/connection:ssh>`\n    from airflow Connections.\n:param sftp_hook: predefined SFTPHook to use\n    Either `sftp_hook` or `ssh_conn_id` needs to be provided.\n:param remote_host: remote host to connect (templated)\n    Nullable. If provided, it will replace the `remote_host` ",
      "success_score": 179,
      "pattern_type": "official_remote_operator",
      "indexed_at": "2026-01-28T16:03:30.605908",
      "relevance_keywords": [
        "sftpoperator",
        "sftp",
        "operator",
        "remote"
      ]
    },
    {
      "id": "official_sftpsensor_20260128_160330",
      "component_name": "SFTPSensor",
      "category": "remote",
      "subcategory": "sftp",
      "provider": "sftp",
      "component_type": "sensor",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SFTPSensor",
        "component_type": "sensor",
        "category": "remote",
        "provider": "sftp",
        "base_classes": [
          "BaseSensorOperator"
        ],
        "template_fields": [
          "path",
          "file_pattern",
          "newer_than"
        ],
        "has_deferrable": true,
        "has_trigger": true,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "path",
          "type": "str",
          "required": false
        },
        {
          "name": "file_pattern",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "newer_than",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "sftp_conn_id",
          "type": "str",
          "required": false,
          "default": "sftp_default"
        },
        {
          "name": "python_callable",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "op_args",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "op_kwargs",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "use_managed_conn",
          "type": "bool",
          "required": false,
          "default": true
        },
        {
          "name": "deferrable",
          "type": "bool",
          "required": false,
          "default": "<computed>"
        }
      ],
      "success_factors": [
        "Inherits from BaseSensorOperator",
        "Implements poke() method for polling",
        "Supports deferrable mode for resource efficiency",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport os\nfrom collections.abc import Callable, Sequence\nfrom datetime import datetime, timedelta\nfrom typing import TYPE_CHECKING, Any\nfrom paramiko.sftp import SFTP_NO_SUCH_FILE\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseSensorOperator, PokeReturnValue, conf\nfrom airflow.providers.sftp.hooks.sftp import SFTPHook",
        "class_attributes": [
          "template_fields"
        ],
        "methods": [
          "__init__",
          "_get_files",
          "poke",
          "execute",
          "execute_complete"
        ],
        "has_poke": true,
        "has_execute_complete": true,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": true
      },
      "docstring_summary": "Waits for a file or directory to be present on SFTP.\n\n:param path: Remote file or directory path\n:param file_pattern: The pattern that will be used to match the file (fnmatch format)\n:param sftp_conn_id: The connection to run the sensor against\n:param newer_than: DateTime for which the file or file path should be newer than, comparison is inclusive\n:param python_callable: Optional callable that will be called after files are found. The callable\n    will receive the found files list in ``op_kwarg",
      "success_score": 194,
      "pattern_type": "official_remote_sensor",
      "indexed_at": "2026-01-28T16:03:30.636420",
      "relevance_keywords": [
        "sftpsensor",
        "sensor",
        "sftp",
        "remote"
      ]
    },
    {
      "id": "official_sftphook_20260128_160330",
      "component_name": "SFTPHook",
      "category": "remote",
      "subcategory": "sftp",
      "provider": "sftp",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SFTPHook",
        "component_type": "hook",
        "category": "remote",
        "provider": "sftp",
        "base_classes": [
          "SSHHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "ssh_conn_id",
          "type": "Union",
          "required": false,
          "default": "sftp_default"
        },
        {
          "name": "host_proxy_cmd",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "use_managed_conn",
          "type": "bool",
          "required": false,
          "default": true
        }
      ],
      "success_factors": [
        "Inherits from SSHHook",
        "Implements get_conn() for connection management"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport concurrent.futures\nimport datetime\nimport functools\nimport os\nimport stat\nimport warnings\nfrom collections.abc import Callable, Generator, Sequence\nfrom contextlib import contextmanager\nfrom fnmatch import fnmatch",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "get_ui_field_behaviour",
          "__init__",
          "get_conn",
          "close_conn",
          "get_managed_conn",
          "get_conn_count",
          "describe_directory",
          "list_directory",
          "list_directory_with_attr",
          "mkdir",
          "isdir",
          "isfile",
          "create_directory",
          "delete_directory",
          "retrieve_file",
          "store_file",
          "delete_file",
          "retrieve_directory",
          "retrieve_directory_concurrently",
          "store_directory",
          "store_directory_concurrently",
          "get_mod_time",
          "path_exists",
          "_is_path_match",
          "walktree",
          "get_tree_map",
          "test_connection",
          "get_file_by_pattern",
          "get_files_by_pattern"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with SFTP.\n\nThis hook inherits the SSH hook. Please refer to SSH hook for the input\narguments.\n\n:Pitfalls::\n\n    - In contrast with FTPHook describe_directory only returns size, type and\n      modify. It doesn't return unix.owner, unix.mode, perm, unix.group and\n      unique.\n    - If no mode is passed to create_directory it will be created with 777\n      permissions.\n\nErrors that may occur throughout but should be handled downstream.\n\nFor consistency reasons with SSHHook, the preferred",
      "success_score": 170,
      "pattern_type": "official_remote_hook",
      "indexed_at": "2026-01-28T16:03:30.665790",
      "relevance_keywords": [
        "sftphook",
        "sftp",
        "hook",
        "remote"
      ]
    },
    {
      "id": "official_sftphookasync_20260128_160330",
      "component_name": "SFTPHookAsync",
      "category": "remote",
      "subcategory": "sftp",
      "provider": "sftp",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SFTPHookAsync",
        "component_type": "hook",
        "category": "remote",
        "provider": "sftp",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "sftp_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "host",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "port",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "username",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "password",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "known_hosts",
          "type": "str",
          "required": false,
          "default": "<default_known_hosts>"
        },
        {
          "name": "key_file",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "passphrase",
          "type": "str",
          "required": false,
          "default": ""
        },
        {
          "name": "private_key",
          "type": "str",
          "required": false,
          "default": ""
        }
      ],
      "success_factors": [
        "Inherits from BaseHook"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport concurrent.futures\nimport datetime\nimport functools\nimport os\nimport stat\nimport warnings\nfrom collections.abc import Callable, Generator, Sequence\nfrom contextlib import contextmanager\nfrom fnmatch import fnmatch",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name",
          "default_known_hosts"
        ],
        "methods": [
          "__init__",
          "_parse_extras"
        ],
        "has_get_conn": false,
        "has_close": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Interact with an SFTP server via asyncssh package.\n\n:param sftp_conn_id: SFTP connection ID to be used for connecting to SFTP server\n:param host: hostname of the SFTP server\n:param port: port of the SFTP server\n:param username: username used when authenticating to the SFTP server\n:param password: password used when authenticating to the SFTP server.\n    Can be left blank if using a key file\n:param known_hosts: path to the known_hosts file on the local file system. Defaults to ``~/.ssh/known_host",
      "success_score": 170,
      "pattern_type": "official_remote_hook",
      "indexed_at": "2026-01-28T16:03:30.666012",
      "relevance_keywords": [
        "sftp",
        "sftphookasync",
        "remote",
        "async",
        "hook"
      ]
    },
    {
      "id": "official_emailoperator_20260128_160330",
      "component_name": "EmailOperator",
      "category": "notification",
      "subcategory": "email",
      "provider": "smtp",
      "component_type": "operator",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "EmailOperator",
        "component_type": "operator",
        "category": "notification",
        "provider": "smtp",
        "base_classes": [
          "BaseOperator"
        ],
        "template_fields": [
          "to",
          "subject",
          "html_content",
          "from_email",
          "files",
          "cc",
          "bcc",
          "mime_subtype",
          "mime_charset",
          "conn_id",
          "custom_headers"
        ],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "to",
          "type": "Union",
          "required": false
        },
        {
          "name": "subject",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "html_content",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "from_email",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "files",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "cc",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "bcc",
          "type": "Union",
          "required": false,
          "default": null
        },
        {
          "name": "mime_subtype",
          "type": "str",
          "required": false,
          "default": "mixed"
        },
        {
          "name": "mime_charset",
          "type": "str",
          "required": false,
          "default": "utf-8"
        },
        {
          "name": "conn_id",
          "type": "str",
          "required": false,
          "default": "smtp_default"
        },
        {
          "name": "custom_headers",
          "type": "Union",
          "required": false,
          "default": null
        }
      ],
      "success_factors": [
        "Inherits from BaseOperator",
        "Implements execute() method",
        "Defines template_fields for Jinja templating support"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nfrom collections.abc import Sequence\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any\nfrom airflow.providers.common.compat.sdk import AirflowException, BaseOperator\nfrom airflow.providers.smtp.hooks.smtp import SmtpHook",
        "class_attributes": [
          "template_fields",
          "template_fields_renderers",
          "template_ext",
          "ui_color"
        ],
        "methods": [
          "__init__",
          "_read_template",
          "execute"
        ],
        "has_execute": true,
        "has_execute_complete": false,
        "uses_cached_property": false,
        "uses_property": false,
        "has_trigger_support": false
      },
      "docstring_summary": "Sends an email.\n\n:param to: list of emails to send the email to. (templated)\n:param from_email: email to send from. (templated)\n:param subject: subject line for the email. (templated)\n:param html_content: content of the email, html markup\n    is allowed. (templated)\n:param files: file names to attach in email (templated)\n:param cc: list of recipients to be added in CC field (templated)\n:param bcc: list of recipients to be added in BCC field (templated)\n:param mime_subtype: MIME sub content type\n",
      "success_score": 185,
      "pattern_type": "official_notification_operator",
      "indexed_at": "2026-01-28T16:03:30.717348",
      "relevance_keywords": [
        "operator",
        "smtp",
        "email",
        "emailoperator",
        "notification"
      ]
    },
    {
      "id": "official_smtphook_20260128_160330",
      "component_name": "SmtpHook",
      "category": "notification",
      "subcategory": "email",
      "provider": "smtp",
      "component_type": "hook",
      "source": "apache-airflow-official",
      "metadata": {
        "component_name": "SmtpHook",
        "component_type": "hook",
        "category": "notification",
        "provider": "smtp",
        "base_classes": [
          "BaseHook"
        ],
        "template_fields": [],
        "has_deferrable": false,
        "has_trigger": false,
        "airflow_3x_compatible": true
      },
      "inputs": [
        {
          "name": "smtp_conn_id",
          "type": "str",
          "required": false,
          "default": "<default_conn_name>"
        },
        {
          "name": "auth_type",
          "type": "str",
          "required": false,
          "default": "basic"
        }
      ],
      "success_factors": [
        "Inherits from BaseHook",
        "Implements get_conn() for connection management",
        "Uses @cached_property for lazy initialization"
      ],
      "code_patterns": {
        "imports": "from __future__ import annotations\nimport collections.abc\nimport os\nimport re\nimport smtplib",
        "class_attributes": [
          "conn_name_attr",
          "default_conn_name",
          "conn_type",
          "hook_name"
        ],
        "methods": [
          "__init__",
          "__enter__",
          "__exit__",
          "_setup_oauth2",
          "get_conn",
          "_build_client_kwargs",
          "_build_client",
          "get_connection_form_widgets",
          "test_connection",
          "_build_message",
          "send_email_smtp",
          "_build_mime_message",
          "_get_email_address_list",
          "_get_email_list_from_str",
          "_get_oauth2_token",
          "conn",
          "smtp_retry_limit",
          "from_email",
          "smtp_user",
          "smtp_password",
          "smtp_starttls",
          "host",
          "port",
          "timeout",
          "use_ssl",
          "subject_template",
          "html_content_template",
          "ssl_context",
          "auth_type",
          "access_token",
          "_read_template",
          "get_ui_field_behaviour"
        ],
        "has_get_conn": true,
        "has_close": false,
        "uses_cached_property": true,
        "uses_property": true,
        "has_trigger_support": false
      },
      "docstring_summary": "This hook connects to a mail server by using the smtp protocol.\n\n.. note:: Please call this Hook as context manager via `with`\n    to automatically open and close the connection to the mail server.\n\n:param smtp_conn_id: The :ref:`smtp connection id <howto/connection:smtp>`\n    that contains the information used to authenticate the client.",
      "success_score": 175,
      "pattern_type": "official_notification_hook",
      "indexed_at": "2026-01-28T16:03:30.748095",
      "relevance_keywords": [
        "smtphook",
        "hook",
        "smtp",
        "notification"
      ]
    }
  ],
  "updated_at": "2026-01-28T16:03:48.933033",
  "pattern_summary": {
    "total_patterns": 926,
    "by_component_type": {
      "operator": 645,
      "sensor": 113,
      "hook": 168
    },
    "by_category": {
      "ml": 2,
      "monitoring": 1,
      "integration": 6,
      "cloud": 849,
      "notification": 10,
      "database": 6,
      "data-quality": 1,
      "data-processing": 1,
      "utility": 3,
      "cache": 4,
      "data-engineering": 19,
      "container": 16,
      "remote": 8
    }
  }
}